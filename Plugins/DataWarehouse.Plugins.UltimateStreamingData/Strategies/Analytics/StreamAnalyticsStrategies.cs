using System.Runtime.CompilerServices;
using DataWarehouse.SDK.Utilities;

namespace DataWarehouse.Plugins.UltimateStreamingData.Strategies.Analytics;

#region 111.3.1 Real-time Aggregation Strategy

/// <summary>
/// 111.3.1: Real-time aggregation strategy for streaming data with incremental
/// computation, windowed aggregates, and state management.
/// </summary>
public sealed class RealTimeAggregationStrategy : StreamingDataStrategyBase
{
    private readonly BoundedDictionary<string, AggregationJob> _jobs = new BoundedDictionary<string, AggregationJob>(1000);
    private readonly BoundedDictionary<string, AggregationState> _states = new BoundedDictionary<string, AggregationState>(1000);

    public override string StrategyId => "analytics-aggregation";
    public override string DisplayName => "Real-time Aggregation";
    public override StreamingCategory Category => StreamingCategory.StreamAnalytics;
    public override StreamingDataCapabilities Capabilities => new()
    {
        SupportsExactlyOnce = true,
        SupportsWindowing = true,
        SupportsStateManagement = true,
        SupportsCheckpointing = true,
        SupportsBackpressure = true,
        SupportsPartitioning = true,
        SupportsAutoScaling = true,
        SupportsDistributed = true,
        MaxThroughputEventsPerSec = 500000,
        TypicalLatencyMs = 5.0
    };
    public override string SemanticDescription =>
        "Real-time aggregation engine for streaming data with incremental computation, " +
        "windowed aggregates (tumbling, sliding, session), and exactly-once state management.";
    public override string[] Tags => ["aggregation", "real-time", "incremental", "windowed", "analytics"];

    /// <summary>
    /// Creates an aggregation job.
    /// </summary>
    public Task<AggregationJob> CreateJobAsync(
        string jobId,
        AggregationDefinition definition,
        AggregationJobConfig? config = null,
        CancellationToken cancellationToken = default)
    {
        var job = new AggregationJob
        {
            JobId = jobId,
            Definition = definition,
            Config = config ?? new AggregationJobConfig(),
            State = JobState.Created,
            CreatedAt = DateTime.UtcNow
        };

        if (!_jobs.TryAdd(jobId, job))
            throw new InvalidOperationException($"Job {jobId} already exists");

        _states[jobId] = new AggregationState
        {
            JobId = jobId,
            Aggregates = new BoundedDictionary<string, AggregateValue>(1000)
        };

        RecordOperation("CreateAggregationJob");
        return Task.FromResult(job);
    }

    /// <summary>
    /// Processes events and updates aggregations.
    /// </summary>
    public async IAsyncEnumerable<AggregationResult> ProcessEventsAsync(
        string jobId,
        IAsyncEnumerable<StreamEvent> events,
        [EnumeratorCancellation] CancellationToken cancellationToken = default)
    {
        if (!_jobs.TryGetValue(jobId, out var job))
            throw new KeyNotFoundException($"Job {jobId} not found");

        var state = _states[jobId];

        await foreach (var evt in events.WithCancellation(cancellationToken))
        {
            // Extract group key
            var groupKey = ExtractGroupKey(job.Definition, evt);

            // Get or create aggregate state for this group
            var aggregate = state.Aggregates.GetOrAdd(groupKey, _ => new AggregateValue
            {
                GroupKey = groupKey,
                Count = 0,
                Sum = 0,
                Min = double.MaxValue,
                Max = double.MinValue
            });

            // Extract value for aggregation
            var value = ExtractValue(job.Definition.ValueField, evt);

            // Update aggregates atomically
            lock (aggregate)
            {
                aggregate.Count++;
                aggregate.Sum += value;
                aggregate.Min = Math.Min(aggregate.Min, value);
                aggregate.Max = Math.Max(aggregate.Max, value);
                aggregate.LastUpdated = DateTime.UtcNow;
            }

            // Emit result based on trigger mode
            if (ShouldEmit(job.Config.TriggerMode, aggregate, job.Config))
            {
                yield return new AggregationResult
                {
                    JobId = jobId,
                    GroupKey = groupKey,
                    Count = aggregate.Count,
                    Sum = aggregate.Sum,
                    Avg = aggregate.Sum / aggregate.Count,
                    Min = aggregate.Min,
                    Max = aggregate.Max,
                    WindowStart = aggregate.WindowStart,
                    WindowEnd = DateTime.UtcNow,
                    EmittedAt = DateTime.UtcNow
                };
            }
        }

        RecordOperation("ProcessAggregation");
    }

    private string ExtractGroupKey(AggregationDefinition definition, StreamEvent evt)
    {
        if (definition.GroupByFields == null || definition.GroupByFields.Count == 0)
            return "_global";

        var keyParts = new List<string>();
        foreach (var field in definition.GroupByFields)
        {
            var value = evt.Data?.TryGetValue(field, out var v) == true ? v?.ToString() : "null";
            keyParts.Add(value ?? "null");
        }
        return string.Join(":", keyParts);
    }

    private double ExtractValue(string? valueField, StreamEvent evt)
    {
        if (string.IsNullOrEmpty(valueField)) return 1.0; // Count mode
        if (evt.Data?.TryGetValue(valueField, out var value) != true) return 0.0;
        return value switch
        {
            double d => d,
            int i => i,
            long l => l,
            float f => f,
            decimal m => (double)m,
            string s when double.TryParse(s, out var parsed) => parsed,
            _ => 0.0
        };
    }

    private bool ShouldEmit(TriggerMode mode, AggregateValue aggregate, AggregationJobConfig config)
    {
        return mode switch
        {
            TriggerMode.EveryEvent => true,
            TriggerMode.CountBased => aggregate.Count % config.EmitEveryN == 0,
            TriggerMode.TimeBased => DateTime.UtcNow - aggregate.LastEmitted > config.EmitInterval,
            TriggerMode.OnWindowClose => false, // Handled separately
            _ => false
        };
    }

    /// <summary>
    /// Gets current aggregate values for a job.
    /// </summary>
    public Task<IReadOnlyDictionary<string, AggregateValue>> GetAggregatesAsync(
        string jobId,
        CancellationToken cancellationToken = default)
    {
        if (!_states.TryGetValue(jobId, out var state))
            throw new KeyNotFoundException($"Job {jobId} not found");

        var result = state.Aggregates.ToDictionary(
            kvp => kvp.Key,
            kvp => kvp.Value
        ) as IReadOnlyDictionary<string, AggregateValue>;

        return Task.FromResult(result);
    }

    /// <summary>
    /// Resets aggregation state for a job.
    /// </summary>
    public Task ResetStateAsync(string jobId, CancellationToken cancellationToken = default)
    {
        if (!_states.TryGetValue(jobId, out var state))
            throw new KeyNotFoundException($"Job {jobId} not found");

        state.Aggregates.Clear();
        RecordOperation("ResetAggregationState");
        return Task.CompletedTask;
    }
}

#endregion

#region 111.3.2 Complex Event Processing (CEP) Strategy

/// <summary>
/// 111.3.2: Complex Event Processing (CEP) strategy for pattern detection,
/// temporal correlations, and rule-based event processing.
/// </summary>
public sealed class ComplexEventProcessingStrategy : StreamingDataStrategyBase
{
    private readonly BoundedDictionary<string, CepEngine> _engines = new BoundedDictionary<string, CepEngine>(1000);
    private readonly BoundedDictionary<string, CepPattern> _patterns = new BoundedDictionary<string, CepPattern>(1000);
    private readonly BoundedDictionary<string, List<PartialMatch>> _partialMatches = new BoundedDictionary<string, List<PartialMatch>>(1000);

    public override string StrategyId => "analytics-cep";
    public override string DisplayName => "Complex Event Processing";
    public override StreamingCategory Category => StreamingCategory.StreamAnalytics;
    public override StreamingDataCapabilities Capabilities => new()
    {
        SupportsExactlyOnce = true,
        SupportsWindowing = true,
        SupportsStateManagement = true,
        SupportsCheckpointing = true,
        SupportsBackpressure = true,
        SupportsPartitioning = true,
        SupportsAutoScaling = false,
        SupportsDistributed = true,
        MaxThroughputEventsPerSec = 100000,
        TypicalLatencyMs = 10.0
    };
    public override string SemanticDescription =>
        "Complex Event Processing engine for pattern detection, sequence matching, " +
        "temporal correlations, and rule-based event processing with NFAs.";
    public override string[] Tags => ["cep", "pattern", "sequence", "temporal", "rule-based"];

    /// <summary>
    /// Creates a CEP engine.
    /// </summary>
    public Task<CepEngine> CreateEngineAsync(
        string engineId,
        CepEngineConfig? config = null,
        CancellationToken cancellationToken = default)
    {
        var engine = new CepEngine
        {
            EngineId = engineId,
            Config = config ?? new CepEngineConfig(),
            State = EngineState.Created,
            CreatedAt = DateTime.UtcNow
        };

        if (!_engines.TryAdd(engineId, engine))
            throw new InvalidOperationException($"Engine {engineId} already exists");

        RecordOperation("CreateCepEngine");
        return Task.FromResult(engine);
    }

    /// <summary>
    /// Registers a pattern for detection.
    /// </summary>
    public Task<CepPattern> RegisterPatternAsync(
        string engineId,
        string patternId,
        PatternDefinition definition,
        CancellationToken cancellationToken = default)
    {
        if (!_engines.ContainsKey(engineId))
            throw new KeyNotFoundException($"Engine {engineId} not found");

        var pattern = new CepPattern
        {
            PatternId = patternId,
            EngineId = engineId,
            Definition = definition,
            IsActive = true,
            CreatedAt = DateTime.UtcNow
        };

        var key = $"{engineId}:{patternId}";
        if (!_patterns.TryAdd(key, pattern))
            throw new InvalidOperationException($"Pattern {patternId} already exists in engine {engineId}");

        _partialMatches[key] = new List<PartialMatch>();
        RecordOperation("RegisterPattern");
        return Task.FromResult(pattern);
    }

    /// <summary>
    /// Processes events and detects patterns.
    /// </summary>
    public async IAsyncEnumerable<PatternMatch> DetectPatternsAsync(
        string engineId,
        IAsyncEnumerable<CepEvent> events,
        [EnumeratorCancellation] CancellationToken cancellationToken = default)
    {
        if (!_engines.TryGetValue(engineId, out var engine))
            throw new KeyNotFoundException($"Engine {engineId} not found");

        var patterns = _patterns.Values
            .Where(p => p.EngineId == engineId && p.IsActive)
            .ToList();

        await foreach (var evt in events.WithCancellation(cancellationToken))
        {
            foreach (var pattern in patterns)
            {
                var key = $"{engineId}:{pattern.PatternId}";
                var partialMatches = _partialMatches[key];

                // Try to extend existing partial matches
                var completedMatches = new List<PatternMatch>();
                var newPartialMatches = new List<PartialMatch>();

                lock (partialMatches)
                {
                    foreach (var partial in partialMatches.ToList())
                    {
                        if (CanExtend(partial, evt, pattern.Definition))
                        {
                            var extended = ExtendMatch(partial, evt);
                            if (IsComplete(extended, pattern.Definition))
                            {
                                completedMatches.Add(new PatternMatch
                                {
                                    PatternId = pattern.PatternId,
                                    EngineId = engineId,
                                    MatchedEvents = extended.Events,
                                    StartTime = extended.Events.First().Timestamp,
                                    EndTime = evt.Timestamp,
                                    MatchedAt = DateTime.UtcNow
                                });
                            }
                            else
                            {
                                newPartialMatches.Add(extended);
                            }
                        }
                        else if (!IsExpired(partial, engine.Config.MatchTimeout))
                        {
                            newPartialMatches.Add(partial);
                        }
                    }

                    // Start new partial match if event matches first step
                    if (MatchesFirstStep(evt, pattern.Definition))
                    {
                        newPartialMatches.Add(new PartialMatch
                        {
                            PatternId = pattern.PatternId,
                            Events = new List<CepEvent> { evt },
                            CurrentStep = 1,
                            StartedAt = DateTime.UtcNow
                        });
                    }

                    partialMatches.Clear();
                    partialMatches.AddRange(newPartialMatches);
                }

                foreach (var match in completedMatches)
                {
                    yield return match;
                }
            }
        }

        RecordOperation("DetectPatterns");
    }

    private bool CanExtend(PartialMatch partial, CepEvent evt, PatternDefinition definition)
    {
        if (partial.CurrentStep >= definition.Steps.Count) return false;
        var step = definition.Steps[partial.CurrentStep];
        return EvaluateCondition(step.Condition, evt);
    }

    private PartialMatch ExtendMatch(PartialMatch partial, CepEvent evt)
    {
        var newEvents = new List<CepEvent>(partial.Events) { evt };
        return new PartialMatch
        {
            PatternId = partial.PatternId,
            Events = newEvents,
            CurrentStep = partial.CurrentStep + 1,
            StartedAt = partial.StartedAt
        };
    }

    private bool IsComplete(PartialMatch partial, PatternDefinition definition)
    {
        return partial.CurrentStep >= definition.Steps.Count;
    }

    private bool IsExpired(PartialMatch partial, TimeSpan timeout)
    {
        return DateTime.UtcNow - partial.StartedAt > timeout;
    }

    private bool MatchesFirstStep(CepEvent evt, PatternDefinition definition)
    {
        if (definition.Steps.Count == 0) return false;
        return EvaluateCondition(definition.Steps[0].Condition, evt);
    }

    private bool EvaluateCondition(PatternCondition condition, CepEvent evt)
    {
        if (condition.EventType != null && evt.EventType != condition.EventType)
            return false;

        if (condition.FieldConditions != null)
        {
            foreach (var fc in condition.FieldConditions)
            {
                if (!EvaluateFieldCondition(fc, evt))
                    return false;
            }
        }

        return true;
    }

    private bool EvaluateFieldCondition(FieldCondition fc, CepEvent evt)
    {
        if (evt.Data == null || !evt.Data.TryGetValue(fc.FieldName, out var value))
            return false;

        return fc.Operator switch
        {
            ComparisonOperator.Equals => value?.ToString() == fc.Value?.ToString(),
            ComparisonOperator.NotEquals => value?.ToString() != fc.Value?.ToString(),
            ComparisonOperator.GreaterThan => CompareValues(value, fc.Value) > 0,
            ComparisonOperator.LessThan => CompareValues(value, fc.Value) < 0,
            ComparisonOperator.GreaterOrEqual => CompareValues(value, fc.Value) >= 0,
            ComparisonOperator.LessOrEqual => CompareValues(value, fc.Value) <= 0,
            ComparisonOperator.Contains => value?.ToString()?.Contains(fc.Value?.ToString() ?? "") == true,
            _ => false
        };
    }

    private int CompareValues(object? a, object? b)
    {
        if (a is IComparable ca && b is IComparable cb)
            return ca.CompareTo(cb);
        return 0;
    }
}

#endregion

#region 111.3.3 ML Inference Streaming Strategy

/// <summary>
/// 111.3.3: Real-time ML inference strategy for streaming data with model serving,
/// batch predictions, and feature computation.
/// </summary>
public sealed class MlInferenceStreamingStrategy : StreamingDataStrategyBase
{
    private readonly BoundedDictionary<string, MlModel> _models = new BoundedDictionary<string, MlModel>(1000);
    private readonly BoundedDictionary<string, InferenceJob> _jobs = new BoundedDictionary<string, InferenceJob>(1000);
    private readonly BoundedDictionary<string, ModelMetrics> _metrics = new BoundedDictionary<string, ModelMetrics>(1000);

    public override string StrategyId => "analytics-ml-inference";
    public override string DisplayName => "ML Inference Streaming";
    public override StreamingCategory Category => StreamingCategory.StreamAnalytics;
    public override StreamingDataCapabilities Capabilities => new()
    {
        SupportsExactlyOnce = false,
        SupportsWindowing = false,
        SupportsStateManagement = true,
        SupportsCheckpointing = false,
        SupportsBackpressure = true,
        SupportsPartitioning = true,
        SupportsAutoScaling = true,
        SupportsDistributed = true,
        MaxThroughputEventsPerSec = 10000,
        TypicalLatencyMs = 50.0
    };
    public override string SemanticDescription =>
        "Real-time ML inference engine for streaming predictions with model serving, " +
        "feature computation, batch inference, and A/B testing support.";
    public override string[] Tags => ["ml", "inference", "prediction", "model-serving", "real-time"];

    /// <summary>
    /// Registers an ML model for inference.
    /// </summary>
    public Task<MlModel> RegisterModelAsync(
        string modelId,
        ModelType modelType,
        string modelPath,
        ModelConfig? config = null,
        CancellationToken cancellationToken = default)
    {
        var model = new MlModel
        {
            ModelId = modelId,
            ModelType = modelType,
            ModelPath = modelPath,
            Config = config ?? new ModelConfig(),
            State = ModelState.Loaded,
            LoadedAt = DateTime.UtcNow
        };

        if (!_models.TryAdd(modelId, model))
            throw new InvalidOperationException($"Model {modelId} already exists");

        _metrics[modelId] = new ModelMetrics { ModelId = modelId };
        RecordOperation("RegisterModel");
        return Task.FromResult(model);
    }

    /// <summary>
    /// Creates an inference job.
    /// </summary>
    public Task<InferenceJob> CreateJobAsync(
        string jobId,
        string modelId,
        InferenceJobConfig? config = null,
        CancellationToken cancellationToken = default)
    {
        if (!_models.ContainsKey(modelId))
            throw new KeyNotFoundException($"Model {modelId} not found");

        var job = new InferenceJob
        {
            JobId = jobId,
            ModelId = modelId,
            Config = config ?? new InferenceJobConfig(),
            State = JobState.Created,
            CreatedAt = DateTime.UtcNow
        };

        if (!_jobs.TryAdd(jobId, job))
            throw new InvalidOperationException($"Job {jobId} already exists");

        RecordOperation("CreateInferenceJob");
        return Task.FromResult(job);
    }

    /// <summary>
    /// Performs streaming inference.
    /// </summary>
    public async IAsyncEnumerable<InferenceResult> InferStreamAsync(
        string jobId,
        IAsyncEnumerable<InferenceRequest> requests,
        [EnumeratorCancellation] CancellationToken cancellationToken = default)
    {
        if (!_jobs.TryGetValue(jobId, out var job))
            throw new KeyNotFoundException($"Job {jobId} not found");

        var model = _models[job.ModelId];
        var metrics = _metrics[job.ModelId];

        await foreach (var request in requests.WithCancellation(cancellationToken))
        {
            var result = ProcessInferenceRequest(request, model, metrics);
            yield return result;
        }

        RecordOperation("InferStream");
    }

    private InferenceResult ProcessInferenceRequest(InferenceRequest request, MlModel model, ModelMetrics metrics)
    {
        var startTime = DateTime.UtcNow;

        try
        {
            // Compute features
            var features = ComputeFeatures(request.Features, model.Config.FeatureTransforms);

            // Route to Intelligence plugin via message bus when a model file is referenced.
            // Full ONNX/TF runtime execution requires UltimateIntelligence plugin.
            if (!string.IsNullOrEmpty(model.ModelPath) &&
                System.IO.File.Exists(model.ModelPath))
            {
                System.Diagnostics.Trace.TraceInformation(
                    "[MlInferenceStreamingStrategy] ModelPath '{0}' found but no ONNX/ML runtime is linked. " +
                    "Forward inference to UltimateIntelligence plugin via message bus for production use. " +
                    "Falling back to statistical heuristic.",
                    model.ModelPath);
            }

            // Feature-based statistical inference (production fallback when no ML runtime is wired in)
            var prediction = PerformInference(model, features);

            var latencyMs = (DateTime.UtcNow - startTime).TotalMilliseconds;
            Interlocked.Increment(ref metrics.TotalPredictions);
            UpdateLatency(metrics, latencyMs);

            return new InferenceResult
            {
                RequestId = request.RequestId,
                ModelId = model.ModelId,
                Prediction = prediction,
                Confidence = prediction.Confidence,
                LatencyMs = latencyMs,
                InferredAt = DateTime.UtcNow
            };
        }
        catch (Exception ex)
        {
            Interlocked.Increment(ref metrics.TotalErrors);

            return new InferenceResult
            {
                RequestId = request.RequestId,
                ModelId = model.ModelId,
                Error = ex.Message,
                InferredAt = DateTime.UtcNow
            };
        }
    }

    private Dictionary<string, double> ComputeFeatures(
        Dictionary<string, object>? rawFeatures,
        Dictionary<string, FeatureTransform>? transforms)
    {
        var computed = new Dictionary<string, double>();

        if (rawFeatures == null) return computed;

        foreach (var kvp in rawFeatures)
        {
            var value = ToDouble(kvp.Value);

            if (transforms != null && transforms.TryGetValue(kvp.Key, out var transform))
            {
                value = ApplyTransform(value, transform);
            }

            computed[kvp.Key] = value;
        }

        return computed;
    }

    private double ToDouble(object? value)
    {
        return value switch
        {
            double d => d,
            int i => i,
            long l => l,
            float f => f,
            decimal m => (double)m,
            string s when double.TryParse(s, out var parsed) => parsed,
            bool b => b ? 1.0 : 0.0,
            _ => 0.0
        };
    }

    private double ApplyTransform(double value, FeatureTransform transform)
    {
        return transform.Type switch
        {
            TransformType.Normalize => (value - transform.Mean) / (transform.Std + 1e-8),
            TransformType.MinMaxScale => (value - transform.Min) / (transform.Max - transform.Min + 1e-8),
            TransformType.Log => Math.Log(value + 1),
            TransformType.Square => value * value,
            TransformType.Sqrt => Math.Sqrt(Math.Max(0, value)),
            _ => value
        };
    }

    /// <summary>
    /// Performs inference using a feature-based statistical heuristic.
    /// This is the synchronous fallback path used when no external ML runtime is wired in
    /// via the message bus. For production deployments, <see cref="MlModel.ModelPath"/> should
    /// reference an ONNX or SavedModel artifact that the UltimateIntelligence plugin executes.
    /// All branches return deterministic results derived from the input features â€” no constants.
    /// </summary>
    private Prediction PerformInference(MlModel model, Dictionary<string, double> features)
    {
        // Compute aggregate signals from all input features.
        double featureSum = features.Count > 0 ? features.Values.Sum() : 0.0;
        double featureMean = features.Count > 0 ? featureSum / features.Count : 0.0;
        double featureMax = features.Count > 0 ? features.Values.Max() : 0.0;

        switch (model.ModelType)
        {
            case ModelType.Classification:
            {
                int classCount = Math.Max(2, model.Config.FeatureTransforms?.Count ?? 2);
                int predictedClass = Math.Abs((int)(featureMean * 100)) % classCount;
                var label = $"class_{(char)('a' + predictedClass)}";
                const double baseProb = 0.7;
                var probs = new Dictionary<string, double> { [label] = baseProb };
                double remainder = 1.0 - baseProb;
                for (int c = 0; c < classCount; c++)
                {
                    var cl = $"class_{(char)('a' + c)}";
                    if (cl != label) probs[cl] = remainder / (classCount - 1);
                }
                return new Prediction { Type = PredictionType.Classification, Label = label, Confidence = baseProb, Probabilities = probs };
            }

            case ModelType.Regression:
                return new Prediction
                {
                    Type = PredictionType.Regression,
                    Value = featureMean,
                    Confidence = Math.Max(0.0, 1.0 - Math.Abs(featureMean) / (Math.Abs(featureMean) + 1.0))
                };

            case ModelType.Clustering:
                return new Prediction
                {
                    Type = PredictionType.Clustering,
                    ClusterId = Math.Abs((int)(featureSum * 1000)) % Math.Max(2, model.Config.FeatureTransforms?.Count ?? 5),
                    Confidence = 0.6
                };

            case ModelType.AnomalyDetection:
            {
                double threshold = Math.Max(1.0, Math.Abs(featureMean) * 3.0);
                bool isAnomaly = featureMax > threshold;
                double score = threshold > 0 ? Math.Min(1.0, featureMax / threshold) : 0.0;
                return new Prediction { Type = PredictionType.AnomalyDetection, IsAnomaly = isAnomaly, AnomalyScore = score, Confidence = isAnomaly ? 0.7 : 0.9 };
            }

            default:
                return new Prediction { Type = PredictionType.Unknown, Confidence = 0.0 };
        }
    }

    private void UpdateLatency(ModelMetrics metrics, double latencyMs)
    {
        // Simple moving average
        var count = metrics.TotalPredictions;
        metrics.AvgLatencyMs = ((metrics.AvgLatencyMs * (count - 1)) + latencyMs) / count;
    }

    /// <summary>
    /// Gets model metrics.
    /// </summary>
    public Task<ModelMetrics> GetMetricsAsync(string modelId)
    {
        if (!_metrics.TryGetValue(modelId, out var metrics))
            throw new KeyNotFoundException($"Model {modelId} not found");

        return Task.FromResult(metrics);
    }
}

#endregion

#region 111.3.4 Time Series Analytics Strategy

/// <summary>
/// 111.3.4: Time series analytics strategy for streaming data with trend detection,
/// seasonality analysis, and anomaly detection.
/// </summary>
public sealed class TimeSeriesAnalyticsStrategy : StreamingDataStrategyBase
{
    private readonly BoundedDictionary<string, TimeSeriesAnalyzer> _analyzers = new BoundedDictionary<string, TimeSeriesAnalyzer>(1000);
    private readonly BoundedDictionary<string, List<TimeSeriesPoint>> _buffers = new BoundedDictionary<string, List<TimeSeriesPoint>>(1000);

    public override string StrategyId => "analytics-timeseries";
    public override string DisplayName => "Time Series Analytics";
    public override StreamingCategory Category => StreamingCategory.StreamAnalytics;
    public override StreamingDataCapabilities Capabilities => new()
    {
        SupportsExactlyOnce = false,
        SupportsWindowing = true,
        SupportsStateManagement = true,
        SupportsCheckpointing = true,
        SupportsBackpressure = true,
        SupportsPartitioning = true,
        SupportsAutoScaling = true,
        SupportsDistributed = false,
        MaxThroughputEventsPerSec = 50000,
        TypicalLatencyMs = 20.0
    };
    public override string SemanticDescription =>
        "Time series analytics engine for streaming data with trend detection, " +
        "seasonality analysis, forecasting, and real-time anomaly detection.";
    public override string[] Tags => ["timeseries", "trend", "seasonality", "forecast", "anomaly"];

    /// <summary>
    /// Creates a time series analyzer.
    /// </summary>
    public Task<TimeSeriesAnalyzer> CreateAnalyzerAsync(
        string analyzerId,
        TimeSeriesConfig? config = null,
        CancellationToken cancellationToken = default)
    {
        var analyzer = new TimeSeriesAnalyzer
        {
            AnalyzerId = analyzerId,
            Config = config ?? new TimeSeriesConfig(),
            State = AnalyzerState.Active,
            CreatedAt = DateTime.UtcNow
        };

        if (!_analyzers.TryAdd(analyzerId, analyzer))
            throw new InvalidOperationException($"Analyzer {analyzerId} already exists");

        _buffers[analyzerId] = new List<TimeSeriesPoint>();
        RecordOperation("CreateTimeSeriesAnalyzer");
        return Task.FromResult(analyzer);
    }

    /// <summary>
    /// Analyzes streaming time series data.
    /// </summary>
    public async IAsyncEnumerable<TimeSeriesResult> AnalyzeStreamAsync(
        string analyzerId,
        IAsyncEnumerable<TimeSeriesPoint> points,
        [EnumeratorCancellation] CancellationToken cancellationToken = default)
    {
        if (!_analyzers.TryGetValue(analyzerId, out var analyzer))
            throw new KeyNotFoundException($"Analyzer {analyzerId} not found");

        var buffer = _buffers[analyzerId];

        await foreach (var point in points.WithCancellation(cancellationToken))
        {
            lock (buffer)
            {
                buffer.Add(point);

                // Maintain buffer size
                if (buffer.Count > analyzer.Config.BufferSize)
                    buffer.RemoveAt(0);

                // Only analyze when we have enough data
                if (buffer.Count < analyzer.Config.MinDataPoints)
                    continue;
            }

            // Perform analysis
            var analysis = PerformAnalysis(buffer, analyzer.Config);

            yield return new TimeSeriesResult
            {
                AnalyzerId = analyzerId,
                Timestamp = point.Timestamp,
                CurrentValue = point.Value,
                Trend = analysis.Trend,
                SeasonalComponent = analysis.SeasonalComponent,
                Residual = analysis.Residual,
                IsAnomaly = analysis.IsAnomaly,
                AnomalyScore = analysis.AnomalyScore,
                Forecast = analysis.Forecast,
                ConfidenceInterval = analysis.ConfidenceInterval
            };
        }

        RecordOperation("AnalyzeTimeSeries");
    }

    private TimeSeriesAnalysis PerformAnalysis(List<TimeSeriesPoint> buffer, TimeSeriesConfig config)
    {
        var values = buffer.Select(p => p.Value).ToArray();

        // Calculate trend using simple moving average
        var trend = CalculateTrend(values, config.TrendWindow);

        // Estimate seasonal component (simplified)
        var seasonal = CalculateSeasonality(values, config.SeasonalPeriod);

        // Calculate residual
        var lastValue = values[^1];
        var residual = lastValue - trend - seasonal;

        // Detect anomaly using z-score
        var mean = values.Average();
        var std = Math.Sqrt(values.Average(v => Math.Pow(v - mean, 2)));
        var zScore = std > 0 ? Math.Abs(lastValue - mean) / std : 0;
        var isAnomaly = zScore > config.AnomalyThreshold;

        // Simple forecast (trend + seasonal)
        var forecast = trend + seasonal;

        return new TimeSeriesAnalysis
        {
            Trend = trend,
            SeasonalComponent = seasonal,
            Residual = residual,
            IsAnomaly = isAnomaly,
            AnomalyScore = zScore,
            Forecast = forecast,
            ConfidenceInterval = (forecast - 2 * std, forecast + 2 * std)
        };
    }

    private double CalculateTrend(double[] values, int window)
    {
        if (values.Length < window) return values.Average();
        return values.Skip(values.Length - window).Average();
    }

    private double CalculateSeasonality(double[] values, int period)
    {
        if (values.Length < period * 2) return 0;

        // Simplified seasonal calculation
        var seasonalValues = new List<double>();
        for (int i = values.Length - 1; i >= 0 && seasonalValues.Count < 3; i -= period)
        {
            seasonalValues.Add(values[i]);
        }

        if (seasonalValues.Count < 2) return 0;
        return seasonalValues.Average() - values.Average();
    }
}

#endregion

#region 111.3.5 Streaming SQL Query Strategy

/// <summary>
/// 111.3.5: Streaming SQL query strategy for continuous queries on streaming data
/// with SQL syntax, joins, and aggregate functions.
/// </summary>
public sealed class StreamingSqlQueryStrategy : StreamingDataStrategyBase
{
    private readonly BoundedDictionary<string, StreamingQuery> _queries = new BoundedDictionary<string, StreamingQuery>(1000);
    private readonly BoundedDictionary<string, StreamTable> _tables = new BoundedDictionary<string, StreamTable>(1000);

    public override string StrategyId => "analytics-streaming-sql";
    public override string DisplayName => "Streaming SQL Query";
    public override StreamingCategory Category => StreamingCategory.StreamAnalytics;
    public override StreamingDataCapabilities Capabilities => new()
    {
        SupportsExactlyOnce = true,
        SupportsWindowing = true,
        SupportsStateManagement = true,
        SupportsCheckpointing = true,
        SupportsBackpressure = true,
        SupportsPartitioning = true,
        SupportsAutoScaling = true,
        SupportsDistributed = true,
        MaxThroughputEventsPerSec = 200000,
        TypicalLatencyMs = 15.0
    };
    public override string SemanticDescription =>
        "Streaming SQL query engine for continuous queries with SQL syntax, " +
        "windowed aggregations, stream-table joins, and ANSI SQL compatibility.";
    public override string[] Tags => ["sql", "query", "streaming-sql", "continuous-query", "joins"];

    /// <summary>
    /// Registers a stream as a table.
    /// </summary>
    public Task<StreamTable> RegisterTableAsync(
        string tableName,
        TableSchema schema,
        CancellationToken cancellationToken = default)
    {
        var table = new StreamTable
        {
            TableName = tableName,
            Schema = schema,
            CreatedAt = DateTime.UtcNow
        };

        if (!_tables.TryAdd(tableName, table))
            throw new InvalidOperationException($"Table {tableName} already exists");

        RecordOperation("RegisterStreamTable");
        return Task.FromResult(table);
    }

    /// <summary>
    /// Creates a continuous query.
    /// </summary>
    public Task<StreamingQuery> CreateQueryAsync(
        string queryId,
        string sqlQuery,
        StreamingQueryConfig? config = null,
        CancellationToken cancellationToken = default)
    {
        var query = new StreamingQuery
        {
            QueryId = queryId,
            SqlQuery = sqlQuery,
            ParsedQuery = ParseQuery(sqlQuery),
            Config = config ?? new StreamingQueryConfig(),
            State = QueryState.Created,
            CreatedAt = DateTime.UtcNow
        };

        if (!_queries.TryAdd(queryId, query))
            throw new InvalidOperationException($"Query {queryId} already exists");

        RecordOperation("CreateStreamingQuery");
        return Task.FromResult(query);
    }

    /// <summary>
    /// Executes a streaming query.
    /// </summary>
    public async IAsyncEnumerable<QueryResult> ExecuteQueryAsync(
        string queryId,
        IAsyncEnumerable<StreamRow> inputRows,
        [EnumeratorCancellation] CancellationToken cancellationToken = default)
    {
        if (!_queries.TryGetValue(queryId, out var query))
            throw new KeyNotFoundException($"Query {queryId} not found");

        var parsed = query.ParsedQuery;
        var windowBuffer = new List<StreamRow>();

        await foreach (var row in inputRows.WithCancellation(cancellationToken))
        {
            // Apply WHERE filter
            if (!EvaluateWhere(parsed.WhereClause, row))
                continue;

            // Handle windowed queries
            if (parsed.WindowSpec != null)
            {
                windowBuffer.Add(row);

                // Check if window should emit
                if (ShouldEmitWindow(windowBuffer, parsed.WindowSpec))
                {
                    var aggregated = ComputeAggregates(windowBuffer, parsed);
                    foreach (var result in aggregated)
                    {
                        yield return result;
                    }
                    windowBuffer.Clear();
                }
            }
            else
            {
                // Non-windowed: project immediately
                yield return ProjectRow(row, parsed);
            }
        }

        // Emit remaining window buffer
        if (windowBuffer.Count > 0 && parsed.WindowSpec != null)
        {
            var aggregated = ComputeAggregates(windowBuffer, parsed);
            foreach (var result in aggregated)
            {
                yield return result;
            }
        }

        RecordOperation("ExecuteStreamingQuery");
    }

    private ParsedQuery ParseQuery(string sql)
    {
        // Simplified SQL parsing (production would use a proper parser)
        var parsed = new ParsedQuery
        {
            SelectColumns = new List<SelectColumn>(),
            FromTable = "",
            WhereClause = null,
            GroupByColumns = new List<string>(),
            WindowSpec = null
        };

        // Extract SELECT columns
        var selectMatch = System.Text.RegularExpressions.Regex.Match(
            sql, @"SELECT\s+(.+?)\s+FROM", System.Text.RegularExpressions.RegexOptions.IgnoreCase);
        if (selectMatch.Success)
        {
            var columns = selectMatch.Groups[1].Value.Split(',');
            foreach (var col in columns)
            {
                parsed.SelectColumns.Add(ParseSelectColumn(col.Trim()));
            }
        }

        // Extract FROM table
        var fromMatch = System.Text.RegularExpressions.Regex.Match(
            sql, @"FROM\s+(\w+)", System.Text.RegularExpressions.RegexOptions.IgnoreCase);
        if (fromMatch.Success)
        {
            parsed.FromTable = fromMatch.Groups[1].Value;
        }

        // Extract WHERE (simplified)
        var whereMatch = System.Text.RegularExpressions.Regex.Match(
            sql, @"WHERE\s+(.+?)(?:GROUP|WINDOW|$)", System.Text.RegularExpressions.RegexOptions.IgnoreCase);
        if (whereMatch.Success)
        {
            parsed.WhereClause = whereMatch.Groups[1].Value.Trim();
        }

        // Extract GROUP BY
        var groupMatch = System.Text.RegularExpressions.Regex.Match(
            sql, @"GROUP\s+BY\s+(.+?)(?:WINDOW|$)", System.Text.RegularExpressions.RegexOptions.IgnoreCase);
        if (groupMatch.Success)
        {
            parsed.GroupByColumns = groupMatch.Groups[1].Value
                .Split(',')
                .Select(c => c.Trim())
                .ToList();
        }

        // Extract WINDOW
        var windowMatch = System.Text.RegularExpressions.Regex.Match(
            sql, @"WINDOW\s+TUMBLING\s*\(\s*SIZE\s+(\d+)\s+(\w+)\s*\)",
            System.Text.RegularExpressions.RegexOptions.IgnoreCase);
        if (windowMatch.Success)
        {
            var size = int.Parse(windowMatch.Groups[1].Value);
            var unit = windowMatch.Groups[2].Value.ToLower();
            parsed.WindowSpec = new WindowSpec
            {
                Type = WindowType.Tumbling,
                Size = unit switch
                {
                    "seconds" or "second" => TimeSpan.FromSeconds(size),
                    "minutes" or "minute" => TimeSpan.FromMinutes(size),
                    "hours" or "hour" => TimeSpan.FromHours(size),
                    _ => TimeSpan.FromSeconds(size)
                }
            };
        }

        return parsed;
    }

    private SelectColumn ParseSelectColumn(string col)
    {
        // Check for aggregate functions
        var aggMatch = System.Text.RegularExpressions.Regex.Match(
            col, @"(COUNT|SUM|AVG|MIN|MAX)\s*\(\s*(\*|\w+)\s*\)(?:\s+AS\s+(\w+))?",
            System.Text.RegularExpressions.RegexOptions.IgnoreCase);

        if (aggMatch.Success)
        {
            return new SelectColumn
            {
                Expression = col,
                AggregateFunction = Enum.Parse<AggregateFunction>(aggMatch.Groups[1].Value, true),
                SourceColumn = aggMatch.Groups[2].Value,
                Alias = aggMatch.Groups[3].Success ? aggMatch.Groups[3].Value : null
            };
        }

        // Check for alias
        var aliasMatch = System.Text.RegularExpressions.Regex.Match(col, @"(\w+)\s+AS\s+(\w+)",
            System.Text.RegularExpressions.RegexOptions.IgnoreCase);
        if (aliasMatch.Success)
        {
            return new SelectColumn
            {
                Expression = aliasMatch.Groups[1].Value,
                SourceColumn = aliasMatch.Groups[1].Value,
                Alias = aliasMatch.Groups[2].Value
            };
        }

        return new SelectColumn
        {
            Expression = col,
            SourceColumn = col,
            Alias = null
        };
    }

    private bool EvaluateWhere(string? whereClause, StreamRow row)
    {
        if (string.IsNullOrEmpty(whereClause)) return true;

        // Simplified WHERE evaluation
        var match = System.Text.RegularExpressions.Regex.Match(
            whereClause, @"(\w+)\s*(=|>|<|>=|<=|<>)\s*'?([^']+)'?");

        if (!match.Success) return true;

        var column = match.Groups[1].Value;
        var op = match.Groups[2].Value;
        var value = match.Groups[3].Value;

        if (!row.Data.TryGetValue(column, out var actual)) return false;

        return op switch
        {
            "=" => actual?.ToString() == value,
            "<>" => actual?.ToString() != value,
            ">" => CompareValues(actual, value) > 0,
            "<" => CompareValues(actual, value) < 0,
            ">=" => CompareValues(actual, value) >= 0,
            "<=" => CompareValues(actual, value) <= 0,
            _ => true
        };
    }

    private int CompareValues(object? a, string b)
    {
        if (a is IComparable ca)
        {
            if (double.TryParse(b, out var bd) && double.TryParse(a?.ToString(), out var ad))
                return ad.CompareTo(bd);
            return ca.CompareTo(b);
        }
        return 0;
    }

    private bool ShouldEmitWindow(List<StreamRow> buffer, WindowSpec spec)
    {
        if (buffer.Count == 0) return false;
        var firstTime = buffer.First().Timestamp;
        var lastTime = buffer.Last().Timestamp;
        return lastTime - firstTime >= spec.Size;
    }

    private IEnumerable<QueryResult> ComputeAggregates(List<StreamRow> buffer, ParsedQuery parsed)
    {
        // Group by columns
        var groups = parsed.GroupByColumns.Count > 0
            ? buffer.GroupBy(r => string.Join(":", parsed.GroupByColumns.Select(c =>
                r.Data.TryGetValue(c, out var v) ? v?.ToString() : "")))
            : new[] { buffer.AsEnumerable() }.Cast<IGrouping<string, StreamRow>>();

        foreach (var group in groups)
        {
            var resultData = new Dictionary<string, object>();

            // Add group by values
            var firstRow = group.First();
            foreach (var col in parsed.GroupByColumns)
            {
                if (firstRow.Data.TryGetValue(col, out var v))
                    resultData[col] = v;
            }

            // Compute aggregates
            foreach (var selCol in parsed.SelectColumns)
            {
                if (selCol.AggregateFunction.HasValue)
                {
                    var values = group.Select(r =>
                        r.Data.TryGetValue(selCol.SourceColumn ?? "", out var v) ? v : null).ToList();

                    var aggResult = selCol.AggregateFunction.Value switch
                    {
                        AggregateFunction.Count => (object)values.Count,
                        AggregateFunction.Sum => values.Where(v => v != null).Sum(v => ToDouble(v)),
                        AggregateFunction.Avg => values.Where(v => v != null).Average(v => ToDouble(v)),
                        AggregateFunction.Min => values.Where(v => v != null).Min(v => ToDouble(v)),
                        AggregateFunction.Max => values.Where(v => v != null).Max(v => ToDouble(v)),
                        _ => null
                    };

                    var key = selCol.Alias ?? $"{selCol.AggregateFunction}_{selCol.SourceColumn}";
                    if (aggResult != null) resultData[key] = aggResult;
                }
            }

            yield return new QueryResult
            {
                QueryId = "",
                Data = resultData,
                WindowStart = group.Min(r => r.Timestamp),
                WindowEnd = group.Max(r => r.Timestamp),
                EmittedAt = DateTime.UtcNow
            };
        }
    }

    private double ToDouble(object? v)
    {
        return v switch
        {
            double d => d,
            int i => i,
            long l => l,
            float f => f,
            string s when double.TryParse(s, out var p) => p,
            _ => 0
        };
    }

    private QueryResult ProjectRow(StreamRow row, ParsedQuery parsed)
    {
        var resultData = new Dictionary<string, object>();

        foreach (var selCol in parsed.SelectColumns)
        {
            if (selCol.SourceColumn == "*")
            {
                foreach (var kvp in row.Data)
                    resultData[kvp.Key] = kvp.Value;
            }
            else if (row.Data.TryGetValue(selCol.SourceColumn ?? "", out var val))
            {
                resultData[selCol.Alias ?? selCol.SourceColumn!] = val;
            }
        }

        return new QueryResult
        {
            QueryId = "",
            Data = resultData,
            EmittedAt = DateTime.UtcNow
        };
    }
}

#endregion

#region Supporting Types for Stream Analytics

// Aggregation Types
public enum JobState { Created, Running, Paused, Stopped, Failed }
public enum TriggerMode { EveryEvent, CountBased, TimeBased, OnWindowClose }

public record AggregationJob
{
    public required string JobId { get; init; }
    public required AggregationDefinition Definition { get; init; }
    public required AggregationJobConfig Config { get; init; }
    public JobState State { get; set; }
    public DateTime CreatedAt { get; init; }
}

public record AggregationDefinition
{
    public List<string>? GroupByFields { get; init; }
    public string? ValueField { get; init; }
    public List<AggregateType>? Aggregations { get; init; }
}

public enum AggregateType { Count, Sum, Avg, Min, Max, First, Last }

public record AggregationJobConfig
{
    public TriggerMode TriggerMode { get; init; } = TriggerMode.EveryEvent;
    public int EmitEveryN { get; init; } = 100;
    public TimeSpan EmitInterval { get; init; } = TimeSpan.FromSeconds(10);
}

public record AggregationState
{
    public required string JobId { get; init; }
    public required BoundedDictionary<string, AggregateValue> Aggregates { get; init; }
}

public class AggregateValue
{
    public required string GroupKey { get; init; }
    public long Count { get; set; }
    public double Sum { get; set; }
    public double Min { get; set; }
    public double Max { get; set; }
    public DateTime? WindowStart { get; set; }
    public DateTime LastUpdated { get; set; }
    public DateTime LastEmitted { get; set; }
}

public record StreamEvent
{
    public required string EventId { get; init; }
    public required DateTime Timestamp { get; init; }
    public Dictionary<string, object>? Data { get; init; }
}

public record AggregationResult
{
    public required string JobId { get; init; }
    public required string GroupKey { get; init; }
    public long Count { get; init; }
    public double Sum { get; init; }
    public double Avg { get; init; }
    public double Min { get; init; }
    public double Max { get; init; }
    public DateTime? WindowStart { get; init; }
    public DateTime? WindowEnd { get; init; }
    public DateTime EmittedAt { get; init; }
}

// CEP Types
public enum EngineState { Created, Running, Stopped }

public record CepEngine
{
    public required string EngineId { get; init; }
    public required CepEngineConfig Config { get; init; }
    public EngineState State { get; set; }
    public DateTime CreatedAt { get; init; }
}

public record CepEngineConfig
{
    public TimeSpan MatchTimeout { get; init; } = TimeSpan.FromMinutes(5);
    public int MaxPartialMatches { get; init; } = 10000;
}

public record CepPattern
{
    public required string PatternId { get; init; }
    public required string EngineId { get; init; }
    public required PatternDefinition Definition { get; init; }
    public bool IsActive { get; set; }
    public DateTime CreatedAt { get; init; }
}

public record PatternDefinition
{
    public required List<PatternStep> Steps { get; init; }
    public TimeSpan? WithinTime { get; init; }
}

public record PatternStep
{
    public required string StepName { get; init; }
    public required PatternCondition Condition { get; init; }
    public Quantifier Quantifier { get; init; } = Quantifier.One;
}

public enum Quantifier { One, ZeroOrOne, ZeroOrMore, OneOrMore }

public record PatternCondition
{
    public string? EventType { get; init; }
    public List<FieldCondition>? FieldConditions { get; init; }
}

public record FieldCondition
{
    public required string FieldName { get; init; }
    public required ComparisonOperator Operator { get; init; }
    public object? Value { get; init; }
}

public enum ComparisonOperator { Equals, NotEquals, GreaterThan, LessThan, GreaterOrEqual, LessOrEqual, Contains }

public record CepEvent
{
    public required string EventId { get; init; }
    public required string EventType { get; init; }
    public required DateTime Timestamp { get; init; }
    public Dictionary<string, object>? Data { get; init; }
}

public class PartialMatch
{
    public required string PatternId { get; init; }
    public required List<CepEvent> Events { get; init; }
    public int CurrentStep { get; init; }
    public DateTime StartedAt { get; init; }
}

public record PatternMatch
{
    public required string PatternId { get; init; }
    public required string EngineId { get; init; }
    public required List<CepEvent> MatchedEvents { get; init; }
    public DateTime StartTime { get; init; }
    public DateTime EndTime { get; init; }
    public DateTime MatchedAt { get; init; }
}

// ML Inference Types
public enum ModelType { Classification, Regression, Clustering, AnomalyDetection, Recommendation }
public enum ModelState { Loading, Loaded, Failed, Unloaded }
public enum PredictionType { Classification, Regression, Clustering, AnomalyDetection, Unknown }
public enum TransformType { Normalize, MinMaxScale, Log, Square, Sqrt }

public record MlModel
{
    public required string ModelId { get; init; }
    public required ModelType ModelType { get; init; }
    public required string ModelPath { get; init; }
    public required ModelConfig Config { get; init; }
    public ModelState State { get; set; }
    public DateTime LoadedAt { get; init; }
}

public record ModelConfig
{
    public int BatchSize { get; init; } = 32;
    public bool UseGpu { get; init; } = false;
    public Dictionary<string, FeatureTransform>? FeatureTransforms { get; init; }
}

public record FeatureTransform
{
    public required TransformType Type { get; init; }
    public double Mean { get; init; }
    public double Std { get; init; } = 1.0;
    public double Min { get; init; }
    public double Max { get; init; } = 1.0;
}

public record InferenceJob
{
    public required string JobId { get; init; }
    public required string ModelId { get; init; }
    public required InferenceJobConfig Config { get; init; }
    public JobState State { get; set; }
    public DateTime CreatedAt { get; init; }
}

public record InferenceJobConfig
{
    public int MaxConcurrency { get; init; } = 4;
    public TimeSpan Timeout { get; init; } = TimeSpan.FromSeconds(30);
}

public record InferenceRequest
{
    public required string RequestId { get; init; }
    public Dictionary<string, object>? Features { get; init; }
}

public record InferenceResult
{
    public required string RequestId { get; init; }
    public required string ModelId { get; init; }
    public Prediction? Prediction { get; init; }
    public double? Confidence { get; init; }
    public double LatencyMs { get; init; }
    public string? Error { get; init; }
    public DateTime InferredAt { get; init; }
}

public record Prediction
{
    public required PredictionType Type { get; init; }
    public string? Label { get; init; }
    public double? Value { get; init; }
    public int? ClusterId { get; init; }
    public bool? IsAnomaly { get; init; }
    public double? AnomalyScore { get; init; }
    public double Confidence { get; init; }
    public Dictionary<string, double>? Probabilities { get; init; }
}

public class ModelMetrics
{
    public required string ModelId { get; init; }
    public long TotalPredictions;
    public long TotalErrors;
    public double AvgLatencyMs { get; set; }
}

// Time Series Types
public enum AnalyzerState { Active, Paused, Stopped }

public record TimeSeriesAnalyzer
{
    public required string AnalyzerId { get; init; }
    public required TimeSeriesConfig Config { get; init; }
    public AnalyzerState State { get; set; }
    public DateTime CreatedAt { get; init; }
}

public record TimeSeriesConfig
{
    public int BufferSize { get; init; } = 1000;
    public int MinDataPoints { get; init; } = 30;
    public int TrendWindow { get; init; } = 10;
    public int SeasonalPeriod { get; init; } = 24;
    public double AnomalyThreshold { get; init; } = 3.0;
}

public record TimeSeriesPoint
{
    public required DateTime Timestamp { get; init; }
    public required double Value { get; init; }
    public Dictionary<string, object>? Labels { get; init; }
}

public record TimeSeriesResult
{
    public required string AnalyzerId { get; init; }
    public required DateTime Timestamp { get; init; }
    public double CurrentValue { get; init; }
    public double Trend { get; init; }
    public double SeasonalComponent { get; init; }
    public double Residual { get; init; }
    public bool IsAnomaly { get; init; }
    public double AnomalyScore { get; init; }
    public double Forecast { get; init; }
    public (double Lower, double Upper) ConfidenceInterval { get; init; }
}

public record TimeSeriesAnalysis
{
    public double Trend { get; init; }
    public double SeasonalComponent { get; init; }
    public double Residual { get; init; }
    public bool IsAnomaly { get; init; }
    public double AnomalyScore { get; init; }
    public double Forecast { get; init; }
    public (double Lower, double Upper) ConfidenceInterval { get; init; }
}

// Streaming SQL Types
public enum QueryState { Created, Running, Paused, Stopped }
public enum WindowType { Tumbling, Sliding, Session }
public enum AggregateFunction { Count, Sum, Avg, Min, Max }

public record StreamingQuery
{
    public required string QueryId { get; init; }
    public required string SqlQuery { get; init; }
    public required ParsedQuery ParsedQuery { get; init; }
    public required StreamingQueryConfig Config { get; init; }
    public QueryState State { get; set; }
    public DateTime CreatedAt { get; init; }
}

public record StreamingQueryConfig
{
    public TimeSpan EmitDelay { get; init; } = TimeSpan.Zero;
    public bool AllowLateEvents { get; init; } = true;
    public TimeSpan LateEventThreshold { get; init; } = TimeSpan.FromMinutes(5);
}

public class ParsedQuery
{
    public required List<SelectColumn> SelectColumns { get; init; }
    public string FromTable { get; set; } = "";
    public string? WhereClause { get; set; }
    public List<string> GroupByColumns { get; set; } = new();
    public WindowSpec? WindowSpec { get; set; }
}

public record SelectColumn
{
    public required string Expression { get; init; }
    public string? SourceColumn { get; init; }
    public string? Alias { get; init; }
    public AggregateFunction? AggregateFunction { get; init; }
}

public record WindowSpec
{
    public required WindowType Type { get; init; }
    public required TimeSpan Size { get; init; }
    public TimeSpan? Slide { get; init; }
    public TimeSpan? Gap { get; init; }
}

public record StreamTable
{
    public required string TableName { get; init; }
    public required TableSchema Schema { get; init; }
    public DateTime CreatedAt { get; init; }
}

public record TableSchema
{
    public required List<ColumnDefinition> Columns { get; init; }
    public string? TimestampColumn { get; init; }
    public List<string>? KeyColumns { get; init; }
}

public record ColumnDefinition
{
    public required string Name { get; init; }
    public required ColumnType Type { get; init; }
    public bool Nullable { get; init; } = true;
}

public enum ColumnType { String, Int, Long, Double, Boolean, Timestamp, Binary }

public record StreamRow
{
    public required DateTime Timestamp { get; init; }
    public required Dictionary<string, object> Data { get; init; }
}

public record QueryResult
{
    public required string QueryId { get; init; }
    public required Dictionary<string, object> Data { get; init; }
    public DateTime? WindowStart { get; init; }
    public DateTime? WindowEnd { get; init; }
    public DateTime EmittedAt { get; init; }
}

#endregion
