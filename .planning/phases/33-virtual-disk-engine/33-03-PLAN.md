---
phase: 33-virtual-disk-engine
plan: 03
type: execute
wave: 2
depends_on: ["33-01"]
files_modified:
  - DataWarehouse.SDK/VirtualDiskEngine/Journal/IWriteAheadLog.cs
  - DataWarehouse.SDK/VirtualDiskEngine/Journal/WriteAheadLog.cs
  - DataWarehouse.SDK/VirtualDiskEngine/Journal/JournalEntry.cs
  - DataWarehouse.SDK/VirtualDiskEngine/Journal/CheckpointManager.cs
  - DataWarehouse.SDK/VirtualDiskEngine/Journal/WalTransaction.cs
autonomous: true

must_haves:
  truths:
    - "WAL entries are written to disk before corresponding data block modifications"
    - "After a simulated crash at any point, WAL replay restores consistent state"
    - "Checkpoint mechanism bounds WAL size by flushing dirty data and advancing the tail"
    - "Multiple operations can be grouped into an atomic transaction"
    - "WAL uses a circular buffer within a reserved block range in the container"
    - "Sequential WAL write performance is within 10% of raw block device speed"
  artifacts:
    - path: "DataWarehouse.SDK/VirtualDiskEngine/Journal/IWriteAheadLog.cs"
      provides: "WAL interface for begin/commit/abort transactions and crash recovery"
      exports: ["IWriteAheadLog"]
    - path: "DataWarehouse.SDK/VirtualDiskEngine/Journal/WriteAheadLog.cs"
      provides: "WAL implementation with sequential writes and circular buffer"
      exports: ["WriteAheadLog"]
    - path: "DataWarehouse.SDK/VirtualDiskEngine/Journal/JournalEntry.cs"
      provides: "On-disk journal entry structure with sequence numbers and before/after images"
      exports: ["JournalEntry", "JournalEntryType"]
    - path: "DataWarehouse.SDK/VirtualDiskEngine/Journal/CheckpointManager.cs"
      provides: "Checkpoint creation, WAL tail advancement, dirty data flush"
      exports: ["CheckpointManager"]
    - path: "DataWarehouse.SDK/VirtualDiskEngine/Journal/WalTransaction.cs"
      provides: "Transaction grouping for atomic multi-block operations"
      exports: ["WalTransaction"]
  key_links:
    - from: "WriteAheadLog"
      to: "IBlockDevice"
      via: "WAL writes journal entries to reserved block range"
      pattern: "IBlockDevice.*WriteBlockAsync"
    - from: "CheckpointManager"
      to: "WriteAheadLog"
      via: "Checkpoint flushes dirty data and advances WAL tail"
      pattern: "WriteAheadLog.*Checkpoint|Truncate"
    - from: "WalTransaction"
      to: "WriteAheadLog"
      via: "Transaction appends entries and commits/aborts atomically"
      pattern: "WriteAheadLog.*Append|Commit"
---

<objective>
Build the Write-Ahead Log (WAL) subsystem with crash recovery for the Virtual Disk Engine.

Purpose: The WAL ensures crash safety -- any power loss or abnormal shutdown during a write results in a consistent state after recovery. This is the foundation that makes all other VDE subsystems (B-Tree modifications, CoW operations, inode updates) safe against corruption.

Output: IWriteAheadLog interface, WriteAheadLog implementation, JournalEntry structure, CheckpointManager, and WalTransaction for atomic operations.
</objective>

<execution_context>
@C:/Users/ddamien/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/ddamien/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/33-virtual-disk-engine/33-RESEARCH.md
@.planning/phases/33-virtual-disk-engine/33-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: JournalEntry, WalTransaction, and IWriteAheadLog interface</name>
  <files>
    DataWarehouse.SDK/VirtualDiskEngine/Journal/JournalEntry.cs
    DataWarehouse.SDK/VirtualDiskEngine/Journal/WalTransaction.cs
    DataWarehouse.SDK/VirtualDiskEngine/Journal/IWriteAheadLog.cs
  </files>
  <action>
    Create the WAL data structures and interface in DataWarehouse.SDK/VirtualDiskEngine/Journal/:

    **JournalEntry.cs**: On-disk journal entry structure:
    - `JournalEntryType` enum: BeginTransaction, BlockWrite, BlockFree, InodeUpdate, BTreeModify, CommitTransaction, AbortTransaction, Checkpoint
    - `JournalEntry` class with:
      - `long SequenceNumber` (monotonically increasing, never reused)
      - `long TransactionId` (groups related entries)
      - `JournalEntryType Type`
      - `long TargetBlockNumber` (which data block this entry affects; -1 for transaction markers)
      - `int DataLength` (length of before/after image data)
      - `byte[]? BeforeImage` (original block data before modification -- for undo)
      - `byte[]? AfterImage` (new block data after modification -- for redo)
      - `ulong Checksum` (XxHash64 of all fields except checksum itself)
    - On-disk format: [SequenceNumber:8][TransactionId:8][Type:1][TargetBlock:8][DataLength:4][BeforeImage:N][AfterImage:N][Checksum:8]
    - `Serialize(Span<byte> buffer)` returns bytes written. Variable size (header = 37 bytes + data).
    - `static Deserialize(ReadOnlySpan<byte> buffer, out JournalEntry entry, out int bytesRead)` with checksum validation.
    - `static int HeaderSize => 37` (fixed header before variable data)
    - Entries may span multiple blocks. The WAL packs entries sequentially with no padding between entries (except block boundary alignment for commit markers).

    **WalTransaction.cs**: Transaction grouping:
    - Constructor takes `long transactionId, IWriteAheadLog wal`
    - `LogBlockWriteAsync(long blockNumber, ReadOnlyMemory<byte> beforeImage, ReadOnlyMemory<byte> afterImage, CancellationToken ct)` -- records a block modification
    - `LogBlockFreeAsync(long blockNumber, ReadOnlyMemory<byte> beforeImage, CancellationToken ct)` -- records a block deallocation
    - `LogInodeUpdateAsync(long blockNumber, ReadOnlyMemory<byte> beforeImage, ReadOnlyMemory<byte> afterImage, CancellationToken ct)` -- records an inode change
    - `LogBTreeModifyAsync(long blockNumber, ReadOnlyMemory<byte> beforeImage, ReadOnlyMemory<byte> afterImage, CancellationToken ct)` -- records a B-Tree structural change
    - `CommitAsync(CancellationToken ct)` -- writes commit marker, flushes WAL, then applies all after-images to actual data blocks. Commit marker flush is the linearization point.
    - `AbortAsync(CancellationToken ct)` -- writes abort marker, discards pending entries
    - Implements IAsyncDisposable (aborts if not committed)
    - Tracks list of pending entries for the transaction

    **IWriteAheadLog.cs**: Interface:
    - `Task<WalTransaction> BeginTransactionAsync(CancellationToken ct = default)` -- starts a new transaction, returns handle
    - `Task AppendEntryAsync(JournalEntry entry, CancellationToken ct = default)` -- appends entry to WAL
    - `Task FlushAsync(CancellationToken ct = default)` -- ensures all pending entries are on disk
    - `Task<IReadOnlyList<JournalEntry>> ReplayAsync(CancellationToken ct = default)` -- reads all committed-but-not-checkpointed entries for crash recovery
    - `Task CheckpointAsync(CancellationToken ct = default)` -- marks all current entries as applied, advances tail
    - `long CurrentSequenceNumber { get; }`
    - `long WalSizeBlocks { get; }`
    - `double WalUtilization { get; }` -- percentage of WAL area used (trigger checkpoint at 75%)
    - `bool NeedsRecovery { get; }` -- true if WAL has uncommitted or unapplied entries on open

    All public APIs have XML docs. Namespace: DataWarehouse.SDK.VirtualDiskEngine.Journal.
  </action>
  <verify>
    Run `dotnet build DataWarehouse.SDK/DataWarehouse.SDK.csproj` -- zero new errors.
  </verify>
  <done>
    JournalEntry structure with sequence numbers, transaction IDs, before/after images, and XxHash64 checksum. WalTransaction groups multiple operations into atomic units with commit/abort. IWriteAheadLog interface defines begin/append/flush/replay/checkpoint contract.
  </done>
</task>

<task type="auto">
  <name>Task 2: WriteAheadLog and CheckpointManager implementation</name>
  <files>
    DataWarehouse.SDK/VirtualDiskEngine/Journal/WriteAheadLog.cs
    DataWarehouse.SDK/VirtualDiskEngine/Journal/CheckpointManager.cs
  </files>
  <action>
    Implement the WAL engine:

    **WriteAheadLog.cs**: Full WAL implementation:
    - Constructor takes `IBlockDevice device, long walStartBlock, long walBlockCount`
    - **Circular buffer**: WAL occupies a fixed range of blocks [walStartBlock, walStartBlock + walBlockCount). Head pointer advances on append. Tail pointer advances on checkpoint. Wrap around when reaching end.
    - **State tracking**: `_headBlock` (next write position), `_tailBlock` (oldest unreclaimed entry), `_nextSequenceNumber` (monotonic counter), `_nextTransactionId` (monotonic counter)
    - **WAL header block**: First block of WAL area stores WAL metadata: headBlock, tailBlock, nextSequenceNumber, lastCheckpointSequence, checksum. Written on every flush.
    - `AppendEntryAsync()`: Serialize entry, write to blocks starting at head. If entry spans a block boundary, write continuation into next block. Advance head. Do NOT flush automatically (batching for performance).
    - `FlushAsync()`: Write WAL header block, then `device.FlushAsync()`. This is the durability point.
    - `BeginTransactionAsync()`: Allocate new transactionId, append BeginTransaction entry, return WalTransaction handle.
    - **Commit flow**: WalTransaction.CommitAsync() appends CommitTransaction marker, calls WAL FlushAsync (linearization point), then applies all after-images to their target blocks via the block device, then calls device.FlushAsync() again for data durability.
    - `ReplayAsync()`: Called on container open. Scan from tail to head. For each committed transaction (has CommitTransaction marker), collect after-images. For each uncommitted transaction (BeginTransaction without CommitTransaction), discard. Return list of committed entries to apply.
    - **Recovery**: After replay, apply all committed after-images to their target blocks (redo). Discard uncommitted transactions. Update WAL header.
    - **WAL full protection**: If appending would overwrite tail, trigger automatic checkpoint first. If checkpoint fails (dirty data can't be flushed), return error (WAL full).
    - Thread safety: `SemaphoreSlim` for append serialization. Multiple concurrent transactions allowed but serialized at append level.
    - Use ArrayPool for all block buffers.

    **CheckpointManager.cs**: Checkpoint coordination:
    - Constructor takes `WriteAheadLog wal, IBlockDevice device`
    - `CheckpointAsync()`:
      1. Record current head sequence number as checkpoint target
      2. Flush device (ensure all data blocks that were written after WAL commits are on disk)
      3. Advance WAL tail to current head (all entries before this point are fully applied)
      4. Write updated WAL header
      5. Update superblock checkpoint timestamp and sequence
    - `ShouldCheckpoint()` -> bool: Returns true when WAL utilization > 75%
    - `AutoCheckpointIfNeededAsync()`: Called after each commit. If ShouldCheckpoint(), triggers checkpoint.
    - Checkpoint is idempotent: running it twice has no effect if no new entries.

    Write ordering guarantees (CRITICAL):
    1. WAL entry written to WAL blocks
    2. WAL flushed (entry durably on disk)
    3. Data blocks modified (after-images applied)
    4. Data flushed
    5. Checkpoint can now reclaim WAL entry

    If crash occurs at step 1-2: WAL entry lost, data unchanged -- consistent.
    If crash occurs at step 3: WAL entry durable, data partially applied -- replay fixes it.
    If crash occurs at step 4: WAL entry durable, data applied -- checkpoint will reclaim.

    All public APIs have XML docs.
  </action>
  <verify>
    Run `dotnet build DataWarehouse.SDK/DataWarehouse.SDK.csproj` -- zero new errors. Verify WriteAheadLog implements IWriteAheadLog.
  </verify>
  <done>
    WriteAheadLog provides circular-buffer WAL with sequential append, flush, and replay. Commit flow ensures WAL entry is durable before data modification. Recovery replays committed transactions and discards uncommitted ones. CheckpointManager advances WAL tail after data durability, with auto-checkpoint at 75% utilization.
  </done>
</task>

</tasks>

<verification>
- `dotnet build DataWarehouse.SDK/DataWarehouse.SDK.csproj` compiles with zero new errors
- All files exist under DataWarehouse.SDK/VirtualDiskEngine/Journal/
- WAL uses circular buffer in reserved block range
- Journal entries have monotonic sequence numbers and XxHash64 checksums
- Write ordering: WAL entry durable before data block modification
- Commit marker flush is the linearization point
- Replay scans for committed transactions, discards uncommitted
- Checkpoint advances tail after data durability confirmed
</verification>

<success_criteria>
- WAL entries written and flushed before corresponding data modifications
- Transaction grouping: multiple operations commit or abort atomically
- Crash recovery: replay committed entries, discard uncommitted
- Circular buffer manages WAL space with automatic checkpoint at 75%
- WAL header persists head/tail/sequence for recovery
- CheckpointManager flushes data and advances WAL tail
- Zero new build errors
</success_criteria>

<output>
After completion, create `.planning/phases/33-virtual-disk-engine/33-03-SUMMARY.md`
</output>
