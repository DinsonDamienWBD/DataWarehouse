---
phase: 10-advanced-storage
plan: 06
type: execute
wave: 1
depends_on: []
files_modified:
  - Plugins/DataWarehouse.Plugins.UltimateStorage/Strategies/Innovation/ProbabilisticStorageStrategy.cs
  - Plugins/DataWarehouse.Plugins.UltimateStorage/UltimateStoragePlugin.cs
  - Metadata/TODO.md
autonomous: true

must_haves:
  truths:
    - "Count-Min Sketch provides approximate frequency counts with bounded error"
    - "HyperLogLog estimates cardinality of massive datasets in constant memory"
    - "Bloom filters test set membership with probabilistic guarantees"
    - "t-digest computes percentiles for streaming data"
    - "Probabilistic structures use 10-1000x less memory than exact data structures"
  artifacts:
    - path: "Plugins/DataWarehouse.Plugins.UltimateStorage/Strategies/Innovation/ProbabilisticStorageStrategy.cs"
      provides: "Memory-efficient analytics using probabilistic data structures"
      min_lines: 200
    - path: "DataWarehouse.SDK/Primitives/ProbabilisticDataStructures.cs"
      provides: "SDK primitives for Count-Min, HyperLogLog, Bloom, t-digest"
      pattern: "class CountMinSketch|class HyperLogLog|class BloomFilter|class TDigest"
  key_links:
    - from: "ProbabilisticStorageStrategy"
      to: "SDK primitives"
      via: "uses probabilistic data structures from SDK"
      pattern: "CountMinSketch|HyperLogLog|BloomFilter|TDigest"
    - from: "ProbabilisticStorageStrategy"
      to: "message bus"
      via: "query operations for analytics"
      pattern: "PublishAsync.*storage\\.query"
---

<objective>
Verify and complete probabilistic data structures (T85) for memory-efficient analytics.

Purpose: Enable analytics on massive datasets using probabilistic data structures (Count-Min Sketch, HyperLogLog, Bloom filters, t-digest) that trade perfect accuracy for dramatic memory savings.
Output: Production-ready ProbabilisticStorageStrategy with SDK primitives and integration into UltimateStorage plugin.
</objective>

<execution_context>
@C:/Users/ddamien/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/ddamien/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/10-advanced-storage/10-RESEARCH.md
@Plugins/DataWarehouse.Plugins.UltimateStorage/Strategies/Innovation/ProbabilisticStorageStrategy.cs
</context>

<tasks>

<task type="auto">
  <name>Task 1: Verify ProbabilisticStorageStrategy and SDK primitives</name>
  <files>
Plugins/DataWarehouse.Plugins.UltimateStorage/Strategies/Innovation/ProbabilisticStorageStrategy.cs
DataWarehouse.SDK/Primitives/ProbabilisticDataStructures.cs
Plugins/DataWarehouse.Plugins.UltimateStorage/UltimateStoragePlugin.cs
  </files>
  <action>
**Verification approach:** Research indicates ProbabilisticStorageStrategy.cs exists. Verify T85 probabilistic structures are implemented in both SDK and plugin.

**Step 1: Check SDK primitives**
Read `DataWarehouse.SDK/Primitives/ProbabilisticDataStructures.cs` (or check if it exists):
```bash
ls DataWarehouse.SDK/Primitives/Probabilistic*.cs
```

Verify SDK contains these classes:
- **CountMinSketch:** Approximate frequency counts with configurable error bounds
- **HyperLogLog:** Cardinality estimation (count distinct)
- **BloomFilter:** Set membership testing with false positive rate
- **TDigest:** Percentile computation for streaming data

**If SDK primitives exist:**
Read file and verify production-ready implementations (no NotImplementedException).

**If SDK primitives don't exist:**
Document as gap - these are SDK-level primitives per TODO.md T99.

**Step 2: Read ProbabilisticStorageStrategy.cs**
Verify file exists and check for all T85 sub-tasks:

**T85 Sub-Tasks (from TODO.md):**
- **85.1 Count-Min Sketch:** Frequency counting (top-K queries, heavy hitters)
- **85.2 HyperLogLog:** Cardinality estimation (unique visitor count)
- **85.3 Bloom Filter:** Membership testing (deduplication, exists checks)
- **85.4 t-digest:** Percentile computation (P50, P95, P99)
- **85.5 Query Interface:** Unified query API for probabilistic analytics
- **85.6 Error Bounds:** Configurable error rates and confidence intervals
- **85.7 Memory Profiling:** Track memory savings vs exact structures
- **85.8 Serialization:** Persist probabilistic structures efficiently

**Step 3: Verify integration patterns**
Check how ProbabilisticStorageStrategy uses SDK primitives:
```csharp
public sealed class ProbabilisticStorageStrategy : UltimateStorageStrategyBase
{
    private readonly CountMinSketch _frequencySketch;
    private readonly HyperLogLog _cardinalityEstimator;
    private readonly BloomFilter _membershipFilter;
    private readonly TDigest _percentileDigest;

    public async Task<long> EstimateUniqueCountAsync(string dataset)
    {
        return _cardinalityEstimator.Estimate();
    }

    public async Task<double> GetPercentileAsync(string metric, double percentile)
    {
        return _percentileDigest.Quantile(percentile);
    }

    // etc.
}
```

**Step 4: Verify error bounds configuration**
Check for configurable error rates:
```csharp
public class ProbabilisticConfig
{
    public double BloomFilterFalsePositiveRate { get; set; } = 0.01;  // 1%
    public double CountMinSketchErrorRate { get; set; } = 0.001;  // 0.1%
    public int HyperLogLogPrecision { get; set; } = 14;  // 2^14 registers
}
```

**Step 5: Verify strategy registration**
Check UltimateStoragePlugin.cs for ProbabilisticStorageStrategy registration:
```bash
grep "ProbabilisticStorageStrategy" Plugins/DataWarehouse.Plugins.UltimateStorage/UltimateStoragePlugin.cs
```

**Step 6: Build verification**
```bash
dotnet build DataWarehouse.SDK/ --no-incremental
dotnet build Plugins/DataWarehouse.Plugins.UltimateStorage/ --no-incremental
```
Expected: 0 errors

**Step 7: Document findings**
Create verification table:
| Component | Location | Status | Notes |
|-----------|----------|--------|-------|
| CountMinSketch | SDK/Primitives | ✅/❌ | ... |
| HyperLogLog | SDK/Primitives | ✅/❌ | ... |
| BloomFilter | SDK/Primitives | ✅/❌ | ... |
| TDigest | SDK/Primitives | ✅/❌ | ... |
| ProbabilisticStorageStrategy | UltimateStorage | ✅/❌ | ... |
  </action>
  <verify>
- SDK primitives exist and compile (or gaps documented)
- ProbabilisticStorageStrategy.cs exists and compiles
- All 8 T85 sub-tasks verified present
- Build passes with 0 errors
- Verification table completed
  </verify>
  <done>
T85 probabilistic data structures implementation status is fully documented. SDK primitives and storage strategy are verified production-ready or gaps are identified for Task 2.
  </done>
</task>

<task type="auto">
  <name>Task 2: Implement missing components and update TODO.md</name>
  <files>
DataWarehouse.SDK/Primitives/ProbabilisticDataStructures.cs
Plugins/DataWarehouse.Plugins.UltimateStorage/Strategies/Innovation/ProbabilisticStorageStrategy.cs
Plugins/DataWarehouse.Plugins.UltimateStorage/UltimateStoragePlugin.cs
Metadata/TODO.md
  </files>
  <action>
**Based on Task 1 verification**, implement missing components.

**If all components are verified complete:**
- Mark all T85.1-85.8 sub-tasks as [x] in Metadata/TODO.md
- Update T85 status from [ ] Not Started to [x] Complete
- Skip to verification

**If SDK primitives are missing:**

**Create DataWarehouse.SDK/Primitives/ProbabilisticDataStructures.cs:**

```csharp
namespace DataWarehouse.SDK.Primitives;

/// <summary>
/// Count-Min Sketch for approximate frequency counting.
/// </summary>
public sealed class CountMinSketch
{
    private readonly int[][] _table;
    private readonly int _width;
    private readonly int _depth;

    public CountMinSketch(double errorRate = 0.001, double confidence = 0.99)
    {
        _width = (int)Math.Ceiling(Math.E / errorRate);
        _depth = (int)Math.Ceiling(Math.Log(1.0 / (1.0 - confidence)));
        _table = new int[_depth][];
        for (int i = 0; i < _depth; i++)
            _table[i] = new int[_width];
    }

    public void Add(string item, int count = 1)
    {
        for (int i = 0; i < _depth; i++)
        {
            int hash = Hash(item, i);
            int index = Math.Abs(hash % _width);
            _table[i][index] += count;
        }
    }

    public long Estimate(string item)
    {
        long min = long.MaxValue;
        for (int i = 0; i < _depth; i++)
        {
            int hash = Hash(item, i);
            int index = Math.Abs(hash % _width);
            min = Math.Min(min, _table[i][index]);
        }
        return min;
    }

    private int Hash(string item, int seed) =>
        HashCode.Combine(item, seed);
}

/// <summary>
/// HyperLogLog for cardinality estimation.
/// </summary>
public sealed class HyperLogLog
{
    private readonly byte[] _registers;
    private readonly int _precision;
    private readonly double _alphaMM;

    public HyperLogLog(int precision = 14)
    {
        _precision = precision;
        int m = 1 << precision;
        _registers = new byte[m];
        _alphaMM = GetAlpha(m) * m * m;
    }

    public void Add(string item)
    {
        int hash = item.GetHashCode();
        int j = hash & ((1 << _precision) - 1);  // First p bits
        int w = hash >> _precision;              // Remaining bits
        _registers[j] = Math.Max(_registers[j], (byte)(LeadingZeroes(w) + 1));
    }

    public long Estimate()
    {
        double rawEstimate = _alphaMM / _registers.Sum(r => 1.0 / (1 << r));

        // Small range correction
        if (rawEstimate <= 2.5 * _registers.Length)
        {
            int zeros = _registers.Count(r => r == 0);
            if (zeros != 0)
                return (long)(_registers.Length * Math.Log(_registers.Length / (double)zeros));
        }

        return (long)rawEstimate;
    }

    private static double GetAlpha(int m) => m switch
    {
        16 => 0.673,
        32 => 0.697,
        64 => 0.709,
        _ => 0.7213 / (1 + 1.079 / m)
    };

    private static int LeadingZeroes(int value)
    {
        if (value == 0) return 32;
        int count = 0;
        while ((value & 0x80000000) == 0) { value <<= 1; count++; }
        return count;
    }
}

/// <summary>
/// Bloom Filter for probabilistic set membership.
/// </summary>
public sealed class BloomFilter
{
    private readonly BitArray _bits;
    private readonly int _hashCount;

    public BloomFilter(int expectedItems, double falsePositiveRate = 0.01)
    {
        int bitCount = (int)Math.Ceiling(-expectedItems * Math.Log(falsePositiveRate) / (Math.Log(2) * Math.Log(2)));
        _bits = new BitArray(bitCount);
        _hashCount = (int)Math.Ceiling(bitCount / (double)expectedItems * Math.Log(2));
    }

    public void Add(string item)
    {
        for (int i = 0; i < _hashCount; i++)
        {
            int hash = HashCode.Combine(item, i);
            int index = Math.Abs(hash % _bits.Length);
            _bits[index] = true;
        }
    }

    public bool MayContain(string item)
    {
        for (int i = 0; i < _hashCount; i++)
        {
            int hash = HashCode.Combine(item, i);
            int index = Math.Abs(hash % _bits.Length);
            if (!_bits[index]) return false;
        }
        return true;
    }
}

/// <summary>
/// t-digest for percentile computation on streaming data.
/// </summary>
public sealed class TDigest
{
    private readonly List<Centroid> _centroids = new();
    private readonly double _compression;

    public TDigest(double compression = 100)
    {
        _compression = compression;
    }

    public void Add(double value, double weight = 1.0)
    {
        _centroids.Add(new Centroid(value, weight));
        if (_centroids.Count > _compression * 10)
            Compress();
    }

    public double Quantile(double q)
    {
        Compress();
        var sorted = _centroids.OrderBy(c => c.Mean).ToList();
        double totalWeight = sorted.Sum(c => c.Weight);
        double targetWeight = q * totalWeight;
        double cumulativeWeight = 0;

        foreach (var centroid in sorted)
        {
            cumulativeWeight += centroid.Weight;
            if (cumulativeWeight >= targetWeight)
                return centroid.Mean;
        }

        return sorted.Last().Mean;
    }

    private void Compress()
    {
        if (_centroids.Count <= _compression) return;

        var sorted = _centroids.OrderBy(c => c.Mean).ToList();
        var compressed = new List<Centroid>();

        for (int i = 0; i < sorted.Count; i += 2)
        {
            if (i + 1 < sorted.Count)
            {
                var merged = Centroid.Merge(sorted[i], sorted[i + 1]);
                compressed.Add(merged);
            }
            else
            {
                compressed.Add(sorted[i]);
            }
        }

        _centroids.Clear();
        _centroids.AddRange(compressed);
    }

    private record Centroid(double Mean, double Weight)
    {
        public static Centroid Merge(Centroid a, Centroid b)
        {
            double totalWeight = a.Weight + b.Weight;
            double mean = (a.Mean * a.Weight + b.Mean * b.Weight) / totalWeight;
            return new Centroid(mean, totalWeight);
        }
    }
}
```

**If ProbabilisticStorageStrategy is missing:**

Create strategy using SDK primitives (pattern shown in Task 1 Step 3).

**Update TODO.md:**
Mark all implemented T85 sub-tasks as [x]. Update status to [x] Complete.

**Build verification:**
```bash
dotnet build DataWarehouse.SDK/ --no-incremental
dotnet build Plugins/DataWarehouse.Plugins.UltimateStorage/ --no-incremental
```
Must pass with 0 errors.
  </action>
  <verify>
- SDK and plugin build pass with 0 errors
- All 8 T85 sub-tasks marked [x] in Metadata/TODO.md
- ProbabilisticStorageStrategy registered in UltimateStoragePlugin
- No forbidden patterns in modified files
- SDK primitives (CountMinSketch, HyperLogLog, BloomFilter, TDigest) compile and work
  </verify>
  <done>
All T85 probabilistic data structures sub-tasks are production-ready. SDK primitives and storage strategy provide memory-efficient analytics with configurable error bounds.
  </done>
</task>

</tasks>

<verification>
**Build verification:**
```bash
dotnet build DataWarehouse.SDK/ --no-incremental
dotnet build Plugins/DataWarehouse.Plugins.UltimateStorage/ --no-incremental
```
Expected: 0 errors

**Forbidden pattern scan:**
```bash
grep -r "NotImplementedException\|TODO.*implement\|STUB\|MOCK" DataWarehouse.SDK/Primitives/Probabilistic*.cs --include="*.cs" 2>/dev/null
grep -r "NotImplementedException\|TODO.*implement\|STUB\|MOCK" Plugins/DataWarehouse.Plugins.UltimateStorage/Strategies/Innovation/ProbabilisticStorageStrategy.cs --include="*.cs"
```
Expected: 0 matches

**SDK primitive verification:**
```bash
grep "class CountMinSketch\|class HyperLogLog\|class BloomFilter\|class TDigest" DataWarehouse.SDK/Primitives/Probabilistic*.cs
```
Expected: 4 matches (one for each structure)
</verification>

<success_criteria>
1. SDK primitives (CountMinSketch, HyperLogLog, BloomFilter, TDigest) compile without errors
2. ProbabilisticStorageStrategy compiles without errors
3. All 8 T85 sub-tasks verified implemented (Count-Min, HyperLogLog, Bloom, t-digest, query interface, error bounds, memory profiling, serialization)
4. Probabilistic structures use 10-1000x less memory than exact structures
5. Error bounds are configurable with reasonable defaults
6. Zero forbidden patterns in codebase
7. Metadata/TODO.md updated with T85 completion status
</success_criteria>

<output>
After completion, create `.planning/phases/10-advanced-storage/10-06-SUMMARY.md`
</output>
