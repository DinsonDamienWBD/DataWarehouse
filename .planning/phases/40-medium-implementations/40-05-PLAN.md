---
phase: 40-medium-implementations
plan: 05
type: execute
wave: 1
depends_on: []
files_modified:
  - Plugins/DataWarehouse.Plugins.UltimateDataFormat/Strategies/Columnar/ParquetStrategy.cs
  - Plugins/DataWarehouse.Plugins.UltimateDataFormat/Strategies/Scientific/Hdf5Strategy.cs
  - Plugins/DataWarehouse.Plugins.UltimateDataFormat/Strategies/Scientific/ArrowStrategy.cs
  - Plugins/DataWarehouse.Plugins.UltimateDataFormat/DataWarehouse.Plugins.UltimateDataFormat.csproj
autonomous: true

must_haves:
  truths:
    - "ParquetStrategy.ParseAsync returns DataFormatResult.Success with structured columnar data and schema"
    - "Hdf5Strategy.ParseAsync returns DataFormatResult.Success with hierarchical dataset structure"
    - "Arrow IPC format can be read and produces columnar RecordBatch-like data"
    - "Format detection (DetectFormatCoreAsync) remains unchanged for both Parquet and HDF5"
  artifacts:
    - path: "Plugins/DataWarehouse.Plugins.UltimateDataFormat/Strategies/Columnar/ParquetStrategy.cs"
      provides: "Real ParseAsync using Parquet.Net library"
      contains: "ParquetReader"
    - path: "Plugins/DataWarehouse.Plugins.UltimateDataFormat/Strategies/Scientific/Hdf5Strategy.cs"
      provides: "Real ParseAsync using HDF.PInvoke or managed HDF5 parsing"
      contains: "DataFormatResult.Success"
    - path: "Plugins/DataWarehouse.Plugins.UltimateDataFormat/Strategies/Scientific/ArrowStrategy.cs"
      provides: "Arrow IPC format strategy using Apache.Arrow NuGet"
      min_lines: 80
  key_links:
    - from: "ParquetStrategy.ParseAsync"
      to: "DataFormatResult.Success"
      via: "returns success with parsed schema and data"
      pattern: "DataFormatResult\\.Success"
    - from: "Hdf5Strategy.ParseAsync"
      to: "DataFormatResult.Success"
      via: "returns success with parsed hierarchy"
      pattern: "DataFormatResult\\.Success"
---

<pre-completion-note>
## PRE-COMPLETED IN PHASE 31.1

This plan's work has been PULLED FORWARD into Phase 31.1-03 (Task 2: UltimateDataFormat Strategy Implementations):

1. **Parquet** — `Parquet.Net` NuGet added, real ParseAsync/SerializeAsync/ExtractSchemaCoreAsync using ParquetReader/ParquetWriter
2. **Arrow** — `Apache.Arrow` NuGet added, new ArrowStrategy with ArrowFileReader/ArrowFileWriter for Apache Arrow IPC
3. **HDF5** — Managed binary parser (no native PInvoke), reads superblock/symbol tables/object headers for hierarchical metadata

**Rationale:** These are straightforward NuGet-based implementations that don't depend on any v3.0 infrastructure. Leaving them as stubs until Phase 40 means carrying placeholder code through the entire v3.0 milestone unnecessarily.

**Action:** This plan should be SKIPPED during Phase 40 execution. Verify the Phase 31.1 implementation is complete instead.
</pre-completion-note>

<objective>
[SUPERSEDED — See pre-completion note above]

Replace stub ParseAsync methods in ParquetStrategy and Hdf5Strategy with real parsing using NuGet packages, and add a new ArrowStrategy for Apache Arrow IPC format.

Purpose: IMPL-05 requires scientific format parsing that returns structured columnar/hierarchical data instead of error messages naming the required libraries. Both Parquet and HDF5 stubs explicitly say "requires library X, install package and implement."

Output: Real parsing implementations in ParquetStrategy (Parquet.Net), Hdf5Strategy (managed parsing or PInvoke), and new ArrowStrategy (Apache.Arrow).
</objective>

<execution_context>
@C:/Users/ddamien/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/ddamien/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/REQUIREMENTS-v3.md
@Plugins/DataWarehouse.Plugins.UltimateDataFormat/Strategies/Columnar/ParquetStrategy.cs
@Plugins/DataWarehouse.Plugins.UltimateDataFormat/Strategies/Scientific/Hdf5Strategy.cs
@Plugins/DataWarehouse.Plugins.UltimateDataFormat/DataWarehouse.Plugins.UltimateDataFormat.csproj
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement Parquet and Arrow parsing with NuGet packages</name>
  <files>
    Plugins/DataWarehouse.Plugins.UltimateDataFormat/Strategies/Columnar/ParquetStrategy.cs
    Plugins/DataWarehouse.Plugins.UltimateDataFormat/Strategies/Scientific/ArrowStrategy.cs
    Plugins/DataWarehouse.Plugins.UltimateDataFormat/DataWarehouse.Plugins.UltimateDataFormat.csproj
  </files>
  <action>
**Step 1: Add NuGet packages to csproj:**
Add to DataWarehouse.Plugins.UltimateDataFormat.csproj:
```xml
<PackageReference Include="Parquet.Net" Version="5.1.1" />
<PackageReference Include="Apache.Arrow" Version="18.1.0" />
```

Note: Do NOT add HDF.PInvoke -- it requires native HDF5 libraries which may not be available on all platforms. HDF5 parsing will use a managed approach (Task 2).

**Step 2: Replace ParquetStrategy.ParseAsync:**

Replace the stub `ParseAsync` that returns `DataFormatResult.Fail(...)` with real parsing:

```csharp
public override async Task<DataFormatResult> ParseAsync(Stream input, DataFormatContext context, CancellationToken ct = default)
```

Implementation using Parquet.Net:
1. Open stream with `await ParquetReader.CreateAsync(input, cancellationToken: ct)`
2. Read schema: iterate `reader.Schema.Fields` to extract column names, data types, nullability
3. Read data: for each row group (`reader.RowGroupCount`), read column data via `reader.ReadEntireRowGroupAsync()`
4. Build result:
   - Schema: DataField names and types mapped to SchemaField[]
   - Data: row groups as List of Dictionary<string, object[]> (column name -> column values)
   - Row count, column count, compression codec info
5. Return `DataFormatResult.Success(parsedData, metadata)` where metadata includes schema and row statistics

Also replace `SerializeAsync` stub:
1. Accept data as IEnumerable<Dictionary<string, object>> or similar
2. Infer or use provided schema
3. Write using `ParquetWriter` with SNAPPY compression (default)
4. Return DataFormatResult.Success

Also replace `ExtractSchemaCoreAsync` stub:
1. Open with ParquetReader
2. Read schema fields (name, type, nullability)
3. Return real FormatSchema with actual column definitions

Keep `DetectFormatCoreAsync` and `ValidateCoreAsync` exactly as they are (they already work).

**Step 3: Create ArrowStrategy:**

Create new file `Plugins/DataWarehouse.Plugins.UltimateDataFormat/Strategies/Scientific/ArrowStrategy.cs`:

Class: `ArrowStrategy : DataFormatStrategyBase`
- StrategyId: "arrow"
- DisplayName: "Apache Arrow IPC"
- FormatInfo: extensions [".arrow", ".feather"], DomainFamily.Analytics
- Capabilities: Bidirectional=true, Streaming=true, SchemaAware=true

DetectFormatCoreAsync:
- Arrow IPC magic: first 8 bytes contain "ARROW1" at offset 4 (continuation marker + schema)
- Check: bytes[4..10] == "ARROW1" for file format
- Or check first 4 bytes for continuation token (0xFFFFFFFF) for streaming format

ParseAsync using Apache.Arrow:
1. Create `ArrowFileReader` from the stream
2. Read RecordBatches
3. For each batch: extract schema (field names, types), extract column data
4. Build result with schema and columnar data
5. Return DataFormatResult.Success

SerializeAsync:
1. Create schema from data
2. Write RecordBatches using `ArrowFileWriter`
3. Return DataFormatResult.Success

ExtractSchemaCoreAsync:
1. Open ArrowFileReader, read schema
2. Map to FormatSchema

ValidateCoreAsync:
1. Check magic bytes
2. Try to open as ArrowFileReader
3. Return Valid or Invalid
  </action>
  <verify>Build: `dotnet build Plugins/DataWarehouse.Plugins.UltimateDataFormat/DataWarehouse.Plugins.UltimateDataFormat.csproj`</verify>
  <done>ParquetStrategy.ParseAsync returns real columnar data with schema using Parquet.Net. ArrowStrategy provides Arrow IPC format parsing using Apache.Arrow. Both return DataFormatResult.Success with structured data.</done>
</task>

<task type="auto">
  <name>Task 2: Implement HDF5 parsing with managed approach</name>
  <files>Plugins/DataWarehouse.Plugins.UltimateDataFormat/Strategies/Scientific/Hdf5Strategy.cs</files>
  <action>
Replace the stub `ParseAsync` in Hdf5Strategy with a managed HDF5 parser that reads the binary format directly, without requiring native HDF5 libraries.

**Why managed parsing instead of HDF.PInvoke:** HDF.PInvoke requires the native HDF5 C library (hdf5.dll / libhdf5.so) to be present at runtime. This creates deployment complexity -- the library must match the platform and architecture. A managed parser that reads the HDF5 binary format directly is more portable and aligns with the "runs anywhere" goal.

**HDF5 Binary Format Essentials (for managed parsing):**
- Superblock at offset 0 (after 8-byte signature): version, sizes, root group address
- B-tree nodes for group entries
- Heap for object names
- Dataset headers with dataspace (dimensions) and datatype information

**Implementation approach -- practical HDF5 metadata reader:**

Replace `ParseAsync`:
1. Verify HDF5 signature (reuse DetectFormatCoreAsync)
2. Read superblock: extract version number, offset sizes, length sizes, group leaf/internal node K values, root group symbol table entry address
3. Navigate to root group symbol table entry
4. Read symbol table: extract object names (from local heap), object header addresses
5. For each dataset object: read object header messages to extract:
   - Dataspace message (rank, dimensions)
   - Datatype message (class: integer, float, string, compound, etc.)
   - Data layout message (contiguous, chunked, compact)
   - Filter pipeline message (compression filters: deflate, shuffle, etc.)
6. Build hierarchical structure: groups contain datasets and sub-groups
7. Return DataFormatResult.Success with:
   - Hierarchical tree of groups and datasets
   - For each dataset: name, dimensions, datatype, compression, data offset/size
   - Schema as FormatSchema with nested fields

Create helper class `Hdf5BinaryReader` (private, within the same file or as a nested class):
- `ReadSuperblock(BinaryReader)` -> HDF5 version, root group offset
- `ReadSymbolTableEntry(BinaryReader, long offset)` -> name, object header offset
- `ReadObjectHeader(BinaryReader, long offset)` -> list of header messages
- `ReadDataspaceMessage(BinaryReader)` -> rank, dimensions
- `ReadDatatypeMessage(BinaryReader)` -> type class, size

Supporting types (private/internal):
- `Hdf5Group` record: Name, Groups (List), Datasets (List), Attributes (Dictionary)
- `Hdf5Dataset` record: Name, Rank, Dimensions (long[]), DataType, CompressionFilter, DataOffset, DataSize
- `Hdf5DataType` record: TypeClass (Int, Float, String, Compound, etc.), Size, ByteOrder

Note: This is a METADATA parser -- it reads the structural information (groups, datasets, dimensions, types) but does NOT decode compressed data blocks. Reading actual data from compressed HDF5 datasets requires decompression filters which need the native library. The metadata parsing is sufficient for catalog/discovery purposes and meets IMPL-05's requirement of "hierarchical dataset structure."

Also replace `SerializeAsync` with a meaningful implementation that creates a minimal valid HDF5 file:
1. Write HDF5 signature
2. Write superblock (version 0, simplest format)
3. Write root group with provided datasets as simple contiguous uncompressed data
4. Return DataFormatResult.Success

Also replace `ExtractSchemaCoreAsync`:
1. Parse the file structure using the managed reader
2. Build FormatSchema with real group/dataset hierarchy
3. Return real schema instead of placeholder

Keep `DetectFormatCoreAsync` and `ValidateCoreAsync` exactly as they are.
  </action>
  <verify>Build: `dotnet build Plugins/DataWarehouse.Plugins.UltimateDataFormat/DataWarehouse.Plugins.UltimateDataFormat.csproj`</verify>
  <done>Hdf5Strategy.ParseAsync returns DataFormatResult.Success with real hierarchical structure (groups, datasets, dimensions, types) parsed from HDF5 binary format. No native library dependency. Stub error message completely removed.</done>
</task>

</tasks>

<verification>
1. `dotnet build Plugins/DataWarehouse.Plugins.UltimateDataFormat/DataWarehouse.Plugins.UltimateDataFormat.csproj` -- zero new errors
2. Grep confirms Parquet stub removed: `grep -r "Parquet parsing requires" Plugins/DataWarehouse.Plugins.UltimateDataFormat/` should return nothing
3. Grep confirms HDF5 stub removed: `grep -r "HDF5 parsing requires" Plugins/DataWarehouse.Plugins.UltimateDataFormat/` should return nothing
4. Grep confirms real parsing: `grep -r "DataFormatResult.Success" Plugins/DataWarehouse.Plugins.UltimateDataFormat/Strategies/Columnar/ParquetStrategy.cs`
5. Grep confirms Arrow strategy: `grep -r "class ArrowStrategy" Plugins/DataWarehouse.Plugins.UltimateDataFormat/`
</verification>

<success_criteria>
- ParquetStrategy.ParseAsync returns structured columnar data with schema using Parquet.Net (not an error message)
- Hdf5Strategy.ParseAsync returns hierarchical dataset structure via managed binary parsing (not an error message)
- ArrowStrategy exists with full ParseAsync/SerializeAsync for Apache Arrow IPC format
- Format detection (DetectFormatCoreAsync) unchanged for Parquet and HDF5
- Two NuGet packages added: Parquet.Net, Apache.Arrow
- Plugin project builds with zero new errors
</success_criteria>

<output>
After completion, create `.planning/phases/40-medium-implementations/40-05-SUMMARY.md`
</output>
