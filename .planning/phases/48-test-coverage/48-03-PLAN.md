---
phase: 48
plan: 48-03
title: "Edge Case & Failure Mode Tests"
depends_on: []
---

# Plan 48-03: Edge Case & Failure Mode Tests

## Goal
Exercise every error path, timeout behavior under network partition, graceful degradation when plugins fail, WAL/checkpoint recovery, and bounded collection overflow. Ensure system resilience under adverse conditions.

## Approach
1. **Error Path Exercise (Every Error Path)**:
   - Review all error handling code:
     - `try-catch` blocks
     - Error return codes
     - Exception types thrown
   - For each error path, write test:
     - Trigger error condition
     - Verify error handled correctly (no crash, error logged, user notified)
   - Example error paths:
     - File not found: read non-existent file → verify FileNotFoundException, error message
     - Network timeout: connect to non-responsive server → verify SocketException, retry logic
     - Invalid input: pass null → verify ArgumentNullException
     - Database constraint violation: insert duplicate key → verify SqlException, rollback
     - Disk full: write when disk full → verify IOException, graceful degradation
     - Permission denied: write to read-only file → verify UnauthorizedAccessException
     - Encryption key not found: decrypt without key → verify CryptographicException

2. **Timeout Behavior Under Network Partition**:
   - Test scenario: Network partition between nodes
   - Test setup:
     - Deploy 3-node cluster
     - Inject network partition (firewall rules, iptables)
   - Test cases:
     - **Client timeout**: client sends request to partitioned node → verify timeout after configured interval (5s, 30s, etc.)
     - **Server timeout**: server waits for client response → verify timeout, connection closed
     - **Replication timeout**: leader replicates to partitioned follower → verify timeout, follower marked unhealthy
     - **Consensus timeout**: Raft election timeout → verify new leader elected in majority partition
   - Verify timeout configuration:
     - Configurable (not hardcoded)
     - Reasonable defaults (not too short, not too long)
     - Exponential backoff for retries
   - Verify partition healing:
     - Remove partition (restore network)
     - Verify partitioned nodes rejoin cluster
     - Verify data synced (catch-up replication)

3. **Graceful Degradation When Plugins Fail**:
   - Test scenario: Plugin throws exception during operation
   - Test cases:
     - **Compression plugin fails**: compression throws exception → verify fallback to uncompressed storage
     - **Encryption plugin fails**: encryption throws exception → verify error (no fallback to plaintext)
     - **Storage backend fails**: S3 unavailable → verify fallback to secondary backend (Azure Blob)
     - **Replication plugin fails**: replication throws exception → verify primary write succeeds, replication retried later
   - Test scenario: Plugin crashes during operation
   - Test cases:
     - Plugin process crashes (out-of-memory, segfault)
     - Verify kernel detects crash (health check failure, process exit)
     - Verify kernel restarts plugin (auto-recovery)
     - Verify operations queued during restart (no data loss)
   - Test scenario: Plugin unloaded during operation
   - Test cases:
     - Plugin unloaded while processing message
     - Verify in-flight operations cancelled gracefully (no partial state)
     - Verify error returned to caller (plugin unavailable)

4. **WAL/Checkpoint Recovery**:
   - Test scenario: Crash during write operation
   - Test setup:
     - Start DataWarehouse
     - Begin write operation (write to VDE, storage backend, database)
     - Simulate crash (kill process, power loss)
   - Test recovery:
     - Restart DataWarehouse
     - Verify WAL replayed (uncommitted operations recovered or rolled back)
     - Verify data consistency (committed operations persisted, uncommitted rolled back)
   - Test cases:
     - **Crash before WAL write**: verify operation rolled back (data not written)
     - **Crash after WAL write, before commit**: verify WAL replayed, operation committed
     - **Crash after commit, before checkpoint**: verify WAL replayed, checkpoint created
     - **Crash during checkpoint**: verify partial checkpoint discarded, previous checkpoint used
   - Verify checkpoint behavior:
     - Checkpoint triggered after N operations or M seconds
     - Checkpoint atomic (all or nothing)
     - Old WAL entries purged after checkpoint

5. **Bounded Collection Overflow**:
   - Identify all unbounded collections:
     - Lists without max size
     - Dictionaries without eviction policy
     - Queues without backpressure
   - For each collection, verify bounded behavior:
     - **Message queue**: publish 1M messages → verify queue bounded (max 10K), oldest evicted
     - **Cache**: insert 1M entries → verify cache bounded (max 1K), LRU eviction
     - **Event log**: log 1M events → verify log bounded (max 100K), oldest archived
   - Test overflow behavior:
     - **Queue full**: publish when queue full → verify backpressure (publisher blocks or error)
     - **Cache full**: insert when cache full → verify eviction (LRU, LFU, FIFO)
     - **Log full**: log when log full → verify archival (move old entries to archive storage)
   - Verify no out-of-memory errors (collections bounded)

6. **Resource Exhaustion Testing**:
   - Test scenario: System under resource pressure
   - Test cases:
     - **Low memory**: run with limited heap (set max memory) → verify graceful degradation (evict cache, reject new requests)
     - **Low disk**: fill disk to 95% → verify writes rejected with error (disk full), no crash
     - **CPU saturation**: 100% CPU load → verify system responsive (slow but not hung)
     - **Connection exhaustion**: open max connections → verify new connections rejected (connection limit)
     - **File descriptor exhaustion**: open max files → verify new opens rejected (too many open files)
   - Verify resource limits configured:
     - Max memory (heap size)
     - Max disk usage (quota)
     - Max connections (connection pool size)
     - Max file descriptors (ulimit)

7. **Partial Failure Testing**:
   - Test scenario: Partial system failure
   - Test cases:
     - **Database read-only**: database enters read-only mode → verify reads succeed, writes fail gracefully
     - **Storage backend degraded**: S3 returns 503 → verify retry logic, fallback to secondary backend
     - **Network intermittent**: packet loss 50% → verify retries, eventual success
     - **Clock skew**: nodes have different system times → verify logical clocks (Lamport, vector clocks) maintain causality

8. **Data Corruption Detection**:
   - Test scenario: Data corrupted on disk
   - Test cases:
     - **Corrupt block**: modify storage block (flip bits) → verify checksum detects corruption, block rejected
     - **Corrupt WAL entry**: modify WAL entry → verify checksum detects corruption, entry skipped
     - **Corrupt index**: modify index file → verify index rebuilt from data
   - Verify corruption handling:
     - Corruption detected (checksum, signature validation)
     - Error logged (corruption event recorded)
     - Data recovered (from replica, from backup, from WAL)

## Scope
**In Scope:**
- Error path exercise (all error handling code)
- Timeout behavior under network partition
- Graceful degradation when plugins fail
- WAL/checkpoint recovery (crash recovery)
- Bounded collection overflow (queues, caches, logs)
- Resource exhaustion testing (memory, disk, CPU, connections, file descriptors)
- Partial failure testing (read-only database, degraded storage, intermittent network, clock skew)
- Data corruption detection (corrupt blocks, WAL entries, indexes)

**Out of Scope:**
- Unit tests (Plan 48-01)
- Integration tests (Plan 48-02)
- Cross-platform tests (Plan 48-04)

## Success Criteria
- [ ] All error paths exercised (every try-catch, error return code tested)
- [ ] Timeout behavior verified (client, server, replication, consensus timeouts)
- [ ] Partition healing verified (partitioned nodes rejoin, data synced)
- [ ] Graceful degradation verified (compression fails → uncompressed, storage fails → fallback)
- [ ] Plugin crash recovery verified (kernel restarts plugin, operations queued)
- [ ] WAL recovery verified (crash → restart → WAL replayed → data consistent)
- [ ] Checkpoint recovery verified (crash during checkpoint → previous checkpoint used)
- [ ] Bounded collections verified (queues, caches, logs have max size)
- [ ] Overflow behavior verified (backpressure, eviction, archival)
- [ ] Resource exhaustion verified (low memory → evict cache, disk full → reject writes)
- [ ] Partial failure verified (read-only database, degraded storage, intermittent network, clock skew)
- [ ] Data corruption detection verified (checksums detect corruption, data recovered)
- [ ] All tests pass (green)

## Output
- `TEST-COVERAGE-edge.md`: Edge case and failure mode test results
  - Error path test results (list all error paths tested)
  - Timeout behavior test results (network partition scenarios)
  - Graceful degradation test results (plugin failure scenarios)
  - WAL/checkpoint recovery test results (crash scenarios)
  - Bounded collection test results (overflow scenarios)
  - Resource exhaustion test results (memory, disk, CPU, connections, file descriptors)
  - Partial failure test results (database, storage, network, clock skew)
  - Data corruption detection test results
  - All failures documented (expected vs actual)
- Test projects:
  - `DataWarehouse.EdgeCaseTests/` with edge case and failure mode tests
  - Test fixtures for injecting failures (network partition, disk full, etc.)
  - Helper classes for simulating adverse conditions
