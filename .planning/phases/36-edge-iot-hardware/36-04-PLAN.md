---
phase: 36-edge-iot-hardware
plan: 04
type: execute
wave: 2
depends_on: []
files_modified:
  - DataWarehouse.SDK/Edge/Inference/IWasiNnHost.cs
  - DataWarehouse.SDK/Edge/Inference/OnnxWasiNnHost.cs
  - DataWarehouse.SDK/Edge/Inference/InferenceSession.cs
  - DataWarehouse.SDK/Edge/Inference/InferenceSettings.cs
  - DataWarehouse.SDK/DataWarehouse.SDK.csproj
autonomous: true

must_haves:
  truths:
    - "IWasiNnHost provides WASI-NN host API for on-device ML inference"
    - "OnnxWasiNnHost wraps ONNX Runtime for model loading and inference"
    - "Model loading from StorageAddress supported"
    - "Cached model instances avoid reload overhead"
    - "Batch inference supported"
    - "Execution providers: CPU, CUDA, DirectML (configurable)"
    - "Inference accuracy matches reference (>95%)"
  artifacts:
    - path: "DataWarehouse.SDK/Edge/Inference/IWasiNnHost.cs"
      provides: "WASI-NN host interface (load/init/compute)"
      min_lines: 30
    - path: "DataWarehouse.SDK/Edge/Inference/OnnxWasiNnHost.cs"
      provides: "ONNX Runtime wrapper implementing WASI-NN API"
      min_lines: 200
    - path: "DataWarehouse.SDK/Edge/Inference/InferenceSession.cs"
      provides: "Inference session management with caching"
      min_lines: 80
    - path: "DataWarehouse.SDK/Edge/Inference/InferenceSettings.cs"
      provides: "Inference configuration (model path, execution provider, batch size)"
      min_lines: 40
---

<objective>
Implement WASI-NN host (EDGE-04) wrapping ONNX Runtime for on-device ML inference with model caching, batch processing, and GPU acceleration support.

Purpose: Enable DataWarehouse to run ML models on edge devices. WASI-NN provides a standard interface for neural network inference in WebAssembly environments. This integration allows DataWarehouse to perform classification, object detection, and other ML tasks locally without cloud dependencies.

Output: WASI-NN host interface, ONNX Runtime wrapper, model caching, execution provider selection.
</objective>

<execution_context>
@C:/Users/ddamien/.claude/get-shit-done/workflows/execute-plan.md
</execution_context>

<context>
@.planning/REQUIREMENTS-v3.md
@.planning/phases/36-edge-iot-hardware/36-RESEARCH.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: WASI-NN Interfaces and Types</name>
  <files>
    DataWarehouse.SDK/Edge/Inference/IWasiNnHost.cs
    DataWarehouse.SDK/Edge/Inference/InferenceSettings.cs
  </files>
  <action>
Create namespace `DataWarehouse.SDK.Edge.Inference`:

**InferenceSettings.cs**:
```csharp
[SdkCompatibility("3.0.0", Notes = "Phase 36: Inference settings (EDGE-04)")]
public sealed record InferenceSettings
{
    public required string ModelPath { get; init; }
    public ExecutionProvider Provider { get; init; } = ExecutionProvider.CPU;
    public int BatchSize { get; init; } = 1;
    public int MaxCachedSessions { get; init; } = 5;
}

public enum ExecutionProvider { CPU, CUDA, DirectML, TensorRT }
```

**IWasiNnHost.cs**:
```csharp
[SdkCompatibility("3.0.0", Notes = "Phase 36: WASI-NN host interface (EDGE-04)")]
public interface IWasiNnHost : IDisposable
{
    Task<IInferenceSession> LoadModelAsync(string modelPath, InferenceSettings settings, CancellationToken ct = default);
    Task<float[]> InferAsync(IInferenceSession session, float[] input, CancellationToken ct = default);
    Task<float[][]> InferBatchAsync(IInferenceSession session, float[][] inputs, CancellationToken ct = default);
}

public interface IInferenceSession : IDisposable
{
    string ModelPath { get; }
    IReadOnlyList<string> InputNames { get; }
    IReadOnlyList<string> OutputNames { get; }
}
```
  </action>
  <verify>
Verify files exist. Grep for `IWasiNnHost`, `InferenceSettings`.
  </verify>
  <done>
WASI-NN host interface defined. Inference settings with execution provider selection.
  </done>
</task>

<task type="auto">
  <name>Task 2: ONNX Runtime Wrapper</name>
  <files>
    DataWarehouse.SDK/Edge/Inference/OnnxWasiNnHost.cs
    DataWarehouse.SDK/Edge/Inference/InferenceSession.cs
    DataWarehouse.SDK/DataWarehouse.SDK.csproj
  </files>
  <action>
Add NuGet package `Microsoft.ML.OnnxRuntime` version 1.20.0+.

**InferenceSession.cs**:
```csharp
internal sealed class OnnxInferenceSession : IInferenceSession
{
    private readonly Microsoft.ML.OnnxRuntime.InferenceSession _session;

    public string ModelPath { get; }
    public IReadOnlyList<string> InputNames { get; }
    public IReadOnlyList<string> OutputNames { get; }

    internal OnnxInferenceSession(string modelPath, Microsoft.ML.OnnxRuntime.InferenceSession session)
    {
        ModelPath = modelPath;
        _session = session;
        InputNames = session.InputMetadata.Keys.ToList();
        OutputNames = session.OutputMetadata.Keys.ToList();
    }

    internal Microsoft.ML.OnnxRuntime.InferenceSession UnderlyingSession => _session;

    public void Dispose() => _session?.Dispose();
}
```

**OnnxWasiNnHost.cs**:
```csharp
[SdkCompatibility("3.0.0", Notes = "Phase 36: ONNX Runtime WASI-NN host (EDGE-04)")]
public sealed class OnnxWasiNnHost : IWasiNnHost
{
    private readonly ConcurrentDictionary<string, OnnxInferenceSession> _sessionCache = new();
    private readonly int _maxCachedSessions;

    public OnnxWasiNnHost(int maxCachedSessions = 5)
    {
        _maxCachedSessions = maxCachedSessions;
    }

    public async Task<IInferenceSession> LoadModelAsync(string modelPath, InferenceSettings settings, CancellationToken ct = default)
    {
        if (_sessionCache.TryGetValue(modelPath, out var cached))
            return cached;

        var sessionOptions = new SessionOptions();

        // Configure execution provider
        switch (settings.Provider)
        {
            case ExecutionProvider.CUDA:
                sessionOptions.AppendExecutionProvider_CUDA();
                break;
            case ExecutionProvider.DirectML:
                sessionOptions.AppendExecutionProvider_DML();
                break;
            case ExecutionProvider.TensorRT:
                sessionOptions.AppendExecutionProvider_Tensorrt();
                break;
            default:
                sessionOptions.AppendExecutionProvider_CPU();
                break;
        }

        var onnxSession = new Microsoft.ML.OnnxRuntime.InferenceSession(modelPath, sessionOptions);
        var session = new OnnxInferenceSession(modelPath, onnxSession);

        if (_sessionCache.Count >= _maxCachedSessions)
        {
            var oldest = _sessionCache.First();
            _sessionCache.TryRemove(oldest.Key, out var removed);
            removed?.Dispose();
        }

        _sessionCache[modelPath] = session;
        return await Task.FromResult<IInferenceSession>(session);
    }

    public async Task<float[]> InferAsync(IInferenceSession session, float[] input, CancellationToken ct = default)
    {
        var onnxSession = ((OnnxInferenceSession)session).UnderlyingSession;
        var inputName = session.InputNames[0];

        var inputTensor = new DenseTensor<float>(input, new[] { 1, input.Length });
        var inputs = new List<NamedOnnxValue> { NamedOnnxValue.CreateFromTensor(inputName, inputTensor) };

        using var results = onnxSession.Run(inputs);
        var output = results.First().AsEnumerable<float>().ToArray();

        return await Task.FromResult(output);
    }

    public async Task<float[][]> InferBatchAsync(IInferenceSession session, float[][] inputs, CancellationToken ct = default)
    {
        var results = new List<float[]>();
        foreach (var input in inputs)
        {
            results.Add(await InferAsync(session, input, ct));
        }
        return results.ToArray();
    }

    public void Dispose()
    {
        foreach (var session in _sessionCache.Values)
            session?.Dispose();
        _sessionCache.Clear();
    }
}
```

Add error handling, XML docs. Use `System.Numerics.Tensors` for tensor operations.
  </action>
  <verify>
Build SDK project. Grep for `Microsoft.ML.OnnxRuntime`. Verify ONNX Runtime package referenced.
  </verify>
  <done>
ONNX Runtime wrapper implements WASI-NN host. Model caching, execution provider selection, batch inference.
  </done>
</task>

</tasks>

<verification>
- SDK builds with zero errors
- ONNX Runtime NuGet package referenced
- Model loading with CPU/CUDA/DirectML providers
- Session caching implemented
- Batch inference supported
</verification>

<success_criteria>
- Load ONNX model from file path
- Run inference with CPU execution provider
- GPU providers (CUDA, DirectML) used when available
- Model caching avoids reload overhead
- Batch inference processes multiple inputs
- Zero build errors
</success_criteria>

<output>
After completion, create `.planning/phases/36-edge-iot-hardware/36-04-SUMMARY.md`
</output>
