---
phase: 65-infrastructure
plan: 08
type: execute
wave: 1
depends_on: []
files_modified:
  - DataWarehouse.SDK/Distributed/SwimClusterMembership.cs
  - DataWarehouse.SDK/Distributed/CrdtTypes.cs
  - DataWarehouse.SDK/Distributed/AutoScaling/ProductionAutoScaler.cs
  - DataWarehouse.SDK/Distributed/Networking/ReliableUdpTransport.cs
autonomous: true

must_haves:
  truths:
    - "ORSet has tombstone GC to prevent unbounded growth"
    - "InMemoryAutoScaler is replaced with a production-ready scaler"
    - "CRDT serialization uses source-generated JSON (not reflection)"
    - "Raft heartbeat path has reduced lock contention"
  artifacts:
    - path: "DataWarehouse.SDK/Distributed/CrdtTypes.cs"
      provides: "ORSet with tombstone GC, source-gen JSON"
    - path: "DataWarehouse.SDK/Distributed/AutoScaling/ProductionAutoScaler.cs"
      provides: "Production auto-scaler with resource-based decisions"
      min_lines: 150
  key_links:
    - from: "ProductionAutoScaler"
      to: "IAutoScaler"
      via: "interface implementation"
      pattern: "IAutoScaler"
---

<objective>
Fix remaining performance issues: ORSet tombstone growth, InMemoryAutoScaler replacement, CRDT JSON performance, Raft lock contention.

Purpose: Phase 46 found ORSet has unbounded tag growth (latent memory leak), CRDT uses reflection-based JSON (unlike Raft/SWIM which have source-gen), and Raft serializes heartbeat path. Phase 45 noted InMemoryAutoScaler is the only auto-scaler (Tier 7 blocker). These are the remaining performance bottlenecks.

Output: Fixes across distributed subsystem.
</objective>

<execution_context>
@C:/Users/ddamien/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/ddamien/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: ORSet tombstone GC + CRDT source-gen JSON + Raft lock reduction</name>
  <files>
    DataWarehouse.SDK/Distributed/CrdtTypes.cs
    DataWarehouse.SDK/Distributed/RaftConsensusEngine.cs
  </files>
  <action>
1. ORSet tombstone GC (Phase 46 finding: unbounded tag growth):
   - Find ORSet in CrdtTypes.cs
   - Add tombstone garbage collection: after merge, if a tag exists in both add-set and remove-set, delete from both (it's been observed by all replicas)
   - GC trigger: after every Merge() call, or when tombstone count exceeds threshold (default 10000)
   - GC strategy: tag is safe to collect when all known replicas have observed it (vector clock comparison)
   - Simpler alternative if vector clocks unavailable: time-based GC — tombstones older than 24h are collected (configurable)
   - Add TombstoneCount property for monitoring
   - Add GarbageCollect() public method for explicit GC

2. CRDT source-generated JSON:
   - Find CRDT serialization code — Phase 46 notes it uses reflection-based JSON
   - Create [JsonSerializable] context for CRDT types (similar to SwimJsonContext, RaftJsonContext, GossipJsonContext that already exist)
   - CrdtJsonContext: include GCounter, PNCounter, ORSet, LWWRegister serialization
   - Replace JsonSerializer.Serialize/Deserialize calls with source-gen context overloads
   - Verify no System.Text.Json.Serialization.Metadata warnings

3. Raft heartbeat lock contention (Phase 46 finding: 4 lock acquisitions per 50ms for 5-node cluster):
   - Find the stateLock SemaphoreSlim in RaftConsensusEngine
   - Heartbeat path needs: read currentTerm, read commitIndex, read log entries for followers
   - Split lock into: _termLock (protects currentTerm/votedFor), _logLock (protects log operations)
   - Heartbeat reads can use volatile reads for currentTerm (no lock needed for reading a single long)
   - Use `Volatile.Read(ref _currentTerm)` instead of locking for heartbeat
   - Keep full lock only for state transitions (election, vote)
   - This should reduce from 4 lock acquisitions to 0-1 per heartbeat cycle
  </action>
  <verify>dotnet build — zero errors. Grep for "JsonSerializerContext" in CrdtTypes to confirm source-gen exists</verify>
  <done>ORSet has tombstone GC, CRDT uses source-gen JSON, Raft heartbeat path uses volatile reads instead of locks</done>
</task>

<task type="auto">
  <name>Task 2: Production auto-scaler replacing InMemoryAutoScaler</name>
  <files>
    DataWarehouse.SDK/Distributed/AutoScaling/ProductionAutoScaler.cs
  </files>
  <action>
Create ProductionAutoScaler : IAutoScaler replacing InMemoryAutoScaler for production use:

- Resource-based scaling decisions using actual metrics:
  - CPU utilization: scale up at 80%, scale down at 30% (configurable thresholds)
  - Memory utilization: scale up at 85%, scale down at 40%
  - Queue depth: scale up when pending requests > 100/node, scale down when < 10/node
  - Custom metrics via IScalingMetricProvider: GetMetricAsync(string name) -> double

- Scaling policy:
  - Cooldown period: minimum 60s between scale events (prevent thrashing)
  - Scale-up aggressive: add 50% capacity (ceil), min 1 node
  - Scale-down conservative: remove 1 node at a time
  - Min/max node count bounds (configurable, default min=1, max=100)

- Resource monitoring:
  - Use System.Diagnostics.Process for CPU (UserProcessorTime / wall time)
  - Use GC.GetGCMemoryInfo() for managed memory
  - Use IHealthCheck results from plugins for custom health signals

- Scaling actions via IScalingExecutor interface:
  - ScaleUpAsync(int additionalNodes) -> Task<ScaleResult>
  - ScaleDownAsync(int removeNodes) -> Task<ScaleResult>
  - Default implementation: InMemoryScalingExecutor (logs scale decisions but no real infra changes)
  - Real implementations would call cloud APIs (AWS ASG, Azure VMSS, K8s HPA) — those are separate strategy registrations

- AutoScalingConfiguration:
  - CpuScaleUpThreshold, CpuScaleDownThreshold
  - MemoryScaleUpThreshold, MemoryScaleDownThreshold
  - QueueDepthScaleUpThreshold, QueueDepthScaleDownThreshold
  - CooldownSeconds, MinNodes, MaxNodes
  - EvaluationIntervalSeconds (default 15)

- Do NOT delete InMemoryAutoScaler — keep it as the simple/test implementation. ProductionAutoScaler is the real one.
  </action>
  <verify>dotnet build — zero errors. ProductionAutoScaler implements IAutoScaler</verify>
  <done>ProductionAutoScaler exists with resource-based scaling (CPU/memory/queue), cooldown, and configurable thresholds; InMemoryAutoScaler preserved for testing</done>
</task>

</tasks>

<verification>
- Full solution build passes
- ORSet.TombstoneCount property exists
- CrdtJsonContext is a [JsonSerializable] class
- ProductionAutoScaler implements IAutoScaler
- Raft heartbeat uses Volatile.Read for currentTerm
</verification>

<success_criteria>
Remaining Phase 46/45 performance findings resolved: ORSet GC, CRDT source-gen, Raft lock reduction, production auto-scaler.
</success_criteria>

<output>
After completion, create `.planning/phases/65-infrastructure/65-08-SUMMARY.md`
</output>
