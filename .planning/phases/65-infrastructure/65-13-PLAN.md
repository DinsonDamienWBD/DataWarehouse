---
phase: 65-infrastructure
plan: 13
type: execute
wave: 1
depends_on: []
files_modified:
  - DataWarehouse.SDK/Hardware/Accelerators/VulkanInterop.cs
  - DataWarehouse.SDK/Hardware/Accelerators/MetalInterop.cs
  - DataWarehouse.SDK/Hardware/Accelerators/WebGpuInterop.cs
  - DataWarehouse.SDK/Hardware/Accelerators/WasiNnAccelerator.cs
autonomous: true

must_haves:
  truths:
    - "Vulkan compute shaders can be dispatched for GPU data processing"
    - "Metal interop enables GPU compute on Apple Silicon"
    - "WebGPU interop enables browser-based GPU compute"
    - "WASI-NN extends existing OnnxWasiNnHost with accelerator-aware inference"
  artifacts:
    - path: "DataWarehouse.SDK/Hardware/Accelerators/VulkanInterop.cs"
      provides: "Vulkan compute P/Invoke bindings"
      min_lines: 120
    - path: "DataWarehouse.SDK/Hardware/Accelerators/MetalInterop.cs"
      provides: "Apple Metal compute interop"
      min_lines: 80
    - path: "DataWarehouse.SDK/Hardware/Accelerators/WebGpuInterop.cs"
      provides: "WebGPU/wgpu-native interop"
      min_lines: 80
    - path: "DataWarehouse.SDK/Hardware/Accelerators/WasiNnAccelerator.cs"
      provides: "WASI-NN with accelerator routing"
      min_lines: 100
  key_links:
    - from: "VulkanInterop"
      to: "IGpuAccelerator"
      via: "accelerator registration"
      pattern: "IGpuAccelerator"
    - from: "WasiNnAccelerator"
      to: "OnnxWasiNnHost"
      via: "WASI-NN backend routing"
      pattern: "OnnxWasiNnHost|WasiNn"
---

<objective>
Add Vulkan, Metal, WebGPU, and WASI-NN accelerator interop to complete GPU/accelerator coverage.

Purpose: Completes the accelerator matrix: Vulkan (cross-platform compute shaders), Metal (Apple), WebGPU (browser/WASM), WASI-NN (standardized inference). Combined with Plan 12 (OpenCL/SYCL/Triton/CANN) and existing CUDA/ROCm, this covers all major GPU compute APIs.

Output: Vulkan, Metal, WebGPU, WASI-NN interop in SDK/Hardware/Accelerators/.
</objective>

<execution_context>
@C:/Users/ddamien/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/ddamien/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@DataWarehouse.SDK/Hardware/Accelerators/CudaInterop.cs
@DataWarehouse.SDK/Hardware/Accelerators/GpuAccelerator.cs
@DataWarehouse.SDK/Edge/Inference/OnnxWasiNnHost.cs
</context>

<tasks>

<task type="auto">
  <name>Task 1: Vulkan compute and Metal interop</name>
  <files>
    DataWarehouse.SDK/Hardware/Accelerators/VulkanInterop.cs
    DataWarehouse.SDK/Hardware/Accelerators/MetalInterop.cs
  </files>
  <action>
Create VulkanInterop.cs:
- P/Invoke bindings for Vulkan compute (libvulkan.so / vulkan-1.dll):
  - vkCreateInstance, vkEnumeratePhysicalDevices, vkGetPhysicalDeviceProperties
  - vkCreateDevice, vkGetDeviceQueue
  - vkAllocateMemory, vkFreeMemory, vkMapMemory, vkUnmapMemory
  - vkCreateBuffer, vkDestroyBuffer, vkBindBufferMemory
  - vkCreateShaderModule (load SPIR-V), vkCreateComputePipelines
  - vkCreateCommandPool, vkAllocateCommandBuffers, vkBeginCommandBuffer
  - vkCmdBindPipeline, vkCmdBindDescriptorSets, vkCmdDispatch
  - vkEndCommandBuffer, vkQueueSubmit, vkQueueWaitIdle
  - vkCreateDescriptorSetLayout, vkCreateDescriptorPool, vkAllocateDescriptorSets
- VulkanAccelerator : IGpuAccelerator
  - Device enumeration via vkEnumeratePhysicalDevices
  - Compute queue family selection (QueueFlags.Compute)
  - Buffer allocation with device-local memory
  - Shader dispatch: load SPIR-V binary -> create pipeline -> dispatch compute
- AcceleratorType.Vulkan

Create MetalInterop.cs:
- Metal is macOS/iOS only — P/Invoke via libobjc + Metal framework
- Since Metal uses Objective-C, use objc_msgSend pattern for interop:
  - MTLCreateSystemDefaultDevice -> id<MTLDevice>
  - device.newCommandQueue -> id<MTLCommandQueue>
  - device.newBufferWithLength:options: -> id<MTLBuffer>
  - device.newLibraryWithSource:options:error: (compile Metal shader)
  - device.newComputePipelineStateWithFunction:error:
  - commandBuffer.computeCommandEncoder -> id<MTLComputeCommandEncoder>
  - encoder.setComputePipelineState, setBuffer, dispatchThreadgroups
- MetalAccelerator : IGpuAccelerator
- Platform check: only available on macOS/iOS (RuntimeInformation.IsOSPlatform(OSPlatform.OSX))
- AcceleratorType.Metal

Both follow the same graceful unavailability pattern as existing accelerators.
  </action>
  <verify>dotnet build DataWarehouse.SDK/DataWarehouse.SDK.csproj — zero errors</verify>
  <done>Vulkan compute and Metal interop exist with P/Invoke bindings; Vulkan supports SPIR-V shader dispatch; Metal supports macOS GPU compute</done>
</task>

<task type="auto">
  <name>Task 2: WebGPU interop and WASI-NN accelerator routing</name>
  <files>
    DataWarehouse.SDK/Hardware/Accelerators/WebGpuInterop.cs
    DataWarehouse.SDK/Hardware/Accelerators/WasiNnAccelerator.cs
  </files>
  <action>
Create WebGpuInterop.cs:
- WebGPU via wgpu-native (libwgpu_native.so / wgpu_native.dll):
  - wgpuCreateInstance, wgpuInstanceRequestAdapter, wgpuAdapterRequestDevice
  - wgpuDeviceCreateBuffer, wgpuDeviceCreateShaderModule (WGSL source)
  - wgpuDeviceCreateComputePipeline, wgpuDeviceCreateBindGroup
  - wgpuDeviceCreateCommandEncoder, wgpuCommandEncoderBeginComputePass
  - wgpuComputePassEncoderSetPipeline, wgpuComputePassEncoderSetBindGroup, wgpuComputePassEncoderDispatchWorkgroups
  - wgpuComputePassEncoderEnd, wgpuCommandEncoderFinish
  - wgpuQueueSubmit, wgpuBufferMapAsync, wgpuBufferGetMappedRange
- WebGpuAccelerator : IGpuAccelerator
  - Supports WGSL (WebGPU Shading Language) compute shaders
  - Buffer operations: create, write, read back
  - Works on any platform with wgpu-native (Windows, Linux, macOS, WASM)
- AcceleratorType.WebGpu

Create WasiNnAccelerator.cs:
- Extends existing OnnxWasiNnHost with accelerator-aware inference routing
- IInferenceAccelerator interface:
  - LoadModelAsync(string modelPath, InferenceBackend backend, CancellationToken ct) -> ModelHandle
  - RunInferenceAsync(ModelHandle model, ReadOnlyMemory<byte> input, CancellationToken ct) -> InferenceResult
  - InferenceBackend enum: CPU, CUDA, ROCm, OpenCL, TensorRT, CoreML, NNAPI, CANN
- WasiNnAccelerator implementation:
  - Routes inference to best available backend based on hardware detection
  - Priority: CUDA > ROCm > CoreML (macOS) > NNAPI (Android) > CANN > OpenCL > CPU
  - Uses IGpuAccelerator.IsAvailable to probe hardware
  - Falls back to CPU (OnnxWasiNnHost) if no accelerator available
- Model format support: ONNX (primary), TensorFlow Lite, PyTorch (via ONNX conversion hint)
- ModelHandle tracks which backend is executing for cleanup

Wire WasiNnAccelerator to message bus: "inference.execute" topic.
  </action>
  <verify>dotnet build DataWarehouse.SDK/DataWarehouse.SDK.csproj — zero errors</verify>
  <done>WebGPU interop (WGSL shaders) and WASI-NN accelerator-aware inference routing exist; 10 accelerator types total</done>
</task>

</tasks>

<verification>
- Build passes with zero errors
- AcceleratorType enum has all 10 entries: CUDA, ROCm, OpenCL, SYCL, Triton, CANN, Vulkan, Metal, WebGpu, WasiNn
- WasiNnAccelerator routes to best available backend
</verification>

<success_criteria>
GPU/accelerator coverage is complete: 10 compute APIs (CUDA, ROCm, OpenCL, SYCL, Triton, CANN, Vulkan, Metal, WebGPU, WASI-NN) all with P/Invoke interop and managed wrappers.
</success_criteria>

<output>
After completion, create `.planning/phases/65-infrastructure/65-13-SUMMARY.md`
</output>
