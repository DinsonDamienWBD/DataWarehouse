---
phase: 65-infrastructure
plan: 12
type: execute
wave: 1
depends_on: []
files_modified:
  - DataWarehouse.SDK/Hardware/Accelerators/OpenClInterop.cs
  - DataWarehouse.SDK/Hardware/Accelerators/SyclInterop.cs
  - DataWarehouse.SDK/Hardware/Accelerators/TritonInterop.cs
  - DataWarehouse.SDK/Hardware/Accelerators/CannInterop.cs
  - DataWarehouse.SDK/Hardware/Accelerators/IGpuAccelerator.cs
autonomous: true

must_haves:
  truths:
    - "OpenCL interop enables GPU compute on AMD/Intel/NVIDIA via standard API"
    - "SYCL interop enables heterogeneous compute (CPU+GPU+FPGA)"
    - "Triton interop enables GPU kernel compilation for ML workloads"
    - "CANN interop enables Huawei Ascend NPU acceleration"
  artifacts:
    - path: "DataWarehouse.SDK/Hardware/Accelerators/OpenClInterop.cs"
      provides: "OpenCL P/Invoke bindings"
      min_lines: 100
    - path: "DataWarehouse.SDK/Hardware/Accelerators/SyclInterop.cs"
      provides: "SYCL interop via DPC++ runtime"
      min_lines: 80
    - path: "DataWarehouse.SDK/Hardware/Accelerators/TritonInterop.cs"
      provides: "Triton GPU kernel compilation interop"
      min_lines: 80
    - path: "DataWarehouse.SDK/Hardware/Accelerators/CannInterop.cs"
      provides: "CANN (Huawei Ascend) interop"
      min_lines: 80
  key_links:
    - from: "OpenClInterop"
      to: "IGpuAccelerator"
      via: "accelerator registration"
      pattern: "IGpuAccelerator|RegisterAccelerator"
---

<objective>
Add GPU/accelerator interop bindings for OpenCL, SYCL, Triton, and CANN to complement existing CUDA and ROCm support.

Purpose: Phase 35 built CUDA and ROCm interop. This extends GPU support to cover the remaining compute APIs: OpenCL (cross-vendor), SYCL (Intel oneAPI), Triton (ML kernels), and CANN (Huawei Ascend). Needed for universal hardware coverage.

Output: P/Invoke interop classes for OpenCL, SYCL, Triton, CANN in SDK/Hardware/Accelerators/.
</objective>

<execution_context>
@C:/Users/ddamien/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/ddamien/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@DataWarehouse.SDK/Hardware/Accelerators/CudaInterop.cs
@DataWarehouse.SDK/Hardware/Accelerators/RocmInterop.cs
@DataWarehouse.SDK/Hardware/Accelerators/GpuAccelerator.cs
</context>

<tasks>

<task type="auto">
  <name>Task 1: OpenCL and SYCL interop</name>
  <files>
    DataWarehouse.SDK/Hardware/Accelerators/OpenClInterop.cs
    DataWarehouse.SDK/Hardware/Accelerators/SyclInterop.cs
    DataWarehouse.SDK/Hardware/Accelerators/IGpuAccelerator.cs
  </files>
  <action>
First, read existing CudaInterop.cs and RocmInterop.cs to match the established P/Invoke pattern.

Update IGpuAccelerator.cs (if needed) to add:
- AcceleratorType enum: add OpenCL, SYCL alongside existing CUDA, ROCm entries
- Ensure IGpuAccelerator has: IsAvailable, DeviceCount, AllocateBuffer, FreeBuffer, CopyToDevice, CopyFromDevice, LaunchKernel

Create OpenClInterop.cs (matching CudaInterop pattern):
- LibraryImport attributes for OpenCL 1.2+ API (libOpenCL.so / OpenCL.dll)
- P/Invoke bindings:
  - clGetPlatformIDs, clGetPlatformInfo
  - clGetDeviceIDs, clGetDeviceInfo
  - clCreateContext, clCreateCommandQueue
  - clCreateBuffer, clEnqueueWriteBuffer, clEnqueueReadBuffer
  - clCreateProgramWithSource, clBuildProgram
  - clCreateKernel, clSetKernelArg, clEnqueueNDRangeKernel
  - clWaitForEvents, clReleaseKernel, clReleaseProgram, clReleaseContext
- OpenClAccelerator : IGpuAccelerator — wraps P/Invoke calls with managed API
- Device enumeration: list all OpenCL platforms and devices (GPU/CPU/FPGA)
- Graceful unavailability: IsAvailable returns false if libOpenCL not found (DllNotFoundException caught)

Create SyclInterop.cs:
- SYCL runs through Intel DPC++ runtime (libsycl.so / sycl.dll) or via OpenCL backend
- P/Invoke bindings for SYCL runtime API (subset):
  - syclDeviceGet, syclDeviceGetInfo
  - syclQueueCreate, syclQueueSubmit
  - syclMallocDevice, syclMallocHost, syclFree
  - syclMemcpy (device<->host)
- SyclAccelerator : IGpuAccelerator
- SYCL is primarily used via kernel compilation — provide LaunchKernel that loads pre-compiled SPIR-V
- Graceful unavailability pattern same as OpenCL
  </action>
  <verify>dotnet build DataWarehouse.SDK/DataWarehouse.SDK.csproj — zero errors</verify>
  <done>OpenCL and SYCL P/Invoke bindings exist with managed wrappers; IGpuAccelerator extended for new types; graceful unavailability</done>
</task>

<task type="auto">
  <name>Task 2: Triton and CANN interop</name>
  <files>
    DataWarehouse.SDK/Hardware/Accelerators/TritonInterop.cs
    DataWarehouse.SDK/Hardware/Accelerators/CannInterop.cs
  </files>
  <action>
Create TritonInterop.cs:
- Triton (OpenAI) is a Python-based GPU kernel compiler — interop via compiled output
- Two integration modes:
  1. Pre-compiled: load Triton-compiled PTX/CUBIN kernels via CUDA driver API (CudaInterop.cuModuleLoad)
  2. Runtime compilation: shell out to triton-compile CLI (if available) then load result
- TritonKernelLoader: LoadKernelAsync(string tritonSourcePath, CancellationToken ct) -> CompiledKernel
- CompiledKernel: BinaryData (byte[]), EntryPoint (string), SharedMemorySize (int)
- TritonAccelerator: wraps kernel loading + execution via underlying CUDA/ROCm
- AcceleratorType.Triton added to enum
- Graceful: if triton-compile not found, log warning and IsAvailable = false

Create CannInterop.cs:
- CANN (Compute Architecture for Neural Networks) — Huawei Ascend NPU
- P/Invoke bindings for CANN C API (libascendcl.so):
  - aclInit, aclFinalize
  - aclrtSetDevice, aclrtGetDeviceCount
  - aclrtMalloc, aclrtFree, aclrtMemcpy
  - aclrtCreateStream, aclrtDestroyStream, aclrtSynchronizeStream
  - aclmdlLoadFromFile (load .om model), aclmdlExecute, aclmdlUnload
- CannAccelerator : IGpuAccelerator
- Device enumeration: aclrtGetDeviceCount, aclrtGetDeviceInfo
- Model execution: load Ascend .om model file and execute inference
- Graceful unavailability: DllNotFoundException -> IsAvailable = false

Both follow the exact same pattern as CudaInterop/RocmInterop:
- internal static partial class with [LibraryImport] attributes
- Public accelerator class wrapping P/Invoke with exception handling
- Registration with GpuAccelerator discovery
  </action>
  <verify>dotnet build DataWarehouse.SDK/DataWarehouse.SDK.csproj — zero errors</verify>
  <done>Triton kernel loader and CANN NPU interop exist; both register as accelerator types; graceful unavailability when hardware absent</done>
</task>

</tasks>

<verification>
- Build passes with zero errors
- AcceleratorType enum has: CUDA, ROCm, OpenCL, SYCL, Triton, CANN
- All 4 new interop files follow established CudaInterop pattern
- IsAvailable returns false gracefully when native libraries absent
</verification>

<success_criteria>
OpenCL, SYCL, Triton, and CANN GPU/accelerator interop bindings exist in SDK/Hardware/Accelerators/, complementing existing CUDA and ROCm support.
</success_criteria>

<output>
After completion, create `.planning/phases/65-infrastructure/65-12-SUMMARY.md`
</output>
