---
phase: 65-infrastructure
plan: 03
type: execute
wave: 2
depends_on: ["65-01"]
files_modified:
  - DataWarehouse.SDK/Contracts/Query/ColumnarEngine.cs
  - DataWarehouse.SDK/Contracts/Query/ColumnarBatch.cs
  - DataWarehouse.SDK/Contracts/Query/ParquetCompatibleWriter.cs
autonomous: true

must_haves:
  truths:
    - "Data is processed in columnar batches for cache-friendly aggregation"
    - "Columnar format is Parquet-compatible (row groups, column chunks, page headers)"
    - "Vectorized operations on columns (SUM, COUNT, MIN, MAX) process batches without per-row overhead"
  artifacts:
    - path: "DataWarehouse.SDK/Contracts/Query/ColumnarBatch.cs"
      provides: "In-memory columnar batch with typed column arrays"
      min_lines: 150
    - path: "DataWarehouse.SDK/Contracts/Query/ColumnarEngine.cs"
      provides: "Columnar scan, filter, aggregate execution"
      min_lines: 300
    - path: "DataWarehouse.SDK/Contracts/Query/ParquetCompatibleWriter.cs"
      provides: "Write columnar batches as Parquet-compatible binary format"
      min_lines: 200
  key_links:
    - from: "ColumnarEngine"
      to: "ColumnarBatch"
      via: "batch processing"
      pattern: "ColumnarBatch"
    - from: "ParquetCompatibleWriter"
      to: "ColumnarBatch"
      via: "serialization"
      pattern: "WriteBatch|WriteRowGroup"
---

<objective>
Build a columnar storage and execution engine that processes data in cache-friendly column batches with Parquet-compatible output.

Purpose: Row-based processing is inefficient for analytical queries (aggregations, scans). Columnar layout enables vectorized processing of typed arrays. Parquet compatibility ensures interop with the broader data ecosystem (Spark, DuckDB, Pandas). This complements the SQL parser (Plan 01) and query planner (Plan 02).

Output: ColumnarBatch (in-memory format), ColumnarEngine (execution), ParquetCompatibleWriter (persistence).
</objective>

<execution_context>
@C:/Users/ddamien/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/ddamien/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Columnar batch format and vectorized operations</name>
  <files>
    DataWarehouse.SDK/Contracts/Query/ColumnarBatch.cs
    DataWarehouse.SDK/Contracts/Query/ColumnarEngine.cs
  </files>
  <action>
Create ColumnarBatch.cs:
- ColumnarBatch class: RowCount (int), Columns (IReadOnlyList of ColumnVector)
- ColumnVector abstract class: Name, DataType (ColumnDataType enum: Int32, Int64, Float64, String, Bool, Binary, Decimal, DateTime, Null), Length, NullBitmap (byte[] — one bit per row)
- TypedColumnVector<T> : ColumnVector with T[] Values array, IsNull(int index) using bitmap
- Concrete vectors: Int32ColumnVector, Int64ColumnVector, Float64ColumnVector, StringColumnVector (string[]), BoolColumnVector, BinaryColumnVector (byte[][]), DecimalColumnVector, DateTimeColumnVector
- ColumnarBatchBuilder: AddColumn(name, type), SetValue(col, row, value), Build() -> ColumnarBatch
- Batch size default 8192 rows (configurable) — chosen for L1/L2 cache friendliness

Create ColumnarEngine.cs:
- ScanBatch(ColumnarBatch batch, List<string> columns) -> ColumnarBatch — projection (select subset of columns)
- FilterBatch(ColumnarBatch batch, Func<int, bool> predicate) -> ColumnarBatch — row-level filter producing new batch
- VectorizedSum(Int64ColumnVector col) -> long — iterate without per-row boxing
- VectorizedSum(Float64ColumnVector col) -> double
- VectorizedCount(ColumnVector col, bool countNulls = false) -> long
- VectorizedMin/VectorizedMax for numeric and DateTime columns
- VectorizedAvg as Sum/Count
- GroupByAggregate(ColumnarBatch batch, List<string> groupKeys, List<AggregateSpec> aggregates) -> ColumnarBatch — hash-based grouping using Dictionary<CompositeKey, AggregateAccumulator[]>
- CompositeKey: wraps object[] with structural equality (GetHashCode/Equals)
- AggregateSpec: ColumnName, Function (Sum/Count/Min/Max/Avg)
- AggregateAccumulator: abstract with typed implementations for each function

Use ArrayPool<T>.Shared for temporary arrays in filter/group operations. Process in batches to stay cache-friendly.
  </action>
  <verify>dotnet build DataWarehouse.SDK/DataWarehouse.SDK.csproj — zero errors</verify>
  <done>ColumnarBatch with 8 typed column vectors exists; ColumnarEngine supports projection, filtering, and vectorized aggregation (Sum/Count/Min/Max/Avg/GroupBy)</done>
</task>

<task type="auto">
  <name>Task 2: Parquet-compatible binary writer</name>
  <files>
    DataWarehouse.SDK/Contracts/Query/ParquetCompatibleWriter.cs
  </files>
  <action>
Create ParquetCompatibleWriter.cs — writes ColumnarBatch data in Parquet-compatible binary format WITHOUT external dependencies (pure C#):

Parquet file structure (implement the minimal viable subset):
- File header: magic bytes "PAR1" (4 bytes)
- Row groups: one per batch (or configurable row group size, default 1M rows)
- Column chunks within row groups: one per column
- Page types: DATA_PAGE only (no dictionary or index pages in v1)
- Encodings: PLAIN encoding for all types (most straightforward)
- Compression: UNCOMPRESSED initially (delegate to UltimateCompression via bus topic for Snappy/Zstd/Gzip)
- Column metadata: type, encoding, compression, num_values, data page offset, data page size
- File footer: FileMetaData with schema, row groups, column chunk metadata
- Footer length (4 bytes, little-endian) + magic "PAR1"

Schema mapping:
- Int32 -> Parquet INT32
- Int64 -> Parquet INT64
- Float64 -> Parquet DOUBLE
- String -> Parquet BYTE_ARRAY
- Bool -> Parquet BOOLEAN
- Binary -> Parquet BYTE_ARRAY
- Decimal -> Parquet FIXED_LEN_BYTE_ARRAY (16 bytes)
- DateTime -> Parquet INT64 (microseconds since Unix epoch)

Write API:
- WriteToStream(Stream output, ColumnarBatch batch, ParquetWriteOptions? options = null)
- WriteToStream(Stream output, IAsyncEnumerable<ColumnarBatch> batches, ParquetWriteOptions? options = null) — streaming multi-batch
- ParquetWriteOptions: RowGroupSize (int), CompressionCodec (None/Snappy/Gzip/Zstd — only None implemented inline, others via bus)

Read API (for round-trip verification):
- ReadFromStream(Stream input) -> IAsyncEnumerable<ColumnarBatch>
- ReadMetadata(Stream input) -> ParquetFileMetadata (schema, row groups, row count)

Use BinaryWriter with little-endian (default on all .NET platforms). Use Thrift compact protocol for metadata (implement minimal subset: struct, list, i32, i64, string, binary field types).

This is NOT a full Parquet library — it's a minimal compatible writer/reader for DW's columnar storage that interops with external Parquet tooling.
  </action>
  <verify>dotnet build DataWarehouse.SDK/DataWarehouse.SDK.csproj — zero errors. Round-trip test: write a ColumnarBatch to MemoryStream, read it back, verify values match</verify>
  <done>ParquetCompatibleWriter can write/read ColumnarBatch data in Parquet-compatible format; minimal Thrift compact protocol for metadata; round-trip preserves all values</done>
</task>

</tasks>

<verification>
- Build passes with zero errors
- ColumnarBatch with 3 columns x 100 rows can be created, projected, filtered, and aggregated
- ParquetCompatibleWriter round-trips data correctly through MemoryStream
- Vectorized SUM on 10K Int64 values produces correct result
</verification>

<success_criteria>
Columnar batch engine with Parquet-compatible I/O exists in SDK/Contracts/Query/. Vectorized aggregation works. Parquet files can be written and read back.
</success_criteria>

<output>
After completion, create `.planning/phases/65-infrastructure/65-03-SUMMARY.md`
</output>
