---
phase: 89-ecosystem-compatibility
plan: 06
type: execute
wave: 2
depends_on: ["89-04"]
files_modified:
  - DataWarehouse.SDK/VirtualDiskEngine/Sql/ArrowColumnarBridge.cs
  - DataWarehouse.SDK/VirtualDiskEngine/Sql/ParquetVdeIntegration.cs
  - Plugins/DataWarehouse.Plugins.UltimateDataFormat/Strategies/Columnar/ArrowFlightStrategy.cs
autonomous: true
must_haves:
  truths:
    - "VDE columnar regions use Arrow memory layout internally for zero-copy interop"
    - "Parquet row group statistics populate VDE zone maps automatically"
    - "Arrow Flight protocol strategy serves data at high throughput using streaming IPC format"
    - "Zero-copy paths exist for Parquet->Arrow->VDE and VDE->Parquet conversions"
  artifacts:
    - path: "DataWarehouse.SDK/VirtualDiskEngine/Sql/ArrowColumnarBridge.cs"
      provides: "Zero-copy bridge between Arrow memory format and VDE columnar regions"
      exports: ["ArrowColumnarBridge"]
    - path: "DataWarehouse.SDK/VirtualDiskEngine/Sql/ParquetVdeIntegration.cs"
      provides: "Parquet row group statistics to zone map population"
      exports: ["ParquetVdeIntegration"]
    - path: "Plugins/DataWarehouse.Plugins.UltimateDataFormat/Strategies/Columnar/ArrowFlightStrategy.cs"
      provides: "Arrow Flight gRPC-based data transfer protocol"
      exports: ["ArrowFlightStrategy"]
  key_links:
    - from: "DataWarehouse.SDK/VirtualDiskEngine/Sql/ArrowColumnarBridge.cs"
      to: "DataWarehouse.SDK/VirtualDiskEngine/Sql/ColumnarRegionEngine.cs"
      via: "Reads/writes VDE columnar regions in Arrow memory format"
      pattern: "ColumnarRegionEngine"
    - from: "DataWarehouse.SDK/VirtualDiskEngine/Sql/ParquetVdeIntegration.cs"
      to: "DataWarehouse.SDK/VirtualDiskEngine/Sql/ZoneMapIndex.cs"
      via: "Populates zone maps from Parquet row group statistics"
      pattern: "ZoneMapIndex"
---

<objective>
Integrate Parquet/Arrow with VDE columnar regions (ECOS-06).

Purpose: VDE columnar regions should use Arrow memory format internally so that reading/writing Parquet or Arrow IPC requires zero or minimal copying. Parquet row group statistics should automatically populate VDE zone maps for predicate pushdown. Arrow Flight provides a high-throughput gRPC-based data transfer protocol.

Output: Arrow-VDE bridge, Parquet-zone map integration, Arrow Flight strategy
</objective>

<execution_context>
@C:/Users/ddamien/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/ddamien/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@DataWarehouse.SDK/VirtualDiskEngine/Sql/ColumnarRegionEngine.cs
@DataWarehouse.SDK/VirtualDiskEngine/Sql/ZoneMapIndex.cs
@DataWarehouse.SDK/Contracts/Query/ColumnarBatch.cs
@DataWarehouse.SDK/Contracts/Query/ParquetCompatibleWriter.cs
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create Arrow-VDE bridge and Parquet zone map integration</name>
  <files>DataWarehouse.SDK/VirtualDiskEngine/Sql/ArrowColumnarBridge.cs, DataWarehouse.SDK/VirtualDiskEngine/Sql/ParquetVdeIntegration.cs</files>
  <action>
**ArrowColumnarBridge.cs** with `[SdkCompatibility("6.0.0", Notes = "Phase 89: Arrow-VDE columnar bridge (ECOS-06)")]`:

Bridge between Arrow IPC memory layout and VDE `ColumnarRegionEngine`:

1. **Arrow Memory Layout representation:**
   - `ArrowBuffer` record: `ReadOnlyMemory<byte> Data`, `int Length`, `int NullCount`, `ReadOnlyMemory<byte> NullBitmap`
   - `ArrowRecordBatch` record: `ArrowSchema Schema`, `IReadOnlyList<ArrowBuffer> Columns`, `int RowCount`
   - `ArrowSchema` record: `IReadOnlyList<ArrowField> Fields`
   - `ArrowField` record: `string Name`, `ArrowDataType Type`, `bool Nullable`
   - `ArrowDataType` enum: `Int32, Int64, Float32, Float64, Utf8, Bool, Binary, Decimal128, Timestamp, Null`

2. **Zero-copy conversion methods:**
   - `static ArrowRecordBatch FromColumnarBatch(ColumnarBatch batch)` — wraps ColumnarBatch column arrays as ArrowBuffers without copying. Uses `MemoryMarshal.AsBytes` for primitive arrays. Strings require offset buffer construction.
   - `static ColumnarBatch ToColumnarBatch(ArrowRecordBatch arrowBatch)` — wraps Arrow buffers as ColumnVector instances. Primitive columns use `MemoryMarshal.Cast`. String columns decode from offset+data buffers.
   - `static ArrowDataType MapFromColumnDataType(ColumnDataType type)` and reverse `MapToColumnDataType`

3. **VDE region integration:**
   - `WriteToRegion(ColumnarRegionEngine region, ArrowRecordBatch batch, CancellationToken ct)` — writes Arrow batch directly to VDE columnar region blocks, preserving Arrow memory layout in block storage
   - `ReadFromRegion(ColumnarRegionEngine region, string tableName, IReadOnlyList<string>? columns, CancellationToken ct)` — reads VDE columnar blocks back as Arrow batches

**ParquetVdeIntegration.cs** with `[SdkCompatibility("6.0.0", Notes = "Phase 89: Parquet-VDE zone map integration (ECOS-06)")]`:

1. **`PopulateZoneMapsFromParquet(ZoneMapIndex zoneMap, ParquetRowGroupStatistics stats, string tableName)`**:
   - For each column in the row group: extract min, max, null_count
   - Create `ZoneMapEntry` with column min/max/null_count and row range
   - Add to `ZoneMapIndex` for the table

2. **`ParquetRowGroupStatistics`** record:
   - `int RowGroupIndex`, `long RowCount`, `long StartRow`
   - `IReadOnlyList<ColumnStatistics> ColumnStats`

3. **`ColumnStatistics`** record:
   - `string ColumnName`, `ColumnDataType Type`
   - `object? MinValue`, `object? MaxValue`, `long NullCount`, `long DistinctCount`

4. **Zero-copy Parquet path:**
   - `ImportParquetToVde(Stream parquetStream, ColumnarRegionEngine region, ZoneMapIndex zoneMap, string tableName, CancellationToken ct)` — reads Parquet -> Arrow (via `ParquetCompatibleWriter.ReadAsync`) -> VDE (via `ArrowColumnarBridge.WriteToRegion`), populating zone maps from row group stats along the way
   - `ExportVdeToParquet(ColumnarRegionEngine region, string tableName, Stream output, ParquetWriteOptions options, CancellationToken ct)` — reads VDE -> Arrow -> Parquet with zone maps exported as row group statistics
  </action>
  <verify>`dotnet build DataWarehouse.SDK/DataWarehouse.SDK.csproj` compiles without errors</verify>
  <done>Arrow-VDE bridge provides zero-copy conversion. Parquet import automatically populates zone maps. Export preserves row group statistics.</done>
</task>

<task type="auto">
  <name>Task 2: Create Arrow Flight protocol strategy</name>
  <files>Plugins/DataWarehouse.Plugins.UltimateDataFormat/Strategies/Columnar/ArrowFlightStrategy.cs</files>
  <action>
Create `ArrowFlightStrategy` extending `DataFormatStrategyBase` with `[SdkCompatibility("6.0.0", Notes = "Phase 89: Arrow Flight protocol (ECOS-06)")]`.

Arrow Flight is a gRPC-based protocol for high-throughput data transfer using Arrow IPC streaming format. This strategy implements the Flight protocol contract:

1. **Strategy identity:**
   - `StrategyId => "arrow-flight"`
   - `DisplayName => "Apache Arrow Flight"`
   - `FormatInfo` with extensions `.flight`, MIME `application/vnd.apache.arrow.flight`

2. **Flight protocol actions:**
   - `GetFlightInfo(FlightDescriptor descriptor, CancellationToken ct)` — returns `FlightInfo` with schema, endpoints, total records, total bytes. Descriptor can be a command (SQL query) or path (table name).
   - `GetSchema(FlightDescriptor descriptor, CancellationToken ct)` — returns Arrow schema for a dataset
   - `DoGet(FlightTicket ticket, CancellationToken ct)` — returns `IAsyncEnumerable<ArrowRecordBatch>` streaming Arrow IPC record batches
   - `DoPut(FlightDescriptor descriptor, IAsyncEnumerable<ArrowRecordBatch> batches, CancellationToken ct)` — ingests Arrow IPC record batches into VDE storage
   - `DoExchange(FlightDescriptor descriptor, IAsyncEnumerable<ArrowRecordBatch> input, CancellationToken ct)` — bidirectional exchange (query with streaming results)
   - `ListFlights(FlightCriteria criteria, CancellationToken ct)` — lists available datasets

3. **Supporting types:**
   - `FlightDescriptor` record: `FlightDescriptorType Type` (Path or Command), `string[] Path`, `string? Command`
   - `FlightInfo` record: `ArrowSchema Schema`, `FlightDescriptor Descriptor`, `IReadOnlyList<FlightEndpoint> Endpoints`, `long TotalRecords`, `long TotalBytes`
   - `FlightEndpoint` record: `FlightTicket Ticket`, `IReadOnlyList<FlightLocation> Locations`
   - `FlightTicket` record: `byte[] Ticket` (opaque token)
   - `FlightLocation` record: `string Uri`
   - `FlightCriteria` record: `byte[]? Expression`

4. **Integration:**
   - `DoGet` uses `ArrowColumnarBridge.ReadFromRegion` to read VDE data as Arrow batches
   - `DoPut` uses `ArrowColumnarBridge.WriteToRegion` to ingest Arrow batches
   - SQL command flights use `PostgreSqlSqlEngineIntegration` to execute queries (if available via message bus)

The strategy implements `ParseCoreAsync` (reads Arrow IPC stream) and `SerializeCoreAsync` (writes Arrow IPC stream) by delegating to `ArrowStrategy` for the IPC format, adding the Flight metadata framing.
  </action>
  <verify>`dotnet build Plugins/DataWarehouse.Plugins.UltimateDataFormat/DataWarehouse.Plugins.UltimateDataFormat.csproj` compiles</verify>
  <done>Arrow Flight strategy provides high-throughput data transfer with DoGet, DoPut, DoExchange, GetFlightInfo, ListFlights using Arrow IPC streaming format backed by VDE storage</done>
</task>

</tasks>

<verification>
- ArrowColumnarBridge zero-copy: FromColumnarBatch uses MemoryMarshal, no buffer copies for primitives
- ParquetVdeIntegration populates ZoneMapIndex entries from row group statistics
- Import path: Parquet -> Arrow -> VDE with zone map population
- Export path: VDE -> Arrow -> Parquet with statistics preservation
- Arrow Flight strategy implements all 6 Flight protocol operations
- Build succeeds for both SDK and UltimateDataFormat
</verification>

<success_criteria>
VDE columnar regions integrate with Arrow memory format. Parquet import populates zone maps. Arrow Flight strategy enables high-throughput data transfer at >=1GB/s potential throughput.
</success_criteria>

<output>
After completion, create `.planning/phases/89-ecosystem-compatibility/89-06-SUMMARY.md`
</output>
