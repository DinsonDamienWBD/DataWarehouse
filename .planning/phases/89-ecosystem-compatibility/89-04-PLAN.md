---
phase: 89-ecosystem-compatibility
plan: 04
type: execute
wave: 1
depends_on: []
files_modified:
  - Plugins/DataWarehouse.Plugins.UltimateDataFormat/Strategies/Columnar/ParquetStrategy.cs
  - Plugins/DataWarehouse.Plugins.UltimateDataFormat/Strategies/Columnar/ArrowStrategy.cs
  - Plugins/DataWarehouse.Plugins.UltimateDataFormat/Strategies/Columnar/OrcStrategy.cs
  - Plugins/DataWarehouse.Plugins.UltimateDataFormat/Strategies/Columnar/ColumnarFormatVerification.cs
autonomous: true
must_haves:
  truths:
    - "Parquet round-trip preserves all ColumnDataType variants with correct encoding"
    - "Arrow IPC read/write handles zero-copy buffer management and schema fidelity"
    - "ORC read/write supports stripe statistics for predicate pushdown"
    - "All three formats handle compression codec delegation via message bus"
  artifacts:
    - path: "Plugins/DataWarehouse.Plugins.UltimateDataFormat/Strategies/Columnar/ParquetStrategy.cs"
      provides: "Verified Parquet read/write with column pruning and row group skipping"
      contains: "class ParquetStrategy"
    - path: "Plugins/DataWarehouse.Plugins.UltimateDataFormat/Strategies/Columnar/ArrowStrategy.cs"
      provides: "Verified Arrow IPC with zero-copy reads"
      contains: "class ArrowStrategy"
    - path: "Plugins/DataWarehouse.Plugins.UltimateDataFormat/Strategies/Columnar/OrcStrategy.cs"
      provides: "Verified ORC with stripe statistics"
      contains: "class OrcStrategy"
    - path: "Plugins/DataWarehouse.Plugins.UltimateDataFormat/Strategies/Columnar/ColumnarFormatVerification.cs"
      provides: "Cross-format verification and compatibility matrix"
      exports: ["ColumnarFormatVerification"]
  key_links:
    - from: "Plugins/DataWarehouse.Plugins.UltimateDataFormat/Strategies/Columnar/ParquetStrategy.cs"
      to: "DataWarehouse.SDK/Contracts/Query/ParquetCompatibleWriter.cs"
      via: "Delegates to SDK ParquetCompatibleWriter for binary format"
      pattern: "ParquetCompatibleWriter"
    - from: "Plugins/DataWarehouse.Plugins.UltimateDataFormat/Strategies/Columnar/ArrowStrategy.cs"
      to: "DataWarehouse.SDK/Contracts/Query/ColumnarBatch.cs"
      via: "Converts between Arrow IPC and ColumnarBatch"
      pattern: "ColumnarBatch"
---

<objective>
Verify and complete Parquet, Arrow, and ORC format strategies (ECOS-03, ECOS-04, ECOS-05).

Purpose: These three columnar formats are the data interop layer — external tools (pandas, PyArrow, Spark, Hive) must be able to read data written by DW and vice versa. This plan audits each strategy for format correctness, fixes gaps in data type handling, compression codec support, and metadata preservation, and creates a cross-format verification class.

Output: Verified Parquet/Arrow/ORC strategies, cross-format verification documentation
</objective>

<execution_context>
@C:/Users/ddamien/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/ddamien/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@Plugins/DataWarehouse.Plugins.UltimateDataFormat/Strategies/Columnar/ParquetStrategy.cs
@Plugins/DataWarehouse.Plugins.UltimateDataFormat/Strategies/Columnar/ArrowStrategy.cs
@Plugins/DataWarehouse.Plugins.UltimateDataFormat/Strategies/Columnar/OrcStrategy.cs
@DataWarehouse.SDK/Contracts/Query/ParquetCompatibleWriter.cs
@DataWarehouse.SDK/Contracts/Query/ColumnarBatch.cs
</context>

<tasks>

<task type="auto">
  <name>Task 1: Audit and fix Parquet and Arrow strategies</name>
  <files>Plugins/DataWarehouse.Plugins.UltimateDataFormat/Strategies/Columnar/ParquetStrategy.cs, Plugins/DataWarehouse.Plugins.UltimateDataFormat/Strategies/Columnar/ArrowStrategy.cs</files>
  <action>
Read the full ParquetStrategy.cs and ArrowStrategy.cs. Audit and fix each:

**ParquetStrategy audit checklist:**
1. `ParseCoreAsync`: Verify it reads PAR1 magic, file metadata (Thrift), row groups, column chunks. Must handle all `ColumnDataType` variants (Int32, Int64, Float64, String, Bool, Binary, Decimal, DateTime, Null). Fix any missing type mappings.
2. `SerializeCoreAsync`: Verify it writes valid Parquet format using `ParquetCompatibleWriter` from SDK. Must include row group statistics (min/max/null_count per column) for predicate pushdown. Fix if statistics are missing.
3. Column pruning: Verify `ParseCoreAsync` accepts column projection (only reading requested columns from row groups). If absent, add `IReadOnlyList<string>? projectedColumns` parameter or use DataFormatOptions.
4. Row group skipping: Verify that row group statistics are checked against filter predicates to skip irrelevant row groups. If absent, add `SkipRowGroup(RowGroupStatistics stats, FilterPredicate? predicate)` method.
5. Compression codecs: Verify Snappy, Gzip, Zstd delegation to UltimateCompression via message bus. The `ParquetCompatibleWriter` already defines `ParquetCompressionCodec` enum — verify the strategy uses it.

**ArrowStrategy audit checklist:**
1. `ParseCoreAsync`: Verify it reads Arrow IPC file format (magic `0xFF 0xFF 0xFF 0xFF` + schema + record batches). Must handle all Arrow logical types mapping to `ColumnDataType`.
2. `SerializeCoreAsync`: Verify it writes valid Arrow IPC format (file magic, schema flatbuffer, record batch flatbuffers, footer).
3. Zero-copy: Verify that `ParseCoreAsync` uses `Memory<byte>` or `ReadOnlyMemory<byte>` slicing instead of buffer copies where possible. If it copies unnecessarily, refactor to use memory slicing.
4. Schema fidelity: Verify that Arrow schema metadata (field names, types, nullable flags) round-trips correctly.
5. Arrow Flight integration point: Add a `SupportsArrowFlight` property returning `true` and an `AsArrowFlightPayload(ColumnarBatch batch)` method that serializes a batch in Arrow IPC streaming format (no file footer, just schema + record batch messages) for use by the Flight protocol.

For each gap found, implement the fix. Mark with `// ECOS-03: Fixed` or `// ECOS-04: Fixed` comments.
  </action>
  <verify>`dotnet build Plugins/DataWarehouse.Plugins.UltimateDataFormat/DataWarehouse.Plugins.UltimateDataFormat.csproj` compiles</verify>
  <done>Parquet handles all data types, row group statistics, column pruning, and row group skipping. Arrow handles zero-copy reads, schema fidelity, and Arrow Flight payload generation.</done>
</task>

<task type="auto">
  <name>Task 2: Audit ORC strategy and create cross-format verification</name>
  <files>Plugins/DataWarehouse.Plugins.UltimateDataFormat/Strategies/Columnar/OrcStrategy.cs, Plugins/DataWarehouse.Plugins.UltimateDataFormat/Strategies/Columnar/ColumnarFormatVerification.cs</files>
  <action>
**OrcStrategy audit** — read the full OrcStrategy.cs and verify:
1. `ParseCoreAsync`: Reads ORC file format (magic "ORC", postscript, footer with stripe info, stripe data). Must handle all ORC types mapping to `ColumnDataType`.
2. `SerializeCoreAsync`: Writes valid ORC format with proper postscript and footer.
3. Stripe statistics: Verify each stripe has column statistics (min, max, count, hasNull) for predicate pushdown. If absent, add `StripeStatistics` record and populate during write.
4. Compression: ORC uses ZLIB, Snappy, LZO, LZ4, ZSTD — verify codec selection delegates to UltimateCompression via message bus.
5. Type mapping: Verify mapping from ORC types (boolean, byte, short, int, long, float, double, string, binary, decimal, date, timestamp, struct, list, map, union) to `ColumnDataType`.

Mark fixes with `// ECOS-05: Fixed` comments.

**ColumnarFormatVerification.cs** — create a verification class with `[SdkCompatibility("6.0.0", Notes = "Phase 89: Columnar format verification (ECOS-03/04/05)")]`:

1. `GetFormatCoverage()` returning `ColumnarFormatCoverage` with:
   - Per-format: data types supported, compression codecs supported, statistics support, column pruning support
   - Cross-format type compatibility matrix (which ColumnDataType maps to which native types in each format)

2. `VerifyRoundTrip(DataFormatStrategyBase strategy, ColumnarBatch testBatch)` method that:
   - Serializes the batch using the strategy
   - Parses it back
   - Compares schema (column names, types, nullable flags)
   - Compares data values row-by-row
   - Returns `RoundTripResult(bool Success, string[] Differences)`

3. `CreateTestBatch()` that generates a `ColumnarBatch` with all 9 `ColumnDataType` variants including nulls.

4. `GetExternalToolCompatibility()` returning compatibility notes for pandas, PyArrow, Spark, Hive for each format.
  </action>
  <verify>`dotnet build Plugins/DataWarehouse.Plugins.UltimateDataFormat/DataWarehouse.Plugins.UltimateDataFormat.csproj` compiles</verify>
  <done>ORC handles stripe statistics and all type mappings. Verification class can test round-trip fidelity for all three formats across all data types.</done>
</task>

</tasks>

<verification>
- All three strategies handle all 9 ColumnDataType variants
- Parquet has row group statistics and column pruning
- Arrow has zero-copy memory slicing and Flight payload support
- ORC has stripe statistics and compression codec delegation
- ColumnarFormatVerification.CreateTestBatch covers all types including nulls
- Build succeeds for UltimateDataFormat project
</verification>

<success_criteria>
Parquet, Arrow, and ORC strategies are verified complete. Round-trip data fidelity confirmed for all data types. External tool compatibility documented.
</success_criteria>

<output>
After completion, create `.planning/phases/89-ecosystem-compatibility/89-04-SUMMARY.md`
</output>
