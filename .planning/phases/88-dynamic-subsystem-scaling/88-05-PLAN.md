---
phase: 88-dynamic-subsystem-scaling
plan: 05
type: execute
wave: 3
depends_on: ["88-01"]
files_modified:
  - Plugins/DataWarehouse.Plugins.UltimateConsensus/Scaling/ConsensusScalingManager.cs
  - Plugins/DataWarehouse.Plugins.UltimateConsensus/Scaling/SegmentedRaftLog.cs
autonomous: true

must_haves:
  truths:
    - "Multi-Raft group support partitions state across groups of <= 100 nodes"
    - "Raft log uses segmented file store (one file per 10K entries, mmap'd hot segments)"
    - "Connection pooling reuses RPC connections across Raft operations"
    - "Election timeouts dynamically adjust based on observed network latency"
  artifacts:
    - path: "Plugins/DataWarehouse.Plugins.UltimateConsensus/Scaling/ConsensusScalingManager.cs"
      provides: "Multi-Raft group management, connection pooling, dynamic election timeouts, IScalableSubsystem"
      exports: ["ConsensusScalingManager"]
    - path: "Plugins/DataWarehouse.Plugins.UltimateConsensus/Scaling/SegmentedRaftLog.cs"
      provides: "IRaftLogStore implementation with segmented files, mmap'd hot segments"
      exports: ["SegmentedRaftLog"]
  key_links:
    - from: "Plugins/DataWarehouse.Plugins.UltimateConsensus/Scaling/SegmentedRaftLog.cs"
      to: "DataWarehouse.SDK/Infrastructure/Distributed/Consensus/IRaftLogStore.cs"
      via: "Implements IRaftLogStore with segmented file backing"
      pattern: "IRaftLogStore"
    - from: "Plugins/DataWarehouse.Plugins.UltimateConsensus/Scaling/ConsensusScalingManager.cs"
      to: "DataWarehouse.SDK/Infrastructure/Distributed/Consensus/MultiRaftManager.cs"
      via: "Extends MultiRaftManager with scaling-aware group partitioning"
      pattern: "MultiRaftManager"
---

<objective>
Scale the Raft consensus subsystem with multi-group support, segmented log storage, connection pooling, and adaptive election timeouts.

Purpose: DSCL-07 -- Raft currently uses a single log and fixed election timeouts. This limits scalability to small clusters and causes unnecessary elections in high-latency environments.

Output: Two new files providing segmented Raft log store and consensus scaling manager.
</objective>

<execution_context>
@C:/Users/ddamien/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/ddamien/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/88-dynamic-subsystem-scaling/88-01-SUMMARY.md
@DataWarehouse.SDK/Infrastructure/Distributed/Consensus/IRaftLogStore.cs
@DataWarehouse.SDK/Infrastructure/Distributed/Consensus/RaftConsensusEngine.cs
@DataWarehouse.SDK/Infrastructure/Distributed/Consensus/MultiRaftManager.cs
@DataWarehouse.SDK/Infrastructure/Distributed/Consensus/FileRaftLogStore.cs
@DataWarehouse.SDK/Contracts/Scaling/IScalableSubsystem.cs
@DataWarehouse.SDK/Contracts/Scaling/BoundedCache.cs
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create SegmentedRaftLog implementing IRaftLogStore</name>
  <files>
    Plugins/DataWarehouse.Plugins.UltimateConsensus/Scaling/SegmentedRaftLog.cs
  </files>
  <action>
Create `SegmentedRaftLog` implementing `IRaftLogStore`:

1. **Segmented file storage**: One file per 10,000 log entries (configurable via `EntriesPerSegment`). Files named by starting index: `raft-log-{groupId}-{startIndex}.seg`. Entries serialized with length-prefix binary format for fast sequential read.

2. **Memory-mapped hot segments**: The most recent N segments (default 3) are memory-mapped for zero-copy reads. Older segments are file-backed, loaded on demand. Use `MemoryMappedFile` with `MemoryMappedViewAccessor`.

3. **Segment lifecycle**: When active segment fills, seal it (set read-only), create new segment, update segment index. Background task compacts old segments (merge small, remove truncated entries).

4. **Performance**: Sequential append is O(1) to current segment. Random read by index is O(1) via segment index lookup. Truncation marks entries as deleted and compacts in background.

5. **Integration with existing IRaftLogStore**: Implement all methods from `IRaftLogStore` (AppendAsync, GetAsync, TruncateAsync, GetLastIndexAsync, GetTermForIndexAsync). Reference existing `FileRaftLogStore.cs` for the contract expectations.

`[SdkCompatibility("6.0.0")]` on all public types. Full XML docs.
  </action>
  <verify>
`dotnet build Plugins/DataWarehouse.Plugins.UltimateConsensus/DataWarehouse.Plugins.UltimateConsensus.csproj` compiles with 0 errors. Grep for `IRaftLogStore` in SegmentedRaftLog to confirm interface implementation.
  </verify>
  <done>SegmentedRaftLog implements IRaftLogStore with 10K-entry file segments, mmap'd hot segments, O(1) append and lookup, background compaction.</done>
</task>

<task type="auto">
  <name>Task 2: Create ConsensusScalingManager with multi-Raft groups and connection pooling</name>
  <files>
    Plugins/DataWarehouse.Plugins.UltimateConsensus/Scaling/ConsensusScalingManager.cs
  </files>
  <action>
Create `ConsensusScalingManager` implementing `IScalableSubsystem`:

1. **Multi-Raft group support**: Partition state across multiple Raft groups. Each group handles <= 100 nodes (configurable via `MaxNodesPerGroup`). A consistent hash ring maps keys to groups. New groups created automatically when node count exceeds threshold. Reference existing `MultiRaftManager.cs` for the group management foundation.

2. **Connection pooling**: Maintain a pool of reusable TCP/gRPC connections per peer node. Pool size configurable (default: `MaxConnectionsPerPeer = 4`). Connections validated with heartbeat before reuse. Idle connections recycled after configurable timeout (default 5 min). Use `BoundedCache<string, ConnectionPool>` for pool management.

3. **Dynamic election timeouts**: Measure network RTT to each peer using heartbeat responses. Set election timeout to `max(MinElectionTimeout, observedP99Rtt * ElectionTimeoutMultiplier)`. Default: `MinElectionTimeout = 150ms`, `ElectionTimeoutMultiplier = 10`. Recalculate every 30s from sliding window of RTT measurements.

4. **IScalableSubsystem**:
   - `GetScalingMetrics()`: group count, nodes per group, log sizes per group, election count, connection pool utilization, RTT percentiles
   - `ReconfigureLimitsAsync()`: change MaxNodesPerGroup, MaxConnectionsPerPeer, election timeout bounds at runtime
   - `CurrentLimits`: reflects active configuration

5. Wire into `UltimateConsensusPlugin`: replace default log store with `SegmentedRaftLog`, register scaling manager.

`[SdkCompatibility("6.0.0")]` on all public types. Full XML docs.
  </action>
  <verify>
`dotnet build Plugins/DataWarehouse.Plugins.UltimateConsensus/DataWarehouse.Plugins.UltimateConsensus.csproj` compiles with 0 errors. Grep for `MultiRaftManager` and `SegmentedRaftLog` in ConsensusScalingManager to verify integration.
  </verify>
  <done>ConsensusScalingManager provides multi-Raft group partitioning with consistent hash routing. Connection pool reuses per-peer connections. Election timeouts adapt to observed RTT. All limits runtime-reconfigurable via IScalableSubsystem.</done>
</task>

</tasks>

<verification>
- Full solution builds with 0 errors
- SegmentedRaftLog implements all IRaftLogStore methods
- Multi-Raft groups partition at 100-node boundary
- Connection pool uses BoundedCache
- Dynamic election timeout computation references RTT measurements
</verification>

<success_criteria>
Raft consensus scales with multi-group partitioning, segmented log store with mmap'd hot segments, connection pooling, and adaptive election timeouts. Solution builds clean.
</success_criteria>

<output>
After completion, create `.planning/phases/88-dynamic-subsystem-scaling/88-05-SUMMARY.md`
</output>
