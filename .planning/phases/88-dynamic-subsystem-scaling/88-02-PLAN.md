---
phase: 88-dynamic-subsystem-scaling
plan: 02
type: execute
wave: 2
depends_on: ["88-01"]
files_modified:
  - Plugins/DataWarehouse.Plugins.UltimateStreamingData/Scaling/StreamingScalingManager.cs
  - Plugins/DataWarehouse.Plugins.UltimateStreamingData/Scaling/StreamingBackpressureHandler.cs
autonomous: true

must_haves:
  truths:
    - "PublishAsync produces real events to configured stream strategies"
    - "SubscribeAsync consumes real events with configurable consumer groups"
    - "ScalabilityConfig drives actual auto-scaling of partition count and consumer instances"
    - "Backpressure engages when queue depth exceeds configured threshold"
    - "Checkpoint storage enables exactly-once processing semantics"
  artifacts:
    - path: "Plugins/DataWarehouse.Plugins.UltimateStreamingData/Scaling/StreamingScalingManager.cs"
      provides: "Wires ScalabilityConfig to actual partition auto-scaling, checkpoint storage, consumer group management"
      exports: ["StreamingScalingManager"]
    - path: "Plugins/DataWarehouse.Plugins.UltimateStreamingData/Scaling/StreamingBackpressureHandler.cs"
      provides: "Implements IBackpressureAware for streaming with adaptive rate limiting"
      exports: ["StreamingBackpressureHandler"]
  key_links:
    - from: "Plugins/DataWarehouse.Plugins.UltimateStreamingData/Scaling/StreamingScalingManager.cs"
      to: "DataWarehouse.SDK/Contracts/Scaling/IScalableSubsystem.cs"
      via: "Implements IScalableSubsystem for streaming subsystem"
      pattern: "IScalableSubsystem"
    - from: "Plugins/DataWarehouse.Plugins.UltimateStreamingData/Scaling/StreamingBackpressureHandler.cs"
      to: "DataWarehouse.SDK/Contracts/Scaling/IBackpressureAware.cs"
      via: "Implements IBackpressureAware with streaming-specific strategies"
      pattern: "IBackpressureAware"
---

<objective>
Fix the critical streaming stub: implement real PublishAsync/SubscribeAsync, wire ScalabilityConfig to actual auto-scaling, add checkpoint storage for exactly-once processing, and implement backpressure.

Purpose: DSCL-10 -- streaming is currently stubbed out. This is a critical bug fix that must work before other subsystems can rely on streaming for event distribution.

Output: Two new files providing streaming scaling manager and backpressure handler, plus modifications to UltimateStreamingDataPlugin to wire them in.
</objective>

<execution_context>
@C:/Users/ddamien/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/ddamien/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/88-dynamic-subsystem-scaling/88-01-SUMMARY.md
@Plugins/DataWarehouse.Plugins.UltimateStreamingData/UltimateStreamingDataPlugin.cs
@Plugins/DataWarehouse.Plugins.UltimateStreamingData/Features/BackpressureHandling.cs
@Plugins/DataWarehouse.Plugins.UltimateStreamingData/Features/StatefulStreamProcessing.cs
@DataWarehouse.SDK/Contracts/Scaling/IScalableSubsystem.cs
@DataWarehouse.SDK/Contracts/Scaling/BoundedCache.cs
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement StreamingScalingManager with real PublishAsync/SubscribeAsync</name>
  <files>
    Plugins/DataWarehouse.Plugins.UltimateStreamingData/Scaling/StreamingScalingManager.cs
  </files>
  <action>
Create `StreamingScalingManager` implementing `IScalableSubsystem` in `Plugins/DataWarehouse.Plugins.UltimateStreamingData/Scaling/`:

1. **Real PublishAsync**: Dispatch events to the active streaming strategy (Kafka/Pulsar/NATS/RabbitMQ/etc.) via the strategy registry. Route to the correct partition based on event key hash. Track publish metrics (throughput, latency, errors).

2. **Real SubscribeAsync**: Create consumer group subscriptions on the active strategy. Support configurable consumer group IDs, partition assignment, and offset management. Return `IAsyncEnumerable<StreamEvent>` for consumption.

3. **ScalabilityConfig wiring**: Read `ScalabilityConfig` from plugin configuration. Use it to:
   - Set `MaxPartitions` and auto-scale partition count when throughput exceeds `ScaleUpThreshold`
   - Set `MaxConsumers` per consumer group and auto-add consumers when lag exceeds `MaxLagMs`
   - Set `TargetThroughputPerPartition` for rebalancing decisions

4. **Checkpoint storage**: Store consumer offsets per partition per consumer group using `IPersistentBackingStore`. On consumer restart, resume from last committed checkpoint. Support both auto-commit (interval-based) and manual commit.

5. Implement `IScalableSubsystem`:
   - `GetScalingMetrics()` returns partition count, consumer count, throughput, lag, checkpoint positions
   - `ReconfigureLimitsAsync()` allows runtime changes to MaxPartitions, MaxConsumers, buffer sizes
   - `CurrentLimits` reflects active scaling configuration
   - `CurrentBackpressureState` delegates to StreamingBackpressureHandler

6. Use `BoundedCache<string, StreamCheckpoint>` for checkpoint cache with write-through to backing store.

`[SdkCompatibility("6.0.0")]` on all public types. Full XML docs.

Wire into `UltimateStreamingDataPlugin` by modifying its `InitializeAsync` to create and register the `StreamingScalingManager`. Replace any existing stub `PublishAsync`/`SubscribeAsync` implementations with calls through the scaling manager.
  </action>
  <verify>
`dotnet build Plugins/DataWarehouse.Plugins.UltimateStreamingData/DataWarehouse.Plugins.UltimateStreamingData.csproj` compiles with 0 errors. Grep for `PublishAsync` in StreamingScalingManager to confirm real implementation exists (not `Task.CompletedTask` or `NotImplementedException`).
  </verify>
  <done>PublishAsync dispatches to active strategy with partition routing. SubscribeAsync creates real consumer groups with offset tracking. ScalabilityConfig drives partition and consumer auto-scaling. Checkpoints persist via IPersistentBackingStore.</done>
</task>

<task type="auto">
  <name>Task 2: Implement StreamingBackpressureHandler</name>
  <files>
    Plugins/DataWarehouse.Plugins.UltimateStreamingData/Scaling/StreamingBackpressureHandler.cs
  </files>
  <action>
Create `StreamingBackpressureHandler` implementing `IBackpressureAware`:

1. **Strategy implementations**:
   - `DropOldest`: When queue depth exceeds limit, discard oldest unprocessed events (log discards to observability)
   - `BlockProducer`: Return `Task` from PublishAsync that blocks (via SemaphoreSlim) until queue depth drops below threshold
   - `ShedLoad`: Reject new publishes with a specific error code when load exceeds capacity; caller can retry
   - `DegradeQuality`: Reduce processing fidelity (skip enrichment, batch more aggressively, reduce ack guarantees)
   - `Adaptive`: Auto-switch between strategies based on observed pressure using exponential moving average of queue depth

2. **Pressure detection**:
   - Monitor queue depth via `Interlocked` counters (no locks on hot path)
   - Three thresholds: `WarningThreshold` (default 70%), `CriticalThreshold` (default 85%), `SheddingThreshold` (default 95%)
   - Thresholds configurable at runtime via `ReconfigureLimitsAsync`

3. **Integration with BackpressureHandling.cs**: Reference the existing `BackpressureHandling` feature in the streaming plugin. The new handler should delegate to or wrap existing backpressure logic where it exists, extending it with the IBackpressureAware contract.

4. Fire `OnBackpressureChanged` event when state transitions occur. Include `SubsystemName = "UltimateStreamingData"`.

`[SdkCompatibility("6.0.0")]` on all public types. Full XML docs.
  </action>
  <verify>
`dotnet build Plugins/DataWarehouse.Plugins.UltimateStreamingData/DataWarehouse.Plugins.UltimateStreamingData.csproj` compiles with 0 errors. Grep for `BackpressureStrategy` in the handler to confirm all five strategies are implemented.
  </verify>
  <done>StreamingBackpressureHandler implements all 5 backpressure strategies. Pressure detection uses lock-free counters. Thresholds are runtime-configurable. State change events fire on transitions. Integrates with existing BackpressureHandling feature.</done>
</task>

</tasks>

<verification>
- Full solution builds: `dotnet build` with 0 errors
- `PublishAsync` in StreamingScalingManager contains real dispatch logic (not stubs)
- `SubscribeAsync` returns `IAsyncEnumerable` with real consumer group logic
- ScalabilityConfig fields are read and applied (not ignored)
- Checkpoint storage uses `BoundedCache` + `IPersistentBackingStore`
- All five backpressure strategies have implementations (not `throw new NotImplementedException()`)
</verification>

<success_criteria>
Streaming plugin has real PublishAsync/SubscribeAsync with strategy dispatch, auto-scaling from ScalabilityConfig, persistent checkpoints, and five backpressure strategies. Solution builds clean.
</success_criteria>

<output>
After completion, create `.planning/phases/88-dynamic-subsystem-scaling/88-02-SUMMARY.md`
</output>
