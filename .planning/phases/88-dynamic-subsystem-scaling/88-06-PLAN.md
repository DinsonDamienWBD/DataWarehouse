---
phase: 88-dynamic-subsystem-scaling
plan: 06
type: execute
wave: 3
depends_on: ["88-01"]
files_modified:
  - DataWarehouse.SDK/Infrastructure/Scaling/ScalableMessageBus.cs
  - DataWarehouse.SDK/Infrastructure/Scaling/MessageBusBackpressure.cs
  - DataWarehouse.SDK/Infrastructure/Scaling/WalMessageQueue.cs
autonomous: true

must_haves:
  truths:
    - "KernelLimitsConfig can be reconfigured at runtime without restart"
    - "Persistent WAL-backed queue survives process crash without message loss"
    - "Topic partitioning distributes load across multiple internal queues"
    - "Backpressure signals producers when queue depth exceeds threshold"
    - "Disruptor ring buffer achieves >= 1M msgs/sec on hot paths"
  artifacts:
    - path: "DataWarehouse.SDK/Infrastructure/Scaling/ScalableMessageBus.cs"
      provides: "Message bus wrapper with runtime reconfiguration, topic partitioning, Disruptor hot path"
      exports: ["ScalableMessageBus"]
    - path: "DataWarehouse.SDK/Infrastructure/Scaling/MessageBusBackpressure.cs"
      provides: "IBackpressureAware for message bus with producer signaling"
      exports: ["MessageBusBackpressure"]
    - path: "DataWarehouse.SDK/Infrastructure/Scaling/WalMessageQueue.cs"
      provides: "WAL-backed persistent message queue for durability"
      exports: ["WalMessageQueue"]
  key_links:
    - from: "DataWarehouse.SDK/Infrastructure/Scaling/ScalableMessageBus.cs"
      to: "DataWarehouse.SDK/Contracts/IMessageBus.cs"
      via: "Wraps existing IMessageBus with scaling capabilities"
      pattern: "IMessageBus"
    - from: "DataWarehouse.SDK/Infrastructure/Scaling/WalMessageQueue.cs"
      to: "DataWarehouse.SDK/Contracts/Persistence/IPersistentBackingStore.cs"
      via: "WAL entries persist through IPersistentBackingStore"
      pattern: "IPersistentBackingStore"
---

<objective>
Scale the message bus with runtime reconfiguration, WAL-backed persistence, topic partitioning, backpressure, and Disruptor integration for hot paths.

Purpose: DSCL-08 -- the message bus is the communication backbone for all 60 plugins. It must handle 1M+ msgs/sec, survive crashes, and signal backpressure to prevent memory exhaustion.

Output: Three new SDK infrastructure files for scalable message bus, backpressure, and WAL queue.
</objective>

<execution_context>
@C:/Users/ddamien/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/ddamien/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/88-dynamic-subsystem-scaling/88-01-SUMMARY.md
@DataWarehouse.SDK/Contracts/IMessageBus.cs
@DataWarehouse.SDK/Infrastructure/KernelInfrastructure.cs
@DataWarehouse.SDK/Contracts/Scaling/IScalableSubsystem.cs
@DataWarehouse.SDK/Contracts/Scaling/IBackpressureAware.cs
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create ScalableMessageBus with runtime reconfiguration and topic partitioning</name>
  <files>
    DataWarehouse.SDK/Infrastructure/Scaling/ScalableMessageBus.cs
  </files>
  <action>
Create `ScalableMessageBus` implementing `IScalableSubsystem` that wraps the existing `IMessageBus`:

1. **Runtime reconfiguration**: `KernelLimitsConfig` values (MaxSubscribersPerTopic, MaxMessageQueueSize, MaxTopics) can be changed at runtime via `ReconfigureLimitsAsync`. Changes take effect immediately without restart. No message loss during reconfiguration (double-buffer swap).

2. **Topic partitioning**: Each topic can have N partitions (configurable, default based on subscriber count). Messages are routed to partitions by hash of message key (or round-robin if no key). Each partition has its own internal queue. Subscribers are assigned to partitions for load distribution.

3. **Disruptor hot path**: For topics with `[HotPath]` attribute or explicitly configured hot topics, use a lock-free ring buffer (Disruptor pattern) instead of `ConcurrentQueue`. Ring buffer size: power of 2, default 65536. Single producer / multi-consumer. Sequence barriers for coordination. Fallback to standard queue if ring buffer is full (no data loss).

4. **Metrics**: `GetScalingMetrics()` returns: topic count, messages/sec, queue depths per partition, subscriber counts, Disruptor sequence positions, drop counts.

5. **Integration**: The `ScalableMessageBus` decorates the existing `IMessageBus` implementation. All existing publish/subscribe calls pass through transparently with added partitioning and backpressure. Wire into `KernelInfrastructure` as an optional upgrade.

`[SdkCompatibility("6.0.0")]` on all public types. Full XML docs.
  </action>
  <verify>
`dotnet build DataWarehouse.SDK/DataWarehouse.SDK.csproj` compiles with 0 errors. Grep for `Disruptor` or `RingBuffer` in ScalableMessageBus to confirm hot-path implementation.
  </verify>
  <done>ScalableMessageBus wraps IMessageBus with runtime-reconfigurable limits, topic partitioning with hash-based routing, and Disruptor ring buffer for hot paths. All metrics exposed via IScalableSubsystem.</done>
</task>

<task type="auto">
  <name>Task 2: Create WalMessageQueue and MessageBusBackpressure</name>
  <files>
    DataWarehouse.SDK/Infrastructure/Scaling/WalMessageQueue.cs
    DataWarehouse.SDK/Infrastructure/Scaling/MessageBusBackpressure.cs
  </files>
  <action>
**WalMessageQueue.cs** -- WAL-backed persistent message queue:
1. Append-only WAL file per topic partition. Messages serialized with length-prefix binary format.
2. Consumer offset tracked per subscriber in a separate offset file.
3. On process restart, replay from last committed offset. Messages retained until all subscribers have consumed them.
4. Configurable retention: by time (default 24h), by size (default 1GB per partition), or by consumer acknowledgment.
5. Background compaction removes fully-consumed messages.
6. Use `IPersistentBackingStore` for WAL file storage (supports file, VDE, or DB backing).
7. Durability guarantee: fsync on commit (configurable: every message, every N messages, or timed).

**MessageBusBackpressure.cs** -- `IBackpressureAware` for message bus:
1. Monitors aggregate queue depth across all partitions.
2. Producer signaling: when in `Warning` state, adds `X-Backpressure: warning` header to published messages. When in `Critical` state, returns `Task` from PublishAsync that delays (configurable: 10ms-1s) before completing. When in `Shedding` state, rejects publishes for non-critical topics (topics without `[Critical]` attribute).
3. Consumer-side: accelerates consumption by temporarily disabling batching in `Critical` state.
4. All thresholds configurable at runtime. Default: Warning at 70% capacity, Critical at 85%, Shedding at 95%.

`[SdkCompatibility("6.0.0")]` on all public types. Full XML docs.
  </action>
  <verify>
`dotnet build DataWarehouse.SDK/DataWarehouse.SDK.csproj` compiles with 0 errors. Grep for `fsync|FlushAsync` in WalMessageQueue to verify durability. Grep for `BackpressureState` in MessageBusBackpressure to verify all states handled.
  </verify>
  <done>WalMessageQueue provides crash-durable message persistence with WAL, consumer offset tracking, and configurable retention. MessageBusBackpressure signals producers and degrades gracefully through Warning/Critical/Shedding states.</done>
</task>

</tasks>

<verification>
- Full solution builds with 0 errors
- ScalableMessageBus decorates existing IMessageBus (does not replace)
- WAL queue uses append-only files with offset tracking
- Disruptor ring buffer implemented for hot paths
- Backpressure has all three escalation levels (Warning, Critical, Shedding)
</verification>

<success_criteria>
Message bus scales with runtime reconfiguration, topic partitioning, WAL-backed persistence, Disruptor hot paths, and three-level backpressure signaling. Solution builds clean.
</success_criteria>

<output>
After completion, create `.planning/phases/88-dynamic-subsystem-scaling/88-06-SUMMARY.md`
</output>
