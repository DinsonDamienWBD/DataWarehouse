---
phase: 37-multi-environment-deployment
plan: 04
type: execute
wave: 2
depends_on: []
files_modified:
  - DataWarehouse.SDK/Deployment/CloudDetector.cs
  - DataWarehouse.SDK/Deployment/HyperscaleProvisioner.cs
  - DataWarehouse.SDK/Deployment/CloudProviders/ICloudProvider.cs
  - DataWarehouse.SDK/Deployment/CloudProviders/CloudProviderFactory.cs
  - DataWarehouse.SDK/Deployment/CloudProviders/AwsProvider.cs
  - DataWarehouse.SDK/Deployment/CloudProviders/AzureProvider.cs
  - DataWarehouse.SDK/Deployment/CloudProviders/GcpProvider.cs
  - DataWarehouse.SDK/Deployment/CloudProviders/VmSpec.cs
  - DataWarehouse.SDK/Deployment/CloudProviders/StorageSpec.cs
  - DataWarehouse.SDK/Deployment/CloudProviders/CloudResourceMetrics.cs
autonomous: true

must_haves:
  truths:
    - "CloudDetector identifies AWS/Azure/GCP via metadata endpoint (169.254.169.254) with 500ms timeout"
    - "ICloudProvider abstracts cloud-specific provisioning (VM, storage, metrics) for AWS/Azure/GCP"
    - "Cloud provider SDKs loaded dynamically via PluginAssemblyLoadContext to avoid bloating base install"
    - "AwsProvider provisions EC2 instances, EBS volumes, S3 buckets via AWS SDK for .NET"
    - "AzureProvider provisions VMs, Managed Disks, Blob storage via Azure SDK"
    - "GcpProvider provisions Compute instances, Persistent Disks, Cloud Storage via Google Cloud Client Libraries"
    - "HyperscaleProvisioner orchestrates auto-scaling policies (add node when storage >80%, remove when <40%)"
    - "All cloud API calls use exponential backoff for rate limiting and retries"
  artifacts:
    - path: "DataWarehouse.SDK/Deployment/CloudDetector.cs"
      provides: "Cloud environment detection via metadata endpoints"
      min_lines: 120
    - path: "DataWarehouse.SDK/Deployment/HyperscaleProvisioner.cs"
      provides: "Cloud provisioning orchestrator with auto-scaling"
      min_lines: 250
    - path: "DataWarehouse.SDK/Deployment/CloudProviders/ICloudProvider.cs"
      provides: "Cloud provider abstraction interface"
      min_lines: 80
    - path: "DataWarehouse.SDK/Deployment/CloudProviders/AwsProvider.cs"
      provides: "AWS EC2/EBS/S3 provisioning via AWS SDK"
      min_lines: 350
    - path: "DataWarehouse.SDK/Deployment/CloudProviders/AzureProvider.cs"
      provides: "Azure VM/Disk/Blob provisioning via Azure SDK"
      min_lines: 350
    - path: "DataWarehouse.SDK/Deployment/CloudProviders/GcpProvider.cs"
      provides: "GCP Compute/PD/Storage provisioning via Google Cloud Client Libraries"
      min_lines: 350
  key_links:
    - from: "CloudDetector"
      to: "HttpClient"
      via: "HTTP request to 169.254.169.254 metadata endpoint with 500ms timeout"
      pattern: "169\\.254\\.169\\.254"
    - from: "CloudProviderFactory"
      to: "PluginAssemblyLoadContext"
      via: "Dynamic SDK loading for cloud providers to avoid bloat"
      pattern: "PluginAssemblyLoadContext"
    - from: "AwsProvider"
      to: "AWSSDK.EC2"
      via: "AWS SDK for EC2 instance provisioning"
      pattern: "IAmazonEC2"
---

<objective>
Implement hyperscale cloud deployment automation (ENV-04) -- detect cloud provider (AWS/Azure/GCP) via metadata endpoints, abstract cloud provisioning behind ICloudProvider, dynamically load cloud SDKs, and orchestrate auto-scaling policies for storage nodes.

Purpose: Hyperscale deployments require automatic node provisioning as storage grows. Auto-scaling policies add nodes when storage utilization exceeds thresholds (e.g., >80%), remove nodes when underutilized (<40%), and optimize costs by selecting appropriate instance types and storage tiers. Cloud provider SDKs (AWS SDK, Azure SDK, GCP Client Libraries) are loaded dynamically to avoid bloating the base DataWarehouse installation.

Output:
- `CloudDetector` for cloud provider detection via metadata
- `ICloudProvider` interface abstracting cloud provisioning
- `AwsProvider`, `AzureProvider`, `GcpProvider` implementations
- `CloudProviderFactory` for dynamic SDK loading
- `HyperscaleProvisioner` for auto-scaling orchestration
- `VmSpec`, `StorageSpec`, `CloudResourceMetrics` supporting types
</objective>

<execution_context>
@C:/Users/ddamien/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/ddamien/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/ROADMAP.md
@.planning/REQUIREMENTS-v3.md
@.planning/STATE.md
@.planning/phases/37-multi-environment-deployment/37-RESEARCH.md

@DataWarehouse.SDK/Infrastructure/KernelInfrastructure.cs
</context>

<tasks>

<task type="auto">
  <name>Task 1: Cloud Provider Detection via Metadata Endpoints</name>
  <files>
    DataWarehouse.SDK/Deployment/CloudDetector.cs
  </files>
  <action>
Create in namespace `DataWarehouse.SDK.Deployment`.

Sealed class implementing `IDeploymentDetector`. Mark with `[SdkCompatibility("3.0.0", Notes = "Phase 37: Cloud provider detection (ENV-04)")]`.

**Metadata endpoint URLs (standard cloud-init pattern):**

```csharp
private const string AwsMetadataUrl = "http://169.254.169.254/latest/meta-data/";
private const string AzureMetadataUrl = "http://169.254.169.254/metadata/instance?api-version=2021-02-01";
private const string GcpMetadataUrl = "http://metadata.google.internal/computeMetadata/v1/";
```

`DetectAsync` implementation:

1. **Create HttpClient with aggressive timeout:**
   ```csharp
   using var client = new HttpClient { Timeout = TimeSpan.FromMilliseconds(500) };
   ```
   - **Why 500ms:** Cloud metadata responds instantly on cloud VMs (<10ms), non-cloud systems timeout immediately instead of waiting 30+ seconds

2. **Try AWS detection:**
   - Send GET request to `AwsMetadataUrl + "instance-id"`
   - If response is non-empty string:
     - Fetch additional metadata: region (`placement/region`), instance type (`instance-type`)
     - Construct DeploymentContext:
       - `Environment = DeploymentEnvironment.HyperscaleCloud`
       - `CloudProvider = "AWS"`
       - `Metadata = { ["InstanceId"] = instanceId, ["Region"] = region, ["InstanceType"] = instanceType }`
     - Return context
   - Catch `HttpRequestException`, `TaskCanceledException` -- continue to next provider

3. **Try Azure detection:**
   - Add required header: `client.DefaultRequestHeaders.Add("Metadata", "true")`
   - Send GET request to `AzureMetadataUrl`
   - If response is non-empty JSON:
     - Parse JSON using System.Text.Json
     - Extract: vmId, location, vmSize from `compute` section
     - Construct DeploymentContext:
       - `Environment = DeploymentEnvironment.HyperscaleCloud`
       - `CloudProvider = "Azure"`
       - `Metadata = { ["VmId"] = vmId, ["Location"] = location, ["VmSize"] = vmSize }`
     - Return context
   - Catch exceptions -- continue to next provider

4. **Try GCP detection:**
   - Clear headers, add required header: `client.DefaultRequestHeaders.Add("Metadata-Flavor", "Google")`
   - Send GET request to `GcpMetadataUrl + "instance/id"`
   - If response is non-empty string:
     - Fetch additional metadata: zone (`instance/zone`), machine-type (`instance/machine-type`)
     - Construct DeploymentContext:
       - `Environment = DeploymentEnvironment.HyperscaleCloud`
       - `CloudProvider = "GCP"`
       - `Metadata = { ["InstanceId"] = instanceId, ["Zone"] = zone, ["MachineType"] = machineType }`
     - Return context
   - Catch exceptions -- continue

5. **No cloud provider detected:**
   - Return null (not cloud environment)

Wrap all HTTP calls in try/catch with specific exception handling for timeout vs network errors.

Add comprehensive XML documentation explaining:
- Detection method (metadata endpoint HTTP requests with 500ms timeout)
- Why timeout is aggressive (prevents 30s hang on non-cloud systems)
- Cloud provider differences (AWS: no headers, Azure: Metadata header, GCP: Metadata-Flavor header)
- Return value (null if not cloud, DeploymentContext with provider if cloud)
  </action>
  <verify>
Run `dotnet build DataWarehouse.SDK/DataWarehouse.SDK.csproj` -- zero new errors. Verify file exists at `DataWarehouse.SDK/Deployment/CloudDetector.cs`. Grep for `IDeploymentDetector` implementation. Grep for `169.254.169.254` metadata endpoint URL. Grep for AWS/Azure/GCP detection attempts. Grep for 500ms timeout. Grep for metadata header requirements (Azure: "Metadata", GCP: "Metadata-Flavor").
  </verify>
  <done>
CloudDetector implements IDeploymentDetector, tries AWS/Azure/GCP metadata endpoints with 500ms timeout, extracts instance ID/region/location/zone from metadata, returns DeploymentContext with HyperscaleCloud environment and detected cloud provider, returns null on non-cloud systems.
  </done>
</task>

<task type="auto">
  <name>Task 2: Cloud Provider Abstraction & Supporting Types</name>
  <files>
    DataWarehouse.SDK/Deployment/CloudProviders/VmSpec.cs
    DataWarehouse.SDK/Deployment/CloudProviders/StorageSpec.cs
    DataWarehouse.SDK/Deployment/CloudProviders/CloudResourceMetrics.cs
    DataWarehouse.SDK/Deployment/CloudProviders/ICloudProvider.cs
  </files>
  <action>
Create in namespace `DataWarehouse.SDK.Deployment.CloudProviders`.

**File 1: VmSpec.cs**

```csharp
[SdkCompatibility("3.0.0", Notes = "Phase 37: Cloud VM specification (ENV-04)")]
public sealed record VmSpec
{
    public string? ImageId { get; init; }          // AMI ID, Azure image URN, GCP image family
    public string InstanceType { get; init; } = "general-purpose"; // t3.medium, Standard_D2s_v3, n2-standard-2
    public int StorageGb { get; init; } = 100;     // Root volume size
    public string? Region { get; init; }           // Deployment region/location/zone
    public IReadOnlyDictionary<string, string> Tags { get; init; } = ImmutableDictionary<string, string>.Empty;
    public string? SecurityGroup { get; init; }    // Security group/firewall rules
    public string? SubnetId { get; init; }         // VPC subnet ID
}
```

**File 2: StorageSpec.cs**

```csharp
[SdkCompatibility("3.0.0", Notes = "Phase 37: Cloud storage specification (ENV-04)")]
public sealed record StorageSpec
{
    public string StorageType { get; init; } = "block"; // "block", "object"
    public long SizeGb { get; init; }
    public string? PerformanceTier { get; init; } // "standard", "premium", "ultra"
    public bool Encrypted { get; init; } = true;
    public string? AvailabilityZone { get; init; }
    public IReadOnlyDictionary<string, string> Tags { get; init; } = ImmutableDictionary<string, string>.Empty;
}
```

**File 3: CloudResourceMetrics.cs**

```csharp
[SdkCompatibility("3.0.0", Notes = "Phase 37: Cloud resource metrics (ENV-04)")]
public sealed record CloudResourceMetrics
{
    public string ResourceId { get; init; }
    public double CpuUtilizationPercent { get; init; }
    public double MemoryUtilizationPercent { get; init; }
    public double StorageUtilizationPercent { get; init; }
    public long NetworkInBytesPerSec { get; init; }
    public long NetworkOutBytesPerSec { get; init; }
    public long DiskReadBytesPerSec { get; init; }
    public long DiskWriteBytesPerSec { get; init; }
    public DateTimeOffset Timestamp { get; init; }
}
```

**File 4: ICloudProvider.cs**

```csharp
[SdkCompatibility("3.0.0", Notes = "Phase 37: Cloud provider abstraction (ENV-04)")]
public interface ICloudProvider : IDisposable
{
    /// <summary>Cloud provider name (AWS, Azure, GCP)</summary>
    string Name { get; }

    /// <summary>Provision a VM instance</summary>
    /// <returns>Instance ID of the provisioned VM</returns>
    Task<string> ProvisionVmAsync(VmSpec spec, CancellationToken ct = default);

    /// <summary>Provision storage (block volume or object bucket)</summary>
    /// <returns>Resource ID of the provisioned storage</returns>
    Task<string> ProvisionStorageAsync(StorageSpec spec, CancellationToken ct = default);

    /// <summary>Deprovision a resource by ID</summary>
    /// <returns>True if deprovisioning succeeded, false if resource not found</returns>
    Task<bool> DeprovisionAsync(string resourceId, CancellationToken ct = default);

    /// <summary>Get current resource metrics (CPU, memory, storage utilization)</summary>
    Task<CloudResourceMetrics?> GetMetricsAsync(string resourceId, CancellationToken ct = default);

    /// <summary>List all DataWarehouse-managed resources (tagged appropriately)</summary>
    Task<IReadOnlyList<string>> ListManagedResourcesAsync(CancellationToken ct = default);
}
```

Add comprehensive XML documentation on interface explaining:
- Purpose: Abstract cloud provider differences for multi-cloud deployment
- ProvisionVmAsync: Launches VM, waits for running state, returns instance ID
- ProvisionStorageAsync: Creates block volume or object bucket, returns resource ID
- DeprovisionAsync: Terminates/deletes resource, idempotent (returns false if not found)
- GetMetricsAsync: Fetches current metrics for auto-scaling decisions
  </action>
  <verify>
Run `dotnet build DataWarehouse.SDK/DataWarehouse.SDK.csproj` -- zero new errors. Verify all 4 files exist in `DataWarehouse.SDK/Deployment/CloudProviders/`. Grep for `VmSpec` record. Grep for `StorageSpec` record. Grep for `CloudResourceMetrics` record. Grep for `ICloudProvider` interface with ProvisionVmAsync, ProvisionStorageAsync, DeprovisionAsync, GetMetricsAsync.
  </verify>
  <done>
VmSpec, StorageSpec, CloudResourceMetrics records defined. ICloudProvider interface abstracts cloud provisioning with ProvisionVmAsync, ProvisionStorageAsync, DeprovisionAsync, GetMetricsAsync, ListManagedResourcesAsync methods.
  </done>
</task>

<task type="auto">
  <name>Task 3: Cloud Provider Implementations (AWS, Azure, GCP)</name>
  <files>
    DataWarehouse.SDK/Deployment/CloudProviders/AwsProvider.cs
    DataWarehouse.SDK/Deployment/CloudProviders/AzureProvider.cs
    DataWarehouse.SDK/Deployment/CloudProviders/GcpProvider.cs
  </files>
  <action>
**IMPORTANT NOTE:** These implementations use cloud provider SDKs (AWS SDK for .NET, Azure SDK, Google Cloud Client Libraries). The SDKs are NOT added as direct dependencies -- they are loaded dynamically via PluginAssemblyLoadContext in Task 4.

**File 1: AwsProvider.cs**

Create in namespace `DataWarehouse.SDK.Deployment.CloudProviders`.

Sealed class implementing `ICloudProvider`. Mark with `[SdkCompatibility("3.0.0", Notes = "Phase 37: AWS provisioning provider (ENV-04)")]`.

**Constructor:** Accept AWS SDK clients via DI or create internally:
```csharp
private readonly IAmazonEC2 _ec2;
private readonly IAmazonS3 _s3;
```

**ProvisionVmAsync implementation:**
1. Map `VmSpec.InstanceType` to AWS instance type string (e.g., "general-purpose" -> "t3.medium")
2. Create `RunInstancesRequest`:
   - `ImageId = spec.ImageId ?? "ami-latest-datawarehouse"` (default AMI)
   - `InstanceType = mappedInstanceType`
   - `MinCount = 1`, `MaxCount = 1`
   - `BlockDeviceMappings` for EBS root volume with `spec.StorageGb` size
   - `SecurityGroupIds = new[] { spec.SecurityGroup }` if provided
   - `SubnetId = spec.SubnetId` if provided
   - `TagSpecifications` with DataWarehouse tags
3. Call `await _ec2.RunInstancesAsync(request, ct)` with exponential backoff retry
4. Extract instance ID from response: `response.Reservation.Instances[0].InstanceId`
5. Wait for instance state = "running" (poll EC2 DescribeInstances every 5s, max 5 minutes)
6. Return instance ID

**ProvisionStorageAsync implementation:**
1. If `spec.StorageType == "block"`:
   - Create `CreateVolumeRequest` for EBS volume
   - `Size = spec.SizeGb`, `VolumeType = spec.PerformanceTier ?? "gp3"`
   - `AvailabilityZone = spec.AvailabilityZone`
   - `Encrypted = spec.Encrypted`, `TagSpecifications`
   - Call `await _ec2.CreateVolumeAsync(request, ct)`
   - Return volume ID
2. If `spec.StorageType == "object"`:
   - Create S3 bucket using `await _s3.PutBucketAsync(bucketName, ct)`
   - Configure encryption, lifecycle policies
   - Return bucket name

**DeprovisionAsync implementation:**
1. Determine resource type from resource ID format (i-, vol-, bucket name)
2. If EC2 instance: `await _ec2.TerminateInstancesAsync(new TerminateInstancesRequest { InstanceIds = new[] { resourceId } }, ct)`
3. If EBS volume: `await _ec2.DeleteVolumeAsync(new DeleteVolumeRequest { VolumeId = resourceId }, ct)`
4. If S3 bucket: Delete all objects, then `await _s3.DeleteBucketAsync(resourceId, ct)`
5. Return true on success, false if resource not found (404)

**GetMetricsAsync implementation:**
1. Use CloudWatch API to fetch metrics for resource
2. Query: CPUUtilization, NetworkIn, NetworkOut, DiskReadBytes, DiskWriteBytes
3. Aggregate metrics from last 5 minutes
4. Return CloudResourceMetrics with timestamp

**ListManagedResourcesAsync implementation:**
1. Use EC2 DescribeInstances with tag filter: `{ "Key": "ManagedBy", "Value": "DataWarehouse" }`
2. Collect all instance IDs, volume IDs, S3 bucket names
3. Return list of resource IDs

Wrap all AWS SDK calls in try/catch with exponential backoff for rate limiting (AmazonServiceException, RequestLimitExceededException).

Add comprehensive XML docs explaining AWS-specific behavior.

**File 2: AzureProvider.cs**

Similar structure to AwsProvider, using Azure SDK:
- `ResourceManagerClient` for VM provisioning
- `BlobServiceClient` for Azure Blob storage
- `ComputeManagementClient` for VM metrics
- Map VmSpec to Azure VM create parameters
- Use Azure Monitor API for metrics

**File 3: GcpProvider.cs**

Similar structure to AwsProvider, using Google Cloud Client Libraries:
- `InstancesClient` for Compute Engine VMs
- `DisksClient` for Persistent Disks
- `StorageClient` for Cloud Storage buckets
- Map VmSpec to GCP instance insert request
- Use GCP Monitoring API for metrics

All three providers follow the same pattern with provider-specific SDK calls.
  </action>
  <verify>
Run `dotnet build DataWarehouse.SDK/DataWarehouse.SDK.csproj` -- compilation may fail if cloud SDKs not referenced (expected -- they are loaded dynamically). Verify all 3 files exist. Grep for `ICloudProvider` implementation in all 3 files. Grep for `ProvisionVmAsync` in AwsProvider. Grep for `IAmazonEC2` in AwsProvider. Grep for Azure SDK types in AzureProvider. Grep for GCP types in GcpProvider.
  </verify>
  <done>
AwsProvider implements ICloudProvider using AWS SDK for .NET (EC2, S3, CloudWatch). AzureProvider implements using Azure SDK (ResourceManager, Compute, Blob). GcpProvider implements using Google Cloud Client Libraries (Compute, Storage, Monitoring). All providers handle provisioning, deprovisioning, metrics, with exponential backoff retry.
  </done>
</task>

<task type="auto">
  <name>Task 4: Cloud Provider Factory with Dynamic SDK Loading</name>
  <files>
    DataWarehouse.SDK/Deployment/CloudProviders/CloudProviderFactory.cs
  </files>
  <action>
Create in namespace `DataWarehouse.SDK.Deployment.CloudProviders`.

Public static class. Mark with `[SdkCompatibility("3.0.0", Notes = "Phase 37: Cloud provider factory with dynamic loading (ENV-04)")]`.

**Public method:**

```csharp
public static ICloudProvider? TryCreate(string cloudProvider)
```

Implementation:

1. **Validate cloud provider name:**
   - Normalize to uppercase: `cloudProvider = cloudProvider.ToUpperInvariant()`
   - Valid values: "AWS", "AZURE", "GCP"
   - If invalid, return null

2. **Dynamic assembly loading pattern (reuse PluginAssemblyLoadContext from Phase 32):**

   ```csharp
   switch (cloudProvider)
   {
       case "AWS":
           return LoadAwsProvider();
       case "AZURE":
           return LoadAzureProvider();
       case "GCP":
           return LoadGcpProvider();
       default:
           return null;
   }
   ```

**LoadAwsProvider implementation:**

```csharp
private static AwsProvider? LoadAwsProvider()
{
    try
    {
        // Locate AWS SDK assemblies in deployment directory or well-known path
        var awsSdkPath = GetCloudSdkPath("aws"); // e.g., {AppDir}/cloud-sdks/aws/

        // Create isolated assembly context for AWS SDK
        var context = new PluginAssemblyLoadContext("aws-sdk", awsSdkPath);

        // Load AWS SDK assemblies
        var ec2Assembly = context.LoadFromAssemblyName(new AssemblyName("AWSSDK.EC2"));
        var s3Assembly = context.LoadFromAssemblyName(new AssemblyName("AWSSDK.S3"));
        var cloudWatchAssembly = context.LoadFromAssemblyName(new AssemblyName("AWSSDK.CloudWatch"));

        // Create AWS clients (via reflection or factory)
        var ec2Client = CreateAwsEc2Client(ec2Assembly);
        var s3Client = CreateAwsS3Client(s3Assembly);

        // Instantiate AwsProvider with clients
        return new AwsProvider(ec2Client, s3Client);
    }
    catch (Exception ex)
    {
        _logger.LogWarning(ex, "Failed to load AWS SDK. AWS provisioning unavailable.");
        return null; // Graceful degradation
    }
}
```

**LoadAzureProvider and LoadGcpProvider:** Similar pattern for Azure SDK and GCP Client Libraries.

**GetCloudSdkPath helper:**
- Check environment variable `DATAWAREHOUSE_CLOUD_SDK_PATH`
- Check well-known paths: `{AppDir}/cloud-sdks/{provider}/`, `/opt/datawarehouse/cloud-sdks/{provider}/`
- Return null if not found (graceful degradation)

Wrap all assembly loading in try/catch with graceful degradation -- cloud provisioning is optional.

Add comprehensive XML documentation explaining:
- Purpose: Load cloud SDKs dynamically to avoid bloating base installation
- Isolation: Each cloud provider SDK loaded in separate PluginAssemblyLoadContext
- Graceful degradation: Returns null if SDK not available (cloud provisioning disabled)
- SDK location: Checks environment variable, well-known paths
  </action>
  <verify>
Run `dotnet build DataWarehouse.SDK/DataWarehouse.SDK.csproj` -- zero new errors. Verify file exists at `DataWarehouse.SDK/Deployment/CloudProviders/CloudProviderFactory.cs`. Grep for `TryCreate` method. Grep for `PluginAssemblyLoadContext` usage. Grep for dynamic assembly loading (LoadAwsProvider, LoadAzureProvider, LoadGcpProvider). Grep for graceful degradation on assembly load failure.
  </verify>
  <done>
CloudProviderFactory.TryCreate loads cloud provider SDKs dynamically via PluginAssemblyLoadContext, creates provider-specific assembly contexts (aws-sdk, azure-sdk, gcp-sdk), instantiates ICloudProvider implementations, returns null on SDK load failure for graceful degradation.
  </done>
</task>

<task type="auto">
  <name>Task 5: Hyperscale Provisioning Orchestrator with Auto-Scaling</name>
  <files>
    DataWarehouse.SDK/Deployment/HyperscaleProvisioner.cs
  </files>
  <action>
Create in namespace `DataWarehouse.SDK.Deployment`.

Public sealed class implementing `IDisposable`. Mark with `[SdkCompatibility("3.0.0", Notes = "Phase 37: Hyperscale cloud provisioning orchestrator (ENV-04)")]`.

**Public record types:**

```csharp
public sealed record AutoScalingPolicy
{
    public double ScaleUpThresholdPercent { get; init; } = 80.0;   // Add node when storage >80%
    public double ScaleDownThresholdPercent { get; init; } = 40.0; // Remove node when storage <40%
    public int MinNodes { get; init; } = 1;
    public int MaxNodes { get; init; } = 100;
    public TimeSpan EvaluationInterval { get; init; } = TimeSpan.FromMinutes(5);
}
```

Constructor: accept `ICloudProvider` (from factory), `AutoScalingPolicy`, configuration.

**Public methods:**

```csharp
public async Task StartAutoScalingAsync(CancellationToken ct = default)
public async Task StopAutoScalingAsync(CancellationToken ct = default)
public Task<int> GetCurrentNodeCountAsync(CancellationToken ct = default)
```

**StartAutoScalingAsync implementation:**

1. **Start evaluation loop:**
   ```csharp
   while (!ct.IsCancellationRequested)
   {
       await EvaluateScalingAsync(ct);
       await Task.Delay(_policy.EvaluationInterval, ct);
   }
   ```

2. **EvaluateScalingAsync logic:**

   a. **Fetch current cluster metrics:**
      - Get all managed DataWarehouse nodes: `await _cloudProvider.ListManagedResourcesAsync()`
      - For each node, fetch metrics: `await _cloudProvider.GetMetricsAsync(nodeId)`
      - Aggregate storage utilization across all nodes

   b. **Check scale-up condition:**
      - If `aggregateStorageUtilization > _policy.ScaleUpThresholdPercent`:
        - If `currentNodeCount < _policy.MaxNodes`:
          - Log: "Storage utilization {util}% exceeds threshold {threshold}%. Provisioning new node."
          - Create VmSpec with default instance type, storage size
          - Call `await _cloudProvider.ProvisionVmAsync(vmSpec, ct)`
          - Wait for new node to join cluster (poll cluster API for new node, max 10 minutes)
          - Log: "New node {instanceId} provisioned and joined cluster."
        - Else:
          - Log warning: "Max node count {maxNodes} reached. Cannot scale up."

   c. **Check scale-down condition:**
      - If `aggregateStorageUtilization < _policy.ScaleDownThresholdPercent`:
        - If `currentNodeCount > _policy.MinNodes`:
          - Select least-loaded node (lowest storage utilization)
          - Drain node: move data to other nodes (use Phase 34 federated rebalancing)
          - Call `await _cloudProvider.DeprovisionAsync(nodeId, ct)`
          - Log: "Node {instanceId} deprovisioned. Storage utilization below threshold."
        - Else:
          - Log: "Min node count {minNodes} reached. Cannot scale down."

**StopAutoScalingAsync implementation:**
- Set cancellation token to stop evaluation loop
- Wait for current evaluation to complete
- Log: "Auto-scaling stopped."

**GetCurrentNodeCountAsync implementation:**
- Call `await _cloudProvider.ListManagedResourcesAsync()`
- Return count of managed nodes

**Exponential backoff for cloud API calls:**
```csharp
private async Task<T> CallWithRetryAsync<T>(Func<Task<T>> operation, CancellationToken ct)
{
    int retryCount = 0;
    while (true)
    {
        try
        {
            return await operation();
        }
        catch (Exception ex) when (IsRetryable(ex) && retryCount < 5)
        {
            var delay = TimeSpan.FromSeconds(Math.Pow(2, retryCount));
            _logger.LogWarning(ex, "Cloud API call failed. Retrying in {delay}s...", delay.TotalSeconds);
            await Task.Delay(delay, ct);
            retryCount++;
        }
    }
}

private static bool IsRetryable(Exception ex)
{
    // AWS: RequestLimitExceededException, ThrottlingException
    // Azure: Azure.RequestFailedException with status 429
    // GCP: Google.GoogleApiException with status 429
    return ex.Message.Contains("Throttl") || ex.Message.Contains("RateLimit") || ex.Message.Contains("429");
}
```

Wrap all cloud provider calls with retry logic.

Add comprehensive XML documentation explaining:
- Auto-scaling logic (scale-up at >80%, scale-down at <40%)
- Node draining procedure before deprovisioning
- Exponential backoff for rate limiting
- Min/max node constraints
  </action>
  <verify>
Run `dotnet build DataWarehouse.SDK/DataWarehouse.SDK.csproj` -- zero new errors. Verify file exists at `DataWarehouse.SDK/Deployment/HyperscaleProvisioner.cs`. Grep for `StartAutoScalingAsync` method. Grep for scale-up threshold check (>80%). Grep for scale-down threshold check (<40%). Grep for exponential backoff retry logic. Grep for `ICloudProvider` usage.
  </verify>
  <done>
HyperscaleProvisioner.StartAutoScalingAsync runs evaluation loop every 5 minutes, checks storage utilization, provisions new nodes when >80% (respects max nodes), deprovisions least-loaded nodes when <40% (respects min nodes), uses exponential backoff for cloud API retries, delegates VM/storage provisioning to ICloudProvider.
  </done>
</task>

</tasks>

<verification>
1. `dotnet build DataWarehouse.SDK/DataWarehouse.SDK.csproj` compiles with zero new errors
2. All 10 new files exist in `DataWarehouse.SDK/Deployment/` and `.../CloudProviders/`
3. CloudDetector tries AWS/Azure/GCP metadata endpoints with 500ms timeout
4. ICloudProvider interface abstracts ProvisionVmAsync, ProvisionStorageAsync, DeprovisionAsync, GetMetricsAsync
5. AwsProvider uses AWS SDK (EC2, S3, CloudWatch) for provisioning
6. AzureProvider uses Azure SDK (ResourceManager, Blob) for provisioning
7. GcpProvider uses Google Cloud Client Libraries (Compute, Storage) for provisioning
8. CloudProviderFactory loads cloud SDKs dynamically via PluginAssemblyLoadContext
9. HyperscaleProvisioner orchestrates auto-scaling with >80% scale-up, <40% scale-down
10. All cloud API calls use exponential backoff for rate limiting
</verification>

<success_criteria>
- CloudDetector identifies AWS/Azure/GCP via metadata endpoints with <1s timeout on non-cloud
- Cloud provider SDKs loaded dynamically (zero bloat for users not using cloud deployment)
- Auto-scaling provisions new node when storage >80%, max 100 nodes
- Auto-scaling deprovisions least-loaded node when storage <40%, min 1 node
- All cloud API calls retry with exponential backoff on rate limit errors
- Cloud provisioning works across AWS EC2/EBS/S3, Azure VM/Disk/Blob, GCP Compute/PD/Storage
- Zero cloud SDK dependencies in base DataWarehouse installation
- Zero existing files modified -- purely additive
</success_criteria>

<output>
After completion, create `.planning/phases/37-multi-environment-deployment/37-04-SUMMARY.md`
</output>
