# DataWarehouse v4.5 Performance Benchmark Analysis
**Phase 46 - Static Hot-Path Performance Analysis**
**Date:** 2026-02-19
**Iteration:** v4.5 (following v4.3 baseline, v4.4 MemoryStream/ArrayPool fixes)

---

## Executive Summary

This v4.5 benchmark measures performance anti-patterns across all hot paths after multiple rounds of fixes. The analysis covers data pipeline, storage/IO, distributed systems, network/transport, and memory management.

### Headline Improvements Since v4.3

| Metric | v4.3 | v4.5 | Delta |
|--------|------|------|-------|
| `GetAwaiter().GetResult()` | 34 | 0 (7 comments only) | **-100%** |
| `new MemoryStream()` no capacity | 319 | 0 | **-100%** |
| ArrayPool adoption (files) | 20 | 40 | **+100%** |
| `new HttpClient()` problematic | 4 | 2 (code-gen templates) | **-50%** |
| `Thread.Sleep` in async | 5 | 5 | No change (all justified) |
| `new byte[]` total | 2,287 | 2,346 | +2.6% (new features) |
| Lock statements | 1,629 | 1,669 | +2.5% (new features) |

**Overall Grade: A-** (up from B+)

---

## 1. Data Pipeline Performance

### 1.1 Compression (Brotli Bottleneck)

**Status: PARTIALLY FIXED**

The transit-mode Brotli strategy properly maps quality levels:
- `CompressionLevel.Optimal` maps to quality 5 (transit), quality 3 (fastest)
- This is appropriate for in-flight compression

**Remaining Issue -- Transform-mode BrotliStrategy still hardcodes Q11:**
```
BrotliStrategy.cs:56: new BrotliEncoder(quality: 11, window: 22);
```
- Brotli Q11 is 14x slower than Q6 for <5% better ratio
- Only appropriate for archival/cold storage, NOT hot-path transform
- `OnStorageBrotliStrategy` has `MaxQuality = 11` but gates it: `quality <= 8 ? Optimal : SmallestSize`

**Impact:** HIGH for transform pipeline. Q11 on every CompressCore call is a CPU bottleneck.
**Recommendation:** Make quality configurable via strategy options (default Q6 for transform, Q11 opt-in for archival).

### 1.2 Encryption Envelope

**Status: GOOD**

`EncryptionPluginBase.cs` uses ArrayPool for header buffer:
```csharp
var headerBuffer = ArrayPool<byte>.Shared.Rent(4096);
// ... properly returned in finally block
ArrayPool<byte>.Shared.Return(headerBuffer, clearArray: true);
```

### 1.3 Memory Allocation Patterns

**Status: EXCELLENT -- MemoryStream fully remediated**

- `new MemoryStream()` (no capacity): **0 occurrences** (was 319 in v4.3)
- All 1,009 MemoryStream constructions now include capacity hints
- This eliminates ~319 sources of repeated 2x array growth + GC pressure

---

## 2. Storage/IO Performance

### 2.1 VDE Write Lock

**Status: UNCHANGED -- Still serialized via SemaphoreSlim(1,1)**

```csharp
VirtualDiskEngine.cs:30: private readonly SemaphoreSlim _writeLock = new(1, 1);
```

The entire `StoreAsync` method (lines 206-316) is under `_writeLock`:
- WAL transaction begin
- Inode resolution
- Block-by-block read + allocate + CoW write + checksum
- Inode update + metadata + B-Tree index + WAL commit
- Auto-checkpoint if WAL utilization high

**Impact:** MEDIUM-HIGH. All writes are fully serialized. A 10MB file with 4KB blocks = 2,560 block operations under lock.

**Positive:** `ReadAsync` (line 330+) does NOT take the write lock, so reads are concurrent. The read path is well-designed.

**Positive:** FileBlockDevice also uses SemaphoreSlim(1,1) but only for the actual I/O operations, which is correct for file-based block devices.

**Recommendation:** Consider splitting into a WAL transaction lock (serialized) and per-block allocation (can use allocator's own ReaderWriterLockSlim). Currently the allocator has its own lock but it's called inside the write lock, creating redundant serialization.

### 2.2 Bitmap Allocator

**Status: GOOD**

- Uses `ReaderWriterLockSlim` (correct for read-heavy workloads with FreeBlockCount queries)
- Next-free hint for O(1) amortized allocation
- Extent allocation with contiguous scan
- ArrayPool for persistence buffer
- No SIMD acceleration for bit scanning (uses byte-by-byte scan)

**Missing:** No fragmentation metrics or defragmentation. After many allocate/free cycles, `FindNextFreeBlock` will degrade from O(1) amortized to O(n) scan if hint region is heavily fragmented. No `BitOperations.TrailingZeroCount` or `PopCount` for hardware-accelerated scanning.

### 2.3 RAID Parity

**Status: NOT APPLICABLE (at VDE level)**

RAID/parity is handled at the storage strategy layer (Sia, LizardFS, BeeGFS, GlusterFS) via external systems. No in-process XOR parity computation found -- strategies delegate to their respective distributed storage backends. No SIMD optimization needed since parity is computed by external services.

---

## 3. Distributed System Performance

### 3.1 ORSet Tag Growth

**Status: UNBOUNDED -- Both implementations lack tag pruning**

**SdkORSet** (`SdkCrdtTypes.cs`):
- `_addSet`: `ConcurrentDictionary<string, HashSet<string>>` -- tags grow unbounded
- `_removeSet`: `ConcurrentDictionary<string, HashSet<string>>` -- tombstones accumulate forever
- Each `Add()` generates a new GUID tag: `$"{nodeId}:{Guid.NewGuid():N}"`
- `Remove()` copies tags to removeSet but never cleans addSet
- `Merge()` unions both sets, compounding growth across replicas
- **No GC/pruning/compaction of observed tags**

**ORSetCrdt<T>** (`CoreReplicationStrategies.cs`):
- Same pattern: `_elements` tags grow unbounded, `_removed` dictionary grows forever
- `Remove()` clears tags from element but `_removed` retains all tags for merge correctness
- No tag epoch or generation counter

**Impact:** HIGH for long-lived ORSets with frequent add/remove cycles. Memory grows linearly with operation count, not element count. A set with 100 elements that's been modified 100K times retains ~100K tags.

**Recommendation:** Implement tag GC with causal stability detection (when all replicas have observed a tag, it can be pruned from the remove set). Alternative: epoch-based tag compaction.

### 3.2 Raft Heartbeat Under stateLock

**Status: IMPROVED -- Lock held only for state reads**

`SendAppendEntriesToPeerAsync` (line 411):
- Acquires `_stateLock` only to read `CurrentTerm`, `CommitIndex`, and copy log entries
- Releases lock before the actual network I/O (`SendHeartbeatsAsync` uses `Task.WhenAll`)
- Batch size capped at 50 entries per heartbeat

**Remaining concern:** `_stateLock.WaitAsync(ct)` in `SendAppendEntriesToPeerAsync` is called per-peer sequentially within `SendHeartbeatsAsync`. However, the outer loop calls `Task.WhenAll(tasks)` which means all peers acquire the lock concurrently. With SemaphoreSlim(1,1), peers serialize on lock acquisition. For a 5-node cluster, this means 4 sequential lock acquisitions per heartbeat.

**Impact:** LOW for typical cluster sizes (3-7 nodes). MEDIUM for large clusters (50+ nodes).

### 3.3 GetMembers Allocation at Heartbeat Frequency

**Status: ALLOCATION ON EVERY CALL**

```csharp
SwimClusterMembership.cs:88-94:
public IReadOnlyList<ClusterNode> GetMembers()
{
    return _members.Values
        .Where(m => m.Status != ClusterNodeStatus.Dead)
        .Select(m => m.Node)
        .ToList()
        .AsReadOnly();
}
```

Called from:
- `RaftConsensusEngine.SendHeartbeatsAsync` (every 50ms by default)
- `RaftConsensusEngine.AdvanceCommitIndexAsync` (every heartbeat)
- `RaftConsensusEngine.RequestVoteAsync`
- `GossipReplicator` (multiple gossip rounds)

**Impact:** MEDIUM. At 50ms heartbeat interval = 20 calls/sec = 20 List<ClusterNode> + ReadOnlyCollection allocations per second per leader. Plus LINQ iterator allocations.

**Recommendation:** Cache member list with invalidation on membership change (already has `OnMembershipChanged` event). Return cached `IReadOnlyList<ClusterNode>` without allocation on hot path.

---

## 4. Network/Transport Performance

### 4.1 Connection Reuse

**Status: IMPROVED -- Connection pools exist for all protocols**

AdaptiveTransport now has `ConnectionPool` per protocol:
- TCP pool: `_connectionPools["Tcp"]` (but WarmupAsync is a no-op -- "created on-demand")
- QUIC pool: `_connectionPools["Quic"]` (conditional on QUIC support)
- UDP pool: `_connectionPools["ReliableUdp"]` (pre-creates UdpClient instances)

TcpP2PNetwork has connection caching:
```csharp
_activeConnections[peer.PeerId] = client;  // Reuses existing connections
```

**Remaining concern:** ConnectionPool uses `ConcurrentBag<object>` with no max size, no health checking, no idle timeout, no connection validation before return. This is a basic pool -- it will grow unbounded and return stale/broken connections.

### 4.2 TLS Configuration

**Status: GOOD**

TcpP2PNetwork properly configures mTLS:
- TLS 1.2 + TLS 1.3 enabled
- Client certificate required for mTLS
- CRL checking enabled (`X509RevocationMode.Online`)
- Mutual authentication verified post-handshake

### 4.3 Congestion Control for UDP

**Status: BASIC**

- Congestion algorithm is hardcoded to "cubic" (line 191)
- No actual congestion window implementation found
- Reliable UDP uses fixed AckTimeout (500ms) + fixed MaxRetries (3)
- No adaptive RTT estimation, no slow start, no congestion avoidance
- BandwidthProbe estimates from TCP window/RTT but does not feed back into UDP transmission rate

**Impact:** MEDIUM for lossy networks. UDP will either over-send (causing drops) or under-utilize bandwidth.

---

## 5. Memory Management

### 5.1 MemoryStream Fixes (v4.4)

**Status: FULLY HELD**

- Zero instances of `new MemoryStream()` without capacity
- All 1,009 constructions include appropriate capacity hints
- This is a complete remediation of the v4.3 finding

### 5.2 ArrayPool Adoption

**Status: DOUBLED**

40 files now use ArrayPool (was 20 in v4.3). New adoptions include:
- `MessageBridge.cs` (shared infrastructure)
- `AdaptiveTransportPlugin.cs` (network buffers)
- `TcpP2PNetwork.cs` (P2P messaging)
- `WebSocketControlPlanePlugin.cs` (control plane)
- Multiple compression strategies (Gipfeli, WebpLossless, Xdelta, Vcdiff, Zdelta)
- Legacy connector strategies (COBOL, VSAM, IMS, Db2, CICS, DICOM)
- TamperProof pipeline (Read/Write phase handlers, RecoveryService)
- Image transcoding (JPEG, PNG)

**Coverage:** ArrayPool usage in ~40/2346 byte[] allocation sites = ~1.7% (was ~0.9%). Still low overall but concentrated in hot paths.

### 5.3 Sync-Over-Async

**Status: FULLY REMEDIATED**

- `GetAwaiter().GetResult()`: **0 actual calls** (was 34 in v4.3)
- 7 remaining occurrences are comments only ("safer than GetAwaiter().GetResult()")
- 4 occurrences in auto-generated xUnit test entry points (not user code)
- Phase 43 scan confirmed: 0 GetAwaiter calls

### 5.4 GC Pressure Points

**Remaining sources of GC pressure:**
1. **ORSet tag accumulation** -- unbounded HashSet<string> growth (see 3.1)
2. **GetMembers() per-call allocation** -- List + ReadOnlyCollection every 50ms (see 3.3)
3. **BrotliStrategy Q11** -- large temporary buffers during slow compression (see 1.1)
4. **HttpClient** -- 31 static instances (correct pattern), 2 in code-gen templates (non-runtime)

---

## Comparison Matrix: v4.3 vs v4.5

| Category | v4.3 Count | v4.5 Count | Status | Trend |
|----------|-----------|-----------|--------|-------|
| Sync-over-async (GetAwaiter) | 34 | 0 | RESOLVED | -- |
| MemoryStream no capacity | 319 | 0 | RESOLVED | -- |
| HttpClient creation (problematic) | 4 | 2 (code-gen) | RESOLVED | -- |
| ArrayPool files | 20 | 40 | IMPROVED | ++ |
| Thread.Sleep (justified) | 5 | 5 | UNCHANGED | = |
| Lock statements | 1,629 | 1,669 | UNCHANGED | = |
| ToList() | 3,153 | 3,163 | UNCHANGED | = |
| ToArray() | 1,969 | 1,995 | UNCHANGED | = |
| StringBuilder | 354 | 354 | CLEAN | = |
| new byte[] | 2,287 | 2,346 | UNCHANGED | = |
| String concat in loops | 0 | 0 | CLEAN | = |

---

## Remaining Bottlenecks (Priority Order)

### P0 -- Critical Hot-Path Issues

| # | Issue | Location | Impact |
|---|-------|----------|--------|
| 1 | **Brotli Q11 hardcoded in transform** | `BrotliStrategy.cs:56` | CPU bottleneck on every transform compress |
| 2 | **ORSet unbounded tag growth** | `SdkCrdtTypes.cs`, `CoreReplicationStrategies.cs` | Memory leak proportional to operation count |

### P1 -- Medium Impact

| # | Issue | Location | Impact |
|---|-------|----------|--------|
| 3 | **VDE write lock serialization** | `VirtualDiskEngine.cs:206-316` | All writes serialized, reads unaffected |
| 4 | **GetMembers() allocation per call** | `SwimClusterMembership.cs:88-94` | 20 allocs/sec at heartbeat frequency |
| 5 | **ConnectionPool no health check** | `AdaptiveTransportPlugin.cs:2068` | Stale connections returned, unbounded growth |
| 6 | **No UDP congestion control** | `AdaptiveTransportPlugin.cs` | Over/under-utilization on lossy networks |

### P2 -- Low Impact / Future

| # | Issue | Location | Impact |
|---|-------|----------|--------|
| 7 | **BitmapAllocator no SIMD scanning** | `BitmapAllocator.cs` | O(n) scan when hint region fragmented |
| 8 | **Raft heartbeat serialized lock** | `RaftConsensusEngine.cs:411` | Only affects 50+ node clusters |
| 9 | **ArrayPool coverage 1.7%** | Codebase-wide | Most allocations are small/infrequent |

---

## What Improved Since v4.3/v4.4

1. **MemoryStream capacity hints** -- 319 -> 0 (complete fix)
2. **Sync-over-async** -- 34 -> 0 (complete fix, including FuseFileSystem hot paths)
3. **ArrayPool adoption** -- 20 -> 40 files (doubled, focused on hot paths)
4. **Connection pooling** -- TCP/QUIC/UDP pools now exist in AdaptiveTransport
5. **TcpP2PNetwork connection reuse** -- Caches active connections per peer
6. **Brotli transit quality** -- Transit mode uses Q5/Q3 (appropriate for in-flight)
7. **Encryption envelope** -- ArrayPool for header buffer with secure clear

## What Remains Unchanged

1. **Brotli Q11** in transform mode (not transit)
2. **ORSet tag growth** -- no pruning mechanism
3. **VDE write serialization** -- architectural choice, needs design review
4. **Lock count** -- stable (~1,669), no new contention points
5. **LINQ materialization** -- stable (~5,158 total), mostly justified

---

**Scan Completed:** 2026-02-19
**Previous Scan:** v4.3 (2026-02-18)
**Overall Grade: A-** (up from B+ in v4.3)
**Next Actions:** Address P0 items (Brotli Q11 transform, ORSet tag GC)
