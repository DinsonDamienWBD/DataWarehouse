---
phase: 21-data-transit
plan: 03
type: execute
wave: 2
depends_on: ["21-01"]
files_modified:
  - Plugins/DataWarehouse.Plugins.UltimateDataTransit/Strategies/Distributed/P2PSwarmStrategy.cs
  - Plugins/DataWarehouse.Plugins.UltimateDataTransit/Strategies/Distributed/MultiPathParallelStrategy.cs
autonomous: true

must_haves:
  truths:
    - "P2PSwarmStrategy splits data into pieces and distributes across peer swarm using BitTorrent-style piece selection"
    - "Peers exchange piece availability bitmaps and download pieces from multiple peers concurrently"
    - "Bounded channels provide backpressure preventing fast peers from overwhelming slow peers"
    - "MultiPathParallelStrategy splits data across multiple network paths for aggregate bandwidth"
    - "Path scoring ranks paths by latency, throughput, and error rate for optimal distribution"
    - "Both strategies report accurate progress accounting for multi-source/multi-path nature"
  artifacts:
    - path: "Plugins/DataWarehouse.Plugins.UltimateDataTransit/Strategies/Distributed/P2PSwarmStrategy.cs"
      provides: "BitTorrent-style peer swarm distribution with piece selection and flow control"
      min_lines: 250
    - path: "Plugins/DataWarehouse.Plugins.UltimateDataTransit/Strategies/Distributed/MultiPathParallelStrategy.cs"
      provides: "Parallel multi-path transfer with path scoring and dynamic rebalancing"
      min_lines: 200
  key_links:
    - from: "P2PSwarmStrategy piece download"
      to: "System.Threading.Channels"
      via: "bounded channel for piece queue with backpressure"
      pattern: "Channel\\.CreateBounded|BoundedChannelOptions"
    - from: "MultiPathParallelStrategy.TransferAsync"
      to: "path scoring"
      via: "scores paths by latency/throughput/error rate, distributes data proportionally"
      pattern: "ScorePath|PathScore|latency.*throughput"
---

<objective>
Implement P2P swarm and multi-path parallel transfer strategies for distributed and high-throughput scenarios.

Purpose: Enable distributed data movement across peer networks and parallel utilization of multiple network paths
Output: 2 distributed strategies (P2PSwarm, MultiPathParallel) compiled and registered in the plugin
</objective>

<execution_context>
@C:/Users/ddamien/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/ddamien/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/21-data-transit/21-RESEARCH.md
@.planning/phases/21-data-transit/21-01-SUMMARY.md

# SDK contracts from Plan 01
@DataWarehouse.SDK/Contracts/Transit/IDataTransitStrategy.cs
@DataWarehouse.SDK/Contracts/Transit/DataTransitTypes.cs
@DataWarehouse.SDK/Contracts/Transit/DataTransitStrategyBase.cs
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement P2PSwarmStrategy with piece-based distribution and flow control</name>
  <files>
    Plugins/DataWarehouse.Plugins.UltimateDataTransit/Strategies/Distributed/P2PSwarmStrategy.cs
  </files>
  <action>
Create `internal sealed class P2PSwarmStrategy : DataTransitStrategyBase` in namespace `DataWarehouse.Plugins.UltimateDataTransit.Strategies.Distributed`.

**Strategy identity:**
- StrategyId: "transit-p2p-swarm"
- Name: "P2P Swarm Transfer"
- Capabilities: SupportsP2P=true, SupportsStreaming=true, SupportsResumable=true, MaxTransferSizeBytes=long.MaxValue, SupportedProtocols=["p2p", "swarm", "http", "https"]

**Internal types:**

`internal sealed record SwarmPeer(string PeerId, string EndpointUri, bool[] PieceBitmap, int ActiveDownloads, long BytesDownloaded, DateTime LastSeen)` - mutable through ConcurrentDictionary replacement

`internal sealed record PieceInfo(int Index, long Offset, int Size, byte[] Sha256Hash, bool Downloaded)`

`internal sealed class SwarmState`:
- `string TransferId`
- `long TotalSize`
- `int PieceSize` (default 256KB = 256 * 1024)
- `PieceInfo[] Pieces`
- `ConcurrentDictionary<string, SwarmPeer> Peers`
- `Channel<int> PieceQueue` - bounded channel for pending piece indices
- `int ConcurrentDownloadLimit` = 4 per peer

**TransferAsync implementation:**
1. Generate transfer ID
2. Split data into pieces: compute piece count = ceil(totalSize / pieceSize), create PieceInfo array with SHA-256 hash per piece computed from source data
3. **Peer discovery:** POST tracker request to `{request.Destination.Uri}/swarm/announce` with transfer ID, piece count, total size. Response returns list of peers with their piece bitmaps.
4. **Initialize bounded channel:** `Channel.CreateBounded<int>(new BoundedChannelOptions(capacity: 64) { FullMode = BoundedChannelFullMode.Wait })` -- this prevents fast peers from queuing unbounded work (research pitfall 3)
5. Enqueue all needed piece indices into the channel
6. **Download loop** using `Task.WhenAll` with `maxConcurrency` semaphore:
   - Read piece index from channel
   - Select peer: choose peer that HAS the piece (bitmap check) AND has fewest active downloads (load balancing). Use "rarest first" selection: prefer pieces available from fewer peers.
   - Download piece from selected peer via HTTP GET `{peer.EndpointUri}/swarm/piece/{transferId}/{pieceIndex}`
   - Verify SHA-256 hash of downloaded piece against expected hash
   - If hash mismatch: ban peer (mark as untrusted), re-queue piece index
   - If hash matches: mark piece as Downloaded, update peer stats
   - Report progress: completedPieces / totalPieces * 100
7. **Seeding:** After all pieces downloaded, register self as seeder by POST to tracker
8. Return TransitResult with total bytes, duration, metadata including peer count and piece count

**ResumeTransferAsync:** Load SwarmState from `ConcurrentDictionary<string, SwarmState> _swarmStates`. Re-announce to tracker. Enqueue only non-downloaded pieces. Resume download loop.

**Backpressure (pitfall 3 from research):** The bounded channel with capacity 64 ensures at most 64 pieces are queued. `SemaphoreSlim(maxConcurrency: 8)` limits total concurrent piece downloads across all peers.

**Thread safety:** All state in ConcurrentDictionary. Piece completion uses Interlocked.CompareExchange on a completion flag array. Channel provides built-in thread-safe producer/consumer semantics.

XML docs on all members. Use `using System.Threading.Channels;` for bounded channels.
  </action>
  <verify>
Build: `dotnet build Plugins/DataWarehouse.Plugins.UltimateDataTransit/DataWarehouse.Plugins.UltimateDataTransit.csproj`
Expect: 0 errors
Grep: `rg "TODO|NotImplementedException|simulate|placeholder|ConcurrentQueue" Plugins/DataWarehouse.Plugins.UltimateDataTransit/Strategies/Distributed/P2PSwarmStrategy.cs`
Expect: 0 matches (ConcurrentQueue is forbidden per research pitfall 3 -- must use bounded Channel)
  </verify>
  <done>
P2PSwarmStrategy compiles with: piece-based splitting, SHA-256 per-piece integrity, peer discovery via tracker, rarest-first piece selection, bounded Channel backpressure, semaphore-limited concurrency, peer banning on hash mismatch, resume from swarm state.
  </done>
</task>

<task type="auto">
  <name>Task 2: Implement MultiPathParallelStrategy with path scoring and dynamic distribution</name>
  <files>
    Plugins/DataWarehouse.Plugins.UltimateDataTransit/Strategies/Distributed/MultiPathParallelStrategy.cs
  </files>
  <action>
Create `internal sealed class MultiPathParallelStrategy : DataTransitStrategyBase` in namespace `DataWarehouse.Plugins.UltimateDataTransit.Strategies.Distributed`.

**Strategy identity:**
- StrategyId: "transit-multipath-parallel"
- Name: "Multi-Path Parallel Transfer"
- Capabilities: SupportsMultiPath=true, SupportsStreaming=true, SupportsResumable=true, MaxTransferSizeBytes=long.MaxValue, SupportedProtocols=["http", "https", "http2", "http3"]

**Internal types:**

`internal sealed class PathInfo`:
- `string PathId` (Guid-based)
- `string EndpointUri`
- `double LatencyMs` (measured via probe)
- `double ThroughputBytesPerSec` (measured during transfer)
- `double ErrorRate` (failures / attempts)
- `double Score` (computed from latency, throughput, error rate)
- `long BytesAssigned`
- `long BytesCompleted`
- `bool IsHealthy`

`internal sealed record DataSegment(int Index, long Offset, long Size, string AssignedPathId, bool Completed)`

**Path scoring algorithm:**
```
Score = (throughputWeight * normalizedThroughput) - (latencyWeight * normalizedLatency) - (errorWeight * errorRate)
```
Where throughputWeight=0.5, latencyWeight=0.3, errorWeight=0.2. Normalize throughput to 0-1 range across all paths. Normalize latency inversely (lower latency = higher score).

**TransferAsync implementation:**
1. Generate transfer ID
2. **Path discovery:** Parse multiple destination endpoints from `request.Metadata["paths"]` (comma-separated URIs) or from `request.Destination.Options["alternatePaths"]`. If only one path, this strategy is not optimal -- return result from single HTTP transfer.
3. **Path probing:** For each path, send HTTP HEAD request and measure round-trip latency. Initialize PathInfo with measured latency, default throughput (estimate from latency using bandwidth-delay product heuristic: throughput = 1MB / latencyMs * 1000).
4. **Score paths:** Compute initial scores for all healthy paths.
5. **Segment assignment:** Split total data into segments proportional to path scores. Higher-scored paths get larger segments. Minimum segment size: 1MB. Formula: `segmentSize[i] = totalSize * (score[i] / totalScore)`.
6. **Parallel transfer:** Launch one `Task` per path. Each task:
   a. Creates `HttpClient` configured for the path endpoint
   b. For each segment assigned to this path: POST segment data with Content-Range header
   c. After each segment completion, update PathInfo throughput (actual bytes / actual time)
   d. Report segment completion
7. **Dynamic rebalancing:** Every 5 seconds (or after each segment), re-score paths based on actual measured throughput. If a path's score drops below 50% of the best path, redistribute its remaining segments to better paths. This handles one path becoming congested mid-transfer.
8. Wait for all tasks to complete. Verify all segments transferred.
9. POST assembly instruction to primary destination: `{destination.Uri}/assemble/{transferId}` with segment manifest
10. Return TransitResult with total bytes, duration, metadata including paths used and per-path throughput

**ResumeTransferAsync:** Load segment assignment state. Re-probe paths. Resume only incomplete segments.

**IsAvailableAsync:** Check that at least 2 paths are reachable (otherwise multi-path adds no value).

**Thread safety:** PathInfo updates use lock per path. Segment reassignment uses `SemaphoreSlim(1, 1)` for exclusive access during rebalancing. Progress reporting uses Interlocked.Add on total completed bytes.

XML docs on all members. Use `System.Diagnostics.Stopwatch` for latency measurement.
  </action>
  <verify>
Build: `dotnet build Plugins/DataWarehouse.Plugins.UltimateDataTransit/DataWarehouse.Plugins.UltimateDataTransit.csproj`
Expect: 0 errors
Grep: `rg "TODO|NotImplementedException|simulate|placeholder" Plugins/DataWarehouse.Plugins.UltimateDataTransit/Strategies/Distributed/MultiPathParallelStrategy.cs`
Expect: 0 matches
  </verify>
  <done>
MultiPathParallelStrategy compiles with: multi-path discovery, latency-based probing, weighted score computation, proportional segment assignment, parallel per-path transfer, dynamic rebalancing on throughput changes, segment-level resume capability.
  </done>
</task>

</tasks>

<verification>
- [ ] Plugin builds: `dotnet build Plugins/DataWarehouse.Plugins.UltimateDataTransit/DataWarehouse.Plugins.UltimateDataTransit.csproj` passes with 0 errors
- [ ] P2PSwarmStrategy uses bounded Channel (NOT ConcurrentQueue) for backpressure
- [ ] P2PSwarmStrategy verifies SHA-256 per piece and bans peers on hash mismatch
- [ ] MultiPathParallelStrategy scores paths and distributes segments proportionally
- [ ] No forbidden patterns (TODO, NotImplementedException, simulate, placeholder, stub)
- [ ] Both strategies extend DataTransitStrategyBase
</verification>

<success_criteria>
Phase 21 Plan 03 succeeds when:
1. P2PSwarmStrategy implements piece-based distribution with bounded-channel backpressure and rarest-first selection
2. MultiPathParallelStrategy splits data across scored paths with dynamic rebalancing
3. Both strategies handle resume, cancellation, and progress reporting
4. Zero Rule 13 violations, zero unbounded queues
</success_criteria>

<output>
After completion, create `.planning/phases/21-data-transit/21-03-SUMMARY.md`
</output>
