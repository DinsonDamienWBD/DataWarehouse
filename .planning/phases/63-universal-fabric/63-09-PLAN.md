---
phase: 63-universal-fabric
plan: 09
type: execute
wave: 3
depends_on: ["63-04"]
files_modified:
  - Plugins/DataWarehouse.Plugins.UniversalFabric/Migration/LiveMigrationEngine.cs
  - Plugins/DataWarehouse.Plugins.UniversalFabric/Migration/MigrationJob.cs
  - Plugins/DataWarehouse.Plugins.UniversalFabric/Migration/MigrationProgress.cs
autonomous: true

must_haves:
  truths:
    - "LiveMigrationEngine moves data between backends via dw:// addresses without downtime"
    - "MigrationJob tracks progress, supports pause/resume, and handles partial failures"
    - "Migration streams data without buffering entire objects in memory"
  artifacts:
    - path: "Plugins/DataWarehouse.Plugins.UniversalFabric/Migration/LiveMigrationEngine.cs"
      provides: "Cross-backend data migration"
      contains: "class LiveMigrationEngine"
    - path: "Plugins/DataWarehouse.Plugins.UniversalFabric/Migration/MigrationJob.cs"
      provides: "Migration job state and lifecycle"
      contains: "class MigrationJob"
    - path: "Plugins/DataWarehouse.Plugins.UniversalFabric/Migration/MigrationProgress.cs"
      provides: "Progress tracking and reporting"
      contains: "record MigrationProgress"
  key_links:
    - from: "Plugins/DataWarehouse.Plugins.UniversalFabric/Migration/LiveMigrationEngine.cs"
      to: "DataWarehouse.SDK/Storage/Fabric/IStorageFabric.cs"
      via: "uses fabric for cross-backend operations"
      pattern: "IStorageFabric"
---

<objective>
Implement the Live Backend Migration engine for moving data between backends via dw:// addressing.

Purpose: Organizations need to move data between storage backends (e.g., from S3 to Azure Blob, from local to cloud, from hot to cold tier) without downtime. The migration engine handles this with streaming, progress tracking, and resume capability.

Output: LiveMigrationEngine with job management, streaming transfers, and progress reporting.
</objective>

<execution_context>
@C:/Users/ddamien/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/ddamien/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/phases/63-universal-fabric/63-04-SUMMARY.md
@DataWarehouse.SDK/Storage/Fabric/IStorageFabric.cs
@DataWarehouse.SDK/Storage/Fabric/IBackendRegistry.cs
</context>

<tasks>

<task type="auto">
  <name>Task 1: Define MigrationJob and MigrationProgress types</name>
  <files>Plugins/DataWarehouse.Plugins.UniversalFabric/Migration/MigrationJob.cs, Plugins/DataWarehouse.Plugins.UniversalFabric/Migration/MigrationProgress.cs</files>
  <action>
1. MigrationJob.cs:

```csharp
public class MigrationJob
{
    public string JobId { get; } = Guid.NewGuid().ToString("N");
    public required string SourceBackendId { get; init; }
    public required string DestinationBackendId { get; init; }
    public string? SourcePrefix { get; init; }              // Filter: only migrate objects matching prefix
    public MigrationMode Mode { get; init; } = MigrationMode.Copy; // Copy or Move
    public MigrationJobStatus Status { get; private set; } = MigrationJobStatus.Pending;
    public DateTime CreatedAt { get; } = DateTime.UtcNow;
    public DateTime? StartedAt { get; private set; }
    public DateTime? CompletedAt { get; private set; }
    public string? ErrorMessage { get; private set; }

    // Progress
    public long TotalObjects { get; private set; }
    public long MigratedObjects { get; private set; }
    public long FailedObjects { get; private set; }
    public long SkippedObjects { get; private set; }
    public long TotalBytes { get; private set; }
    public long MigratedBytes { get; private set; }

    // Configuration
    public int MaxConcurrency { get; init; } = 4;          // Parallel object transfers
    public int MaxRetries { get; init; } = 3;
    public bool VerifyAfterCopy { get; init; } = true;     // Compare checksums
    public bool DeleteSourceAfterVerify { get; init; }     // Only for Move mode
    public bool SkipExisting { get; init; } = true;        // Skip if dest already has object

    // Failed objects log
    private readonly ConcurrentBag<MigrationFailure> _failures = new();
    public IReadOnlyList<MigrationFailure> Failures => _failures.ToList();

    // State transitions
    public void Start() { Status = MigrationJobStatus.Running; StartedAt = DateTime.UtcNow; }
    public void Pause() { Status = MigrationJobStatus.Paused; }
    public void Resume() { Status = MigrationJobStatus.Running; }
    public void Complete() { Status = MigrationJobStatus.Completed; CompletedAt = DateTime.UtcNow; }
    public void Fail(string error) { Status = MigrationJobStatus.Failed; ErrorMessage = error; CompletedAt = DateTime.UtcNow; }
    public void Cancel() { Status = MigrationJobStatus.Cancelled; CompletedAt = DateTime.UtcNow; }

    // Progress updates (thread-safe)
    public void RecordMigrated(long bytes) { Interlocked.Increment(ref _migratedObjects); Interlocked.Add(ref _migratedBytes, bytes); }
    public void RecordFailed(string key, string error) { Interlocked.Increment(ref _failedObjects); _failures.Add(new(key, error)); }
    public void RecordSkipped() { Interlocked.Increment(ref _skippedObjects); }
    public void SetTotal(long objects, long bytes) { _totalObjects = objects; _totalBytes = bytes; }

    // Private backing fields for Interlocked
    private long _migratedObjects, _failedObjects, _skippedObjects, _totalObjects, _totalBytes, _migratedBytes;
}

public enum MigrationJobStatus { Pending, Running, Paused, Completed, Failed, Cancelled }
public enum MigrationMode { Copy, Move }
public record MigrationFailure(string Key, string Error);
```

2. MigrationProgress.cs:

```csharp
public record MigrationProgress
{
    public required string JobId { get; init; }
    public required MigrationJobStatus Status { get; init; }
    public required long TotalObjects { get; init; }
    public required long MigratedObjects { get; init; }
    public required long FailedObjects { get; init; }
    public required long SkippedObjects { get; init; }
    public required long TotalBytes { get; init; }
    public required long MigratedBytes { get; init; }
    public double PercentComplete => TotalObjects > 0 ? (MigratedObjects + SkippedObjects) * 100.0 / TotalObjects : 0;
    public TimeSpan Elapsed { get; init; }
    public double BytesPerSecond => Elapsed.TotalSeconds > 0 ? MigratedBytes / Elapsed.TotalSeconds : 0;
    public TimeSpan? EstimatedRemaining => BytesPerSecond > 0 ? TimeSpan.FromSeconds((TotalBytes - MigratedBytes) / BytesPerSecond) : null;
}
```
  </action>
  <verify>dotnet build Plugins/DataWarehouse.Plugins.UniversalFabric/DataWarehouse.Plugins.UniversalFabric.csproj 2>&1 | tail -5</verify>
  <done>MigrationJob and MigrationProgress types compile with thread-safe progress tracking</done>
</task>

<task type="auto">
  <name>Task 2: Implement LiveMigrationEngine</name>
  <files>Plugins/DataWarehouse.Plugins.UniversalFabric/Migration/LiveMigrationEngine.cs</files>
  <action>
```csharp
public class LiveMigrationEngine
{
    private readonly IStorageFabric _fabric;
    private readonly ConcurrentDictionary<string, MigrationJob> _jobs = new();
    private readonly ConcurrentDictionary<string, CancellationTokenSource> _jobCts = new();

    public LiveMigrationEngine(IStorageFabric fabric)

    // Start a new migration job
    public async Task<MigrationJob> StartMigrationAsync(MigrationJob job, CancellationToken ct = default)
    {
        _jobs[job.JobId] = job;
        var cts = CancellationTokenSource.CreateLinkedTokenSource(ct);
        _jobCts[job.JobId] = cts;
        job.Start();

        // Run migration in background
        _ = Task.Run(() => ExecuteMigrationAsync(job, cts.Token), cts.Token);
        return job;
    }

    private async Task ExecuteMigrationAsync(MigrationJob job, CancellationToken ct)
    {
        try
        {
            var sourceBackend = _fabric.Registry.GetStrategy(job.SourceBackendId)
                ?? throw new BackendNotFoundException(job.SourceBackendId);
            var destBackend = _fabric.Registry.GetStrategy(job.DestinationBackendId)
                ?? throw new BackendNotFoundException(job.DestinationBackendId);

            // Phase 1: Enumerate source objects
            var objects = new List<StorageObjectMetadata>();
            await foreach (var obj in sourceBackend.ListAsync(job.SourcePrefix, ct))
            {
                objects.Add(obj);
            }
            job.SetTotal(objects.Count, objects.Sum(o => o.Size));

            // Phase 2: Migrate objects with bounded parallelism
            using var semaphore = new SemaphoreSlim(job.MaxConcurrency);
            var tasks = objects.Select(async obj =>
            {
                await semaphore.WaitAsync(ct);
                try
                {
                    if (job.Status == MigrationJobStatus.Paused)
                    {
                        // Wait for resume
                        while (job.Status == MigrationJobStatus.Paused && !ct.IsCancellationRequested)
                            await Task.Delay(1000, ct);
                    }

                    ct.ThrowIfCancellationRequested();
                    await MigrateObjectAsync(sourceBackend, destBackend, obj, job, ct);
                }
                finally { semaphore.Release(); }
            });

            await Task.WhenAll(tasks);
            job.Complete();
        }
        catch (OperationCanceledException) { job.Cancel(); }
        catch (Exception ex) { job.Fail(ex.Message); }
    }

    private async Task MigrateObjectAsync(
        IStorageStrategy source, IStorageStrategy dest,
        StorageObjectMetadata obj, MigrationJob job, CancellationToken ct)
    {
        for (int attempt = 0; attempt <= job.MaxRetries; attempt++)
        {
            try
            {
                // Check if exists in destination (skip if configured)
                if (job.SkipExisting && await dest.ExistsAsync(obj.Key, ct))
                {
                    job.RecordSkipped();
                    return;
                }

                // Stream from source to destination (no full buffering)
                using var stream = await source.RetrieveAsync(obj.Key, ct);
                await dest.StoreAsync(obj.Key, stream, obj.CustomMetadata as IDictionary<string, string>, ct);

                // Verify if configured
                if (job.VerifyAfterCopy)
                {
                    var destMeta = await dest.GetMetadataAsync(obj.Key, ct);
                    if (destMeta.Size != obj.Size)
                        throw new IOException($"Size mismatch: source={obj.Size}, dest={destMeta.Size}");
                }

                // Delete source if Move mode
                if (job.Mode == MigrationMode.Move && job.DeleteSourceAfterVerify)
                    await source.DeleteAsync(obj.Key, ct);

                job.RecordMigrated(obj.Size);
                return;
            }
            catch (Exception ex) when (attempt < job.MaxRetries)
            {
                await Task.Delay(TimeSpan.FromSeconds(Math.Pow(2, attempt)), ct);
            }
            catch (Exception ex)
            {
                job.RecordFailed(obj.Key, ex.Message);
            }
        }
    }

    // Job management
    public MigrationProgress GetProgress(string jobId) { /* build from job */ }
    public void PauseJob(string jobId) { _jobs[jobId].Pause(); }
    public void ResumeJob(string jobId) { _jobs[jobId].Resume(); }
    public void CancelJob(string jobId) { _jobCts[jobId].Cancel(); }
    public IReadOnlyList<MigrationJob> ListJobs() => _jobs.Values.ToList();
}
```
  </action>
  <verify>dotnet build Plugins/DataWarehouse.Plugins.UniversalFabric/DataWarehouse.Plugins.UniversalFabric.csproj 2>&1 | tail -5</verify>
  <done>LiveMigrationEngine compiles with streaming migration, bounded parallelism, pause/resume, verification, and retry</done>
</task>

</tasks>

<verification>
- `dotnet build Plugins/DataWarehouse.Plugins.UniversalFabric/` compiles
- Migration streams data (no full object buffering)
- Pause/resume works via job status checks
</verification>

<success_criteria>
- Data moves between any two backends via streaming
- Failed objects tracked with error details
- Progress includes ETA and throughput metrics
- Verification confirms integrity after copy
</success_criteria>

<output>
After completion, create `.planning/phases/63-universal-fabric/63-09-SUMMARY.md`
</output>
