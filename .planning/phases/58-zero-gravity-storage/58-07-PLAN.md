---
phase: 58-zero-gravity-storage
plan: 07
type: execute
wave: 2
depends_on: ["58-01"]
files_modified:
  - DataWarehouse.SDK/Storage/Migration/BackgroundMigrationEngine.cs
  - DataWarehouse.SDK/Storage/Migration/ReadForwardingTable.cs
  - DataWarehouse.SDK/Storage/Migration/MigrationCheckpointStore.cs
autonomous: true

must_haves:
  truths:
    - "Migration engine moves data between nodes without downtime"
    - "Read forwarding ensures reads succeed during migration (transparently)"
    - "Migration is checkpointed and resumable after failure"
    - "Throttling prevents migration from overwhelming production traffic"
  artifacts:
    - path: "DataWarehouse.SDK/Storage/Migration/BackgroundMigrationEngine.cs"
      provides: "Background migration engine with throttling, checkpointing, read forwarding"
      exports: ["BackgroundMigrationEngine"]
      contains: "IMigrationEngine"
    - path: "DataWarehouse.SDK/Storage/Migration/ReadForwardingTable.cs"
      provides: "In-memory forwarding table for transparent read redirection"
      exports: ["ReadForwardingTable"]
    - path: "DataWarehouse.SDK/Storage/Migration/MigrationCheckpointStore.cs"
      provides: "File-based checkpoint persistence for migration resumption"
      exports: ["MigrationCheckpointStore"]
  key_links:
    - from: "BackgroundMigrationEngine"
      to: "ReadForwardingTable"
      via: "registers forwarding entries during object migration"
      pattern: "_forwardingTable\\.RegisterForwarding"
    - from: "BackgroundMigrationEngine"
      to: "MigrationCheckpointStore"
      via: "saves progress after each batch"
      pattern: "_checkpointStore\\.SaveCheckpointAsync"
---

<objective>
Implement a background migration engine that moves data between storage nodes with zero downtime. Uses read forwarding during migration (reads to migrating objects are transparently forwarded to the new location) and checkpointing for crash recovery.

Purpose: The rebalancer (Plan 08) needs a mechanism to actually move data. This engine handles the mechanics: copy data, register forwarding entry, verify checksum, remove source, with throttling to avoid overwhelming production traffic.
Output: BackgroundMigrationEngine + ReadForwardingTable + MigrationCheckpointStore.
</objective>

<execution_context>
@C:/Users/ddamien/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/ddamien/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/phases/58-zero-gravity-storage/58-01-SUMMARY.md
@DataWarehouse.SDK/Storage/Migration/IMigrationEngine.cs
@DataWarehouse.SDK/Storage/Migration/MigrationTypes.cs
</context>

<tasks>

<task type="auto">
  <name>Task 1: ReadForwardingTable and MigrationCheckpointStore</name>
  <files>
    DataWarehouse.SDK/Storage/Migration/ReadForwardingTable.cs
    DataWarehouse.SDK/Storage/Migration/MigrationCheckpointStore.cs
  </files>
  <action>
**ReadForwardingTable.cs** — namespace `DataWarehouse.SDK.Storage.Migration`:

Thread-safe forwarding table that maps object keys to their new locations during migration.

```csharp
/// In-memory forwarding table for transparent read redirection during data migration.
/// When an object is being migrated from NodeA to NodeB:
/// 1. Object is copied to NodeB
/// 2. Forwarding entry registered: objectKey -> NodeB
/// 3. Reads for objectKey on NodeA are forwarded to NodeB
/// 4. After migration completes, forwarding entry expires
///
/// Thread-safe via ConcurrentDictionary. Entries have TTL for automatic cleanup.
[SdkCompatibility("5.0.0", Notes = "Phase 58: Background migration read forwarding")]
public sealed class ReadForwardingTable : IDisposable
{
    private readonly ConcurrentDictionary<string, ReadForwardingEntry> _entries = new();
    private readonly Timer _cleanupTimer;
    private readonly TimeSpan _defaultTtl;
    private bool _disposed;

    public ReadForwardingTable(TimeSpan? defaultTtl = null)
    {
        _defaultTtl = defaultTtl ?? TimeSpan.FromHours(24);
        _cleanupTimer = new Timer(_ => CleanExpired(), null, TimeSpan.FromMinutes(5), TimeSpan.FromMinutes(5));
    }

    public int ActiveEntries => _entries.Count;

    /// Register a forwarding entry for an object being migrated.
    public void RegisterForwarding(string objectKey, string originalNode, string newNode, int maxHops = 3)
    {
        var entry = new ReadForwardingEntry
        {
            ObjectKey = objectKey,
            OriginalNode = originalNode,
            NewNode = newNode,
            ExpiresUtc = DateTimeOffset.UtcNow.Add(_defaultTtl),
            Hops = maxHops
        };
        _entries.AddOrUpdate(objectKey, entry, (_, _) => entry);
    }

    /// Lookup forwarding for an object. Returns null if no forwarding active.
    public ReadForwardingEntry? Lookup(string objectKey)
    {
        if (_entries.TryGetValue(objectKey, out var entry))
        {
            if (entry.ExpiresUtc > DateTimeOffset.UtcNow && entry.Hops > 0)
                return entry;
            _entries.TryRemove(objectKey, out _);
        }
        return null;
    }

    /// Remove forwarding after migration is fully complete and verified.
    public bool RemoveForwarding(string objectKey)
    {
        return _entries.TryRemove(objectKey, out _);
    }

    /// Bulk remove all forwarding entries for a migration job.
    public int RemoveByOriginalNode(string nodeId)
    {
        var toRemove = _entries.Where(e => e.Value.OriginalNode == nodeId).Select(e => e.Key).ToList();
        int removed = 0;
        foreach (var key in toRemove)
            if (_entries.TryRemove(key, out _)) removed++;
        return removed;
    }

    private void CleanExpired()
    {
        var now = DateTimeOffset.UtcNow;
        var expired = _entries.Where(e => e.Value.ExpiresUtc <= now).Select(e => e.Key).ToList();
        foreach (var key in expired)
            _entries.TryRemove(key, out _);
    }

    public void Dispose()
    {
        if (!_disposed) { _disposed = true; _cleanupTimer.Dispose(); }
    }
}
```

**MigrationCheckpointStore.cs**:

```csharp
/// File-based checkpoint store for migration job resumption.
/// Saves progress after each batch so migrations can resume after crashes.
[SdkCompatibility("5.0.0", Notes = "Phase 58: Background migration checkpointing")]
public sealed class MigrationCheckpointStore
{
    private readonly string _checkpointDirectory;
    private readonly ConcurrentDictionary<string, MigrationCheckpoint> _inMemory = new();

    public MigrationCheckpointStore(string checkpointDirectory)
    {
        _checkpointDirectory = checkpointDirectory ?? throw new ArgumentNullException(nameof(checkpointDirectory));
        Directory.CreateDirectory(checkpointDirectory);
    }

    public async Task SaveCheckpointAsync(MigrationCheckpoint checkpoint, CancellationToken ct = default)
    {
        _inMemory.AddOrUpdate(checkpoint.JobId, checkpoint, (_, _) => checkpoint);
        var path = GetCheckpointPath(checkpoint.JobId);
        var json = System.Text.Json.JsonSerializer.Serialize(checkpoint);
        await File.WriteAllTextAsync(path, json, ct);
    }

    public async Task<MigrationCheckpoint?> LoadCheckpointAsync(string jobId, CancellationToken ct = default)
    {
        if (_inMemory.TryGetValue(jobId, out var cached))
            return cached;

        var path = GetCheckpointPath(jobId);
        if (!File.Exists(path)) return null;

        var json = await File.ReadAllTextAsync(path, ct);
        var checkpoint = System.Text.Json.JsonSerializer.Deserialize<MigrationCheckpoint>(json);
        if (checkpoint != null)
            _inMemory.TryAdd(checkpoint.JobId, checkpoint);
        return checkpoint;
    }

    public Task DeleteCheckpointAsync(string jobId, CancellationToken ct = default)
    {
        _inMemory.TryRemove(jobId, out _);
        var path = GetCheckpointPath(jobId);
        if (File.Exists(path)) File.Delete(path);
        return Task.CompletedTask;
    }

    private string GetCheckpointPath(string jobId) =>
        Path.Combine(_checkpointDirectory, $"migration-{jobId}.checkpoint.json");
}
```

Add necessary usings: `System.Collections.Concurrent`, `System.Text.Json`, `System.IO`.
  </action>
  <verify>dotnet build DataWarehouse.SDK/DataWarehouse.SDK.csproj --no-restore --verbosity quiet 2>&1 | tail -3</verify>
  <done>ReadForwardingTable and MigrationCheckpointStore compile. Forwarding table has RegisterForwarding, Lookup, TTL cleanup. Checkpoint store has file-based persistence.</done>
</task>

<task type="auto">
  <name>Task 2: BackgroundMigrationEngine</name>
  <files>DataWarehouse.SDK/Storage/Migration/BackgroundMigrationEngine.cs</files>
  <action>
Create BackgroundMigrationEngine.cs — namespace `DataWarehouse.SDK.Storage.Migration`:

```csharp
/// Background migration engine implementing zero-downtime data migration.
///
/// Migration lifecycle per object:
/// 1. COPY: Read from source, write to target (respects throttle)
/// 2. FORWARD: Register read forwarding entry (reads go to new location)
/// 3. VERIFY: Checksum comparison between source and target
/// 4. CLEANUP: Remove source copy (after forwarding TTL or explicit confirmation)
///
/// Features:
/// - Throttling: configurable bytes/sec limit to avoid overwhelming production
/// - Checkpointing: progress saved after each batch for crash recovery
/// - Read forwarding: transparent redirection during migration
/// - Batch processing: configurable batch size for memory efficiency
/// - Pause/Resume: graceful pause with checkpoint save
/// - Cancellation: clean abort with partial migration rollback
[SdkCompatibility("5.0.0", Notes = "Phase 58: Background migration engine")]
public sealed class BackgroundMigrationEngine : IMigrationEngine, IDisposable
{
    private readonly ReadForwardingTable _forwardingTable;
    private readonly MigrationCheckpointStore _checkpointStore;
    private readonly ConcurrentDictionary<string, MigrationJobState> _jobs = new();
    private readonly int _batchSize;

    // Data operation delegates (injected by host)
    public Func<string, string, CancellationToken, Task<Stream>>? ReadObjectAsync { get; set; }
    public Func<string, string, Stream, CancellationToken, Task>? WriteObjectAsync { get; set; }
    public Func<string, string, CancellationToken, Task<bool>>? DeleteObjectAsync { get; set; }
    public Func<string, string, CancellationToken, Task<byte[]>>? GetChecksumAsync { get; set; }

    public BackgroundMigrationEngine(
        ReadForwardingTable forwardingTable,
        MigrationCheckpointStore checkpointStore,
        int batchSize = 100)
    {
        _forwardingTable = forwardingTable ?? throw new ArgumentNullException(nameof(forwardingTable));
        _checkpointStore = checkpointStore ?? throw new ArgumentNullException(nameof(checkpointStore));
        _batchSize = batchSize;
    }

    public async Task<MigrationJob> StartMigrationAsync(MigrationPlan plan, CancellationToken ct = default)
    {
        var jobId = Guid.NewGuid().ToString("N")[..12];
        var job = new MigrationJob
        {
            JobId = jobId,
            Description = $"Migrate {plan.Objects.Count} objects from {plan.SourceNode} to {plan.TargetNode}",
            Plan = plan,
            Status = MigrationStatus.Preparing,
            CreatedUtc = DateTimeOffset.UtcNow,
            TotalObjects = plan.Objects.Count,
            MigratedObjects = 0,
            FailedObjects = 0,
            BytesMigrated = 0,
            CurrentThroughputBytesPerSec = 0
        };

        var state = new MigrationJobState
        {
            Job = job,
            Cts = CancellationTokenSource.CreateLinkedTokenSource(ct),
            Paused = false
        };
        _jobs[jobId] = state;

        // Check for existing checkpoint (resume)
        var checkpoint = await _checkpointStore.LoadCheckpointAsync(jobId, ct);

        // Start background execution
        _ = Task.Run(() => ExecuteMigrationAsync(state, checkpoint), state.Cts.Token);

        return job with { Status = MigrationStatus.InProgress, StartedUtc = DateTimeOffset.UtcNow };
    }

    public Task PauseMigrationAsync(string jobId, CancellationToken ct = default)
    {
        if (_jobs.TryGetValue(jobId, out var state))
            state.Paused = true;
        return Task.CompletedTask;
    }

    public Task ResumeMigrationAsync(string jobId, CancellationToken ct = default)
    {
        if (_jobs.TryGetValue(jobId, out var state))
            state.Paused = false;
        return Task.CompletedTask;
    }

    public Task CancelMigrationAsync(string jobId, CancellationToken ct = default)
    {
        if (_jobs.TryGetValue(jobId, out var state))
        {
            state.Cts.Cancel();
            state.Job = state.Job with { Status = MigrationStatus.Cancelled };
        }
        return Task.CompletedTask;
    }

    public Task<MigrationJob> GetStatusAsync(string jobId, CancellationToken ct = default)
    {
        if (_jobs.TryGetValue(jobId, out var state))
            return Task.FromResult(state.Job);
        throw new KeyNotFoundException($"Migration job {jobId} not found.");
    }

    public Task<IReadOnlyList<MigrationJob>> ListJobsAsync(MigrationStatus? statusFilter = null, CancellationToken ct = default)
    {
        var jobs = _jobs.Values.Select(s => s.Job);
        if (statusFilter.HasValue)
            jobs = jobs.Where(j => j.Status == statusFilter.Value);
        return Task.FromResult<IReadOnlyList<MigrationJob>>(jobs.ToList());
    }

    public async IAsyncEnumerable<MigrationJob> MonitorAsync(
        string jobId, [System.Runtime.CompilerServices.EnumeratorCancellation] CancellationToken ct = default)
    {
        while (!ct.IsCancellationRequested)
        {
            if (_jobs.TryGetValue(jobId, out var state))
            {
                yield return state.Job;
                if (state.Job.Status is MigrationStatus.Completed or MigrationStatus.Failed or MigrationStatus.Cancelled)
                    yield break;
            }
            else
            {
                yield break;
            }
            await Task.Delay(1000, ct);
        }
    }

    public Task<ReadForwardingEntry?> GetForwardingEntryAsync(string objectKey, CancellationToken ct = default)
    {
        return Task.FromResult(_forwardingTable.Lookup(objectKey));
    }

    private async Task ExecuteMigrationAsync(MigrationJobState state, MigrationCheckpoint? checkpoint)
    {
        var plan = state.Job.Plan;
        var objects = plan.Objects.ToList();
        int startIndex = 0;
        long bytesMigrated = 0;

        // Resume from checkpoint if available
        if (checkpoint != null)
        {
            startIndex = (int)checkpoint.ProcessedCount;
            bytesMigrated = state.Job.BytesMigrated;
        }

        state.Job = state.Job with { Status = MigrationStatus.InProgress, StartedUtc = DateTimeOffset.UtcNow };
        var sw = System.Diagnostics.Stopwatch.StartNew();

        try
        {
            for (int i = startIndex; i < objects.Count; i++)
            {
                state.Cts.Token.ThrowIfCancellationRequested();

                // Pause support
                while (state.Paused && !state.Cts.Token.IsCancellationRequested)
                {
                    state.Job = state.Job with { Status = MigrationStatus.Paused };
                    await Task.Delay(500, state.Cts.Token);
                }
                state.Job = state.Job with { Status = MigrationStatus.InProgress };

                var obj = objects[i];
                try
                {
                    await MigrateObjectAsync(obj, plan, state);
                    bytesMigrated += obj.SizeBytes;
                    state.Job = state.Job with
                    {
                        MigratedObjects = state.Job.MigratedObjects + 1,
                        BytesMigrated = bytesMigrated,
                        CurrentThroughputBytesPerSec = bytesMigrated / Math.Max(1, sw.Elapsed.TotalSeconds)
                    };
                }
                catch (Exception)
                {
                    state.Job = state.Job with { FailedObjects = state.Job.FailedObjects + 1 };
                }

                // Throttle
                if (plan.ThrottleBytesPerSec.HasValue && plan.ThrottleBytesPerSec.Value > 0)
                {
                    double targetElapsed = bytesMigrated / (double)plan.ThrottleBytesPerSec.Value;
                    double actualElapsed = sw.Elapsed.TotalSeconds;
                    if (targetElapsed > actualElapsed)
                        await Task.Delay(TimeSpan.FromSeconds(targetElapsed - actualElapsed), state.Cts.Token);
                }

                // Checkpoint every batch
                if ((i + 1) % _batchSize == 0)
                {
                    await _checkpointStore.SaveCheckpointAsync(new MigrationCheckpoint
                    {
                        JobId = state.Job.JobId,
                        LastProcessedKey = obj.ObjectKey,
                        ProcessedCount = i + 1,
                        TimestampUtc = DateTimeOffset.UtcNow
                    });
                }
            }

            state.Job = state.Job with
            {
                Status = MigrationStatus.Completed,
                CompletedUtc = DateTimeOffset.UtcNow
            };
            await _checkpointStore.DeleteCheckpointAsync(state.Job.JobId);
        }
        catch (OperationCanceledException)
        {
            state.Job = state.Job with { Status = MigrationStatus.Cancelled };
        }
        catch (Exception)
        {
            state.Job = state.Job with { Status = MigrationStatus.Failed };
        }
    }

    private async Task MigrateObjectAsync(MigrationObject obj, MigrationPlan plan, MigrationJobState state)
    {
        if (ReadObjectAsync == null || WriteObjectAsync == null)
            throw new InvalidOperationException("Data operation delegates must be set before starting migration.");

        // 1. Copy data
        using var sourceStream = await ReadObjectAsync(obj.ObjectKey, plan.SourceNode, state.Cts.Token);
        await WriteObjectAsync(obj.ObjectKey, plan.TargetNode, sourceStream, state.Cts.Token);

        // 2. Register read forwarding
        if (plan.EnableReadForwarding)
        {
            _forwardingTable.RegisterForwarding(obj.ObjectKey, plan.SourceNode, plan.TargetNode);
            state.Job = state.Job with { Status = MigrationStatus.ReadForwarding };
        }

        // 3. Verify checksums
        if (plan.ValidateChecksums && GetChecksumAsync != null)
        {
            var sourceChecksum = await GetChecksumAsync(obj.ObjectKey, plan.SourceNode, state.Cts.Token);
            var targetChecksum = await GetChecksumAsync(obj.ObjectKey, plan.TargetNode, state.Cts.Token);
            if (!sourceChecksum.SequenceEqual(targetChecksum))
                throw new InvalidOperationException($"Checksum mismatch for {obj.ObjectKey} after migration.");
        }

        // 4. Delete source (only if zero-downtime mode allows)
        if (!plan.ZeroDowntime && DeleteObjectAsync != null)
        {
            await DeleteObjectAsync(obj.ObjectKey, plan.SourceNode, state.Cts.Token);
            _forwardingTable.RemoveForwarding(obj.ObjectKey);
        }
    }

    public void Dispose()
    {
        foreach (var state in _jobs.Values)
            state.Cts.Dispose();
    }

    private sealed class MigrationJobState
    {
        public MigrationJob Job { get; set; } = null!;
        public CancellationTokenSource Cts { get; set; } = null!;
        public volatile bool Paused;
    }
}
```
  </action>
  <verify>dotnet build DataWarehouse.SDK/DataWarehouse.SDK.csproj --no-restore --verbosity quiet 2>&1 | tail -3</verify>
  <done>BackgroundMigrationEngine compiles, implements IMigrationEngine. Has throttling, checkpointing, read forwarding, pause/resume. MonitorAsync uses IAsyncEnumerable for streaming updates.</done>
</task>

</tasks>

<verification>
- `dotnet build DataWarehouse.SDK/DataWarehouse.SDK.csproj` succeeds
- BackgroundMigrationEngine implements all 8 IMigrationEngine methods
- ReadForwardingTable has Lookup, RegisterForwarding, TTL cleanup
- MigrationCheckpointStore persists to JSON files
</verification>

<success_criteria>
Background migration engine supports zero-downtime migration with read forwarding. Checkpointing enables crash recovery. Throttling prevents production impact. Pause/resume/cancel lifecycle works.
</success_criteria>

<output>
After completion, create `.planning/phases/58-zero-gravity-storage/58-07-SUMMARY.md`
</output>
