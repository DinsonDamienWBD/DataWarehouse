---
phase: 58-zero-gravity-storage
plan: 04
type: execute
wave: 2
depends_on: ["58-01"]
files_modified:
  - DataWarehouse.SDK/Storage/Placement/CrushPlacementAlgorithm.cs
  - DataWarehouse.SDK/Storage/Placement/CrushBucket.cs
autonomous: true

must_haves:
  truths:
    - "CRUSH placement is deterministic: same inputs always produce same node selection"
    - "Client can compute placement locally without central lookup"
    - "Node addition/removal causes minimal data movement (proportional to capacity change)"
    - "Placement respects zone/rack/host failure domain separation"
  artifacts:
    - path: "DataWarehouse.SDK/Storage/Placement/CrushPlacementAlgorithm.cs"
      provides: "CRUSH-equivalent deterministic placement algorithm"
      exports: ["CrushPlacementAlgorithm"]
      contains: "IPlacementAlgorithm"
    - path: "DataWarehouse.SDK/Storage/Placement/CrushBucket.cs"
      provides: "Hierarchical bucket structure (root > zone > rack > host)"
      exports: ["CrushBucket", "BucketType"]
  key_links:
    - from: "CrushPlacementAlgorithm.ComputePlacement"
      to: "CrushBucket"
      via: "traverses bucket hierarchy to select nodes"
      pattern: "CrushBucket.*Select"
    - from: "CrushPlacementAlgorithm"
      to: "IPlacementAlgorithm"
      via: "implements interface"
      pattern: "class CrushPlacementAlgorithm : IPlacementAlgorithm"
---

<objective>
Implement a CRUSH-equivalent (Controlled Replication Under Scalable Hashing) deterministic placement algorithm. Given an object key and cluster map, computes placement without central lookup.

Purpose: Eliminates central metadata lookup for object location. Any node can compute where an object lives by running the same CRUSH function locally. This is the foundation of zero-gravity storage — objects can be placed and found without a centralized catalog.
Output: CrushPlacementAlgorithm implementing IPlacementAlgorithm + CrushBucket hierarchy.
</objective>

<execution_context>
@C:/Users/ddamien/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/ddamien/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/phases/58-zero-gravity-storage/58-01-SUMMARY.md
@DataWarehouse.SDK/Storage/Placement/PlacementTypes.cs
@DataWarehouse.SDK/Storage/Placement/IPlacementAlgorithm.cs
</context>

<tasks>

<task type="auto">
  <name>Task 1: CrushBucket Hierarchy</name>
  <files>DataWarehouse.SDK/Storage/Placement/CrushBucket.cs</files>
  <action>
Create CrushBucket.cs — namespace `DataWarehouse.SDK.Storage.Placement`:

```csharp
/// Bucket types in the CRUSH hierarchy (most general to most specific).
public enum BucketType { Root, Zone, Rack, Host }

/// A bucket in the CRUSH hierarchy tree.
/// CRUSH uses a hierarchical structure: Root -> Zones -> Racks -> Hosts (leaf nodes with actual storage).
/// Each bucket has a selection algorithm for choosing children.
[SdkCompatibility("5.0.0", Notes = "Phase 58: CRUSH-equivalent placement")]
public sealed class CrushBucket
{
    public string Id { get; init; }
    public BucketType Type { get; init; }
    public double Weight { get; init; } = 1.0;
    public List<CrushBucket> Children { get; } = new();
    public NodeDescriptor? Node { get; init; } // Only set for Host-level (leaf) buckets

    /// Straw2 selection: deterministic weighted selection from children.
    /// Given a placement group seed, selects one child using the Straw2 algorithm.
    /// Straw2 assigns each child a "straw length" = hash(input, child.id) * child.weight.
    /// The child with the longest straw wins. This is deterministic and weight-proportional.
    public CrushBucket SelectChild(uint pgSeed, int replicaIndex)
    {
        if (Children.Count == 0)
            throw new InvalidOperationException($"Bucket {Id} has no children to select from.");
        if (Children.Count == 1)
            return Children[0];

        CrushBucket? winner = null;
        long maxStraw = long.MinValue;

        foreach (var child in Children)
        {
            // Straw2 hash: combine pgSeed, replicaIndex, and child ID
            uint hash = CrushHash(pgSeed, (uint)replicaIndex, child.Id);
            // Weight adjustment: ln(hash/0xFFFF) / weight
            // Using simplified integer arithmetic: straw = hash * weight
            long straw = (long)(hash * child.Weight * 65536.0);

            if (straw > maxStraw)
            {
                maxStraw = straw;
                winner = child;
            }
        }

        return winner!;
    }

    /// CRUSH hash function: Jenkins one-at-a-time variant for deterministic mixing.
    private static uint CrushHash(uint seed1, uint seed2, string childId)
    {
        uint hash = seed1 ^ (seed2 * 0x5BD1E995);
        foreach (char c in childId)
        {
            hash += (uint)c;
            hash += hash << 10;
            hash ^= hash >> 6;
        }
        hash += hash << 3;
        hash ^= hash >> 11;
        hash += hash << 15;
        return hash;
    }

    /// Builds a CRUSH bucket hierarchy from a flat list of NodeDescriptors.
    /// Groups by Zone -> Rack -> Host automatically.
    public static CrushBucket BuildHierarchy(IReadOnlyList<NodeDescriptor> nodes)
    {
        var root = new CrushBucket { Id = "root", Type = BucketType.Root };

        var byZone = nodes.GroupBy(n => n.Zone ?? "default-zone");
        foreach (var zoneGroup in byZone)
        {
            var zoneBucket = new CrushBucket
            {
                Id = $"zone-{zoneGroup.Key}",
                Type = BucketType.Zone,
                Weight = zoneGroup.Sum(n => n.Weight)
            };

            var byRack = zoneGroup.GroupBy(n => n.Rack ?? "default-rack");
            foreach (var rackGroup in byRack)
            {
                var rackBucket = new CrushBucket
                {
                    Id = $"rack-{zoneGroup.Key}-{rackGroup.Key}",
                    Type = BucketType.Rack,
                    Weight = rackGroup.Sum(n => n.Weight)
                };

                foreach (var node in rackGroup)
                {
                    var hostBucket = new CrushBucket
                    {
                        Id = $"host-{node.NodeId}",
                        Type = BucketType.Host,
                        Weight = node.Weight,
                        Node = node
                    };
                    rackBucket.Children.Add(hostBucket);
                }
                zoneBucket.Children.Add(rackBucket);
            }
            root.Children.Add(zoneBucket);
        }

        root = root with { Weight = nodes.Sum(n => n.Weight) };
        return root;
    }
}
```

Use `record` for CrushBucket with `init` setters so the `with` expression works. Ensure Children is a mutable list (records can have mutable list properties).
  </action>
  <verify>dotnet build DataWarehouse.SDK/DataWarehouse.SDK.csproj --no-restore --verbosity quiet 2>&1 | tail -3</verify>
  <done>CrushBucket compiles. BuildHierarchy creates Root > Zone > Rack > Host. SelectChild uses Straw2 algorithm with Jenkins hash.</done>
</task>

<task type="auto">
  <name>Task 2: CrushPlacementAlgorithm Implementation</name>
  <files>DataWarehouse.SDK/Storage/Placement/CrushPlacementAlgorithm.cs</files>
  <action>
Create CrushPlacementAlgorithm.cs — namespace `DataWarehouse.SDK.Storage.Placement`:

```csharp
/// CRUSH-equivalent deterministic placement algorithm.
/// Given an object key and cluster map, deterministically selects primary + replica nodes
/// without any central lookup. Any client with the same cluster map computes the same result.
///
/// Algorithm:
/// 1. Hash the object key to produce a placement group seed (uint)
/// 2. Build a bucket hierarchy from the cluster map
/// 3. For each replica (0..N-1), traverse the hierarchy using Straw2 selection
/// 4. Apply failure domain separation (no two replicas in same zone/rack unless forced)
/// 5. Apply placement constraints (storage class, tags, compliance regions)
[SdkCompatibility("5.0.0", Notes = "Phase 58: CRUSH-equivalent deterministic placement")]
public sealed class CrushPlacementAlgorithm : IPlacementAlgorithm
{
    private const int MaxRetries = 50; // prevent infinite loops on constraint violations

    public PlacementDecision ComputePlacement(
        PlacementTarget target,
        IReadOnlyList<NodeDescriptor> clusterMap,
        IReadOnlyList<PlacementConstraint>? constraints = null)
    {
        if (clusterMap.Count == 0)
            throw new ArgumentException("Cluster map is empty.", nameof(clusterMap));

        // Filter nodes by constraints first
        var eligibleNodes = ApplyConstraints(clusterMap, target, constraints);
        if (eligibleNodes.Count == 0)
            throw new InvalidOperationException("No eligible nodes after applying placement constraints.");

        var hierarchy = CrushBucket.BuildHierarchy(eligibleNodes);
        uint pgSeed = HashObjectKey(target.ObjectKey);
        int replicaCount = Math.Min(target.ReplicaCount, eligibleNodes.Count);

        var selectedNodes = new List<NodeDescriptor>();
        var usedZones = new HashSet<string>();
        var usedRacks = new HashSet<string>();
        var usedHosts = new HashSet<string>();

        for (int replica = 0; replica < replicaCount; replica++)
        {
            NodeDescriptor? selected = null;
            for (int retry = 0; retry < MaxRetries; retry++)
            {
                // Traverse hierarchy with replica-specific seed
                var leaf = TraverseHierarchy(hierarchy, pgSeed, replica + retry * 1000);

                if (leaf.Node == null) continue;
                var candidate = leaf.Node;

                // Failure domain separation: prefer different zones, then different racks
                bool zoneConflict = target.RequiredZones == null &&
                    usedZones.Contains(candidate.Zone ?? "") &&
                    eligibleNodes.Count(n => !usedZones.Contains(n.Zone ?? "")) > 0;
                bool rackConflict = usedRacks.Contains(candidate.Rack ?? "") &&
                    eligibleNodes.Count(n => !usedRacks.Contains(n.Rack ?? "")) > 0;
                bool hostConflict = usedHosts.Contains(candidate.NodeId);

                if (hostConflict) continue; // never place two replicas on same host
                if (zoneConflict && retry < MaxRetries / 2) continue; // try harder for zone separation
                if (rackConflict && retry < MaxRetries / 3) continue; // try for rack separation

                selected = candidate;
                break;
            }

            if (selected == null)
            {
                // Fallback: take any unused host
                selected = eligibleNodes.FirstOrDefault(n => !usedHosts.Contains(n.NodeId));
                if (selected == null) break; // exhausted all hosts
            }

            selectedNodes.Add(selected);
            usedZones.Add(selected.Zone ?? "");
            usedRacks.Add(selected.Rack ?? "");
            usedHosts.Add(selected.NodeId);
        }

        if (selectedNodes.Count == 0)
            throw new InvalidOperationException("Failed to select any nodes for placement.");

        return new PlacementDecision
        {
            TargetNodes = selectedNodes,
            PrimaryNode = selectedNodes[0],
            ReplicaNodes = selectedNodes.Skip(1).ToList(),
            PlacementRuleId = $"crush-{pgSeed:X8}",
            Deterministic = true,
            Timestamp = DateTimeOffset.UtcNow
        };
    }

    public PlacementDecision RecomputeOnNodeChange(
        PlacementTarget target,
        IReadOnlyList<NodeDescriptor> newClusterMap,
        PlacementDecision previousDecision)
    {
        // CRUSH property: recompute with new map. Only objects on changed nodes move.
        var newDecision = ComputePlacement(target, newClusterMap);

        // The CRUSH algorithm naturally minimizes movement because:
        // - Hash is stable (same key = same pgSeed)
        // - Straw2 only changes selection when weights shift significantly
        // - Removed nodes force reselection only for their objects
        return newDecision;
    }

    public double EstimateMovementOnResize(
        IReadOnlyList<NodeDescriptor> currentMap,
        IReadOnlyList<NodeDescriptor> newMap)
    {
        // Estimate: fraction of data that moves ≈ |capacity_change| / total_capacity
        double currentTotal = currentMap.Sum(n => n.Weight);
        double newTotal = newMap.Sum(n => n.Weight);

        if (currentTotal == 0) return 1.0;

        // Nodes removed: their data must move
        var removedNodes = currentMap.Where(c => !newMap.Any(n => n.NodeId == c.NodeId));
        double removedWeight = removedNodes.Sum(n => n.Weight);

        // Nodes added: pull data proportional to their weight
        var addedNodes = newMap.Where(n => !currentMap.Any(c => c.NodeId == n.NodeId));
        double addedWeight = addedNodes.Sum(n => n.Weight);

        return Math.Min(1.0, (removedWeight + addedWeight / newTotal * currentTotal) / currentTotal);
    }

    private CrushBucket TraverseHierarchy(CrushBucket bucket, uint pgSeed, int replicaIndex)
    {
        if (bucket.Type == BucketType.Host)
            return bucket;

        var child = bucket.SelectChild(pgSeed, replicaIndex);
        return TraverseHierarchy(child, pgSeed, replicaIndex);
    }

    private static IReadOnlyList<NodeDescriptor> ApplyConstraints(
        IReadOnlyList<NodeDescriptor> nodes,
        PlacementTarget target,
        IReadOnlyList<PlacementConstraint>? constraints)
    {
        var eligible = nodes.ToList();

        // Filter by required storage class
        if (target.RequiredStorageClass != null)
        {
            eligible = eligible.Where(n =>
                n.StorageClasses != null && n.StorageClasses.Contains(target.RequiredStorageClass.Value))
                .ToList();
        }

        // Filter by required zones
        if (target.RequiredZones != null && target.RequiredZones.Count > 0)
        {
            eligible = eligible.Where(n =>
                target.RequiredZones.Contains(n.Zone ?? ""))
                .ToList();
        }

        // Filter by required tags
        if (target.RequiredTags != null)
        {
            foreach (var tag in target.RequiredTags)
            {
                eligible = eligible.Where(n =>
                    n.Tags != null && n.Tags.TryGetValue(tag.Key, out var v) && v == tag.Value)
                    .ToList();
            }
        }

        // Apply explicit constraints
        if (constraints != null)
        {
            foreach (var constraint in constraints.Where(c => c.Required))
            {
                eligible = constraint.ConstraintType switch
                {
                    PlacementConstraintType.Zone => eligible.Where(n => n.Zone == constraint.Value).ToList(),
                    PlacementConstraintType.Rack => eligible.Where(n => n.Rack == constraint.Value).ToList(),
                    PlacementConstraintType.Host => eligible.Where(n => n.Host == constraint.Value).ToList(),
                    PlacementConstraintType.Tag => eligible.Where(n =>
                        n.Tags != null && n.Tags.TryGetValue(constraint.Key, out var v) && v == constraint.Value).ToList(),
                    _ => eligible
                };
            }
        }

        return eligible;
    }

    /// Deterministic hash of object key to placement group seed.
    /// Uses FNV-1a for speed and good distribution.
    private static uint HashObjectKey(string key)
    {
        uint hash = 2166136261u;
        foreach (char c in key)
        {
            hash ^= (uint)c;
            hash *= 16777619u;
        }
        return hash;
    }
}
```

NOTE: PlacementConstraint.ConstraintType is an enum — ensure PlacementTypes.cs from Plan 01 defined `PlacementConstraintType` enum (Zone, Rack, Host, Tag, StorageClass, Compliance). If the Plan 01 named it differently, adjust the reference here. The `ConstraintType` property on the `PlacementConstraint` record should use this enum.
  </action>
  <verify>dotnet build DataWarehouse.SDK/DataWarehouse.SDK.csproj --no-restore --verbosity quiet 2>&1 | tail -3</verify>
  <done>CrushPlacementAlgorithm compiles and implements IPlacementAlgorithm. ComputePlacement is deterministic (pure function of inputs). Straw2 selection with failure domain separation works. EstimateMovementOnResize returns proportion of affected data.</done>
</task>

</tasks>

<verification>
- `dotnet build DataWarehouse.SDK/DataWarehouse.SDK.csproj` succeeds
- CrushPlacementAlgorithm implements all 3 IPlacementAlgorithm methods
- BuildHierarchy creates proper Root > Zone > Rack > Host tree
- SelectChild uses Straw2 with Jenkins hash
</verification>

<success_criteria>
CRUSH-equivalent algorithm deterministically places objects across nodes. Supports zone/rack/host failure domain separation. Constraint filtering works. Movement estimation available for capacity planning.
</success_criteria>

<output>
After completion, create `.planning/phases/58-zero-gravity-storage/58-04-SUMMARY.md`
</output>
