---
phase: 35-hardware-accelerator-hypervisor
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - DataWarehouse.SDK/Hardware/Accelerators/GpuAccelerator.cs
  - DataWarehouse.SDK/Hardware/Accelerators/CudaInterop.cs
  - DataWarehouse.SDK/Hardware/Accelerators/RocmInterop.cs
autonomous: true

must_haves:
  truths:
    - "GpuAccelerator implements IGpuAccelerator and IHardwareAccelerator"
    - "GpuAccelerator detects CUDA (NVIDIA) or ROCm (AMD) runtime at initialization"
    - "VectorMultiplyAsync, MatrixMultiplyAsync work when GPU is available"
    - "IsAvailable returns true when CUDA or ROCm detected, false otherwise"
    - "Runtime property returns Cuda, RoCm, or None based on detected runtime"
    - "DeviceCount property returns number of available GPU devices"
    - "Platform capability registry registers 'gpu', 'gpu.cuda' or 'gpu.rocm' based on detected runtime"
  artifacts:
    - path: "DataWarehouse.SDK/Hardware/Accelerators/GpuAccelerator.cs"
      provides: "IGpuAccelerator implementation with CUDA/ROCm detection"
      min_lines: 250
    - path: "DataWarehouse.SDK/Hardware/Accelerators/CudaInterop.cs"
      provides: "P/Invoke declarations for CUDA runtime"
      min_lines: 60
    - path: "DataWarehouse.SDK/Hardware/Accelerators/RocmInterop.cs"
      provides: "P/Invoke declarations for ROCm runtime"
      min_lines: 60
  key_links:
    - from: "GpuAccelerator"
      to: "IPlatformCapabilityRegistry"
      via: "Registers 'gpu.cuda' or 'gpu.rocm' capability when detected"
      pattern: "registry\\.RegisterCapability.*gpu"
---

<objective>
Create IGpuAccelerator implementation (HW-02) for CUDA (NVIDIA) and ROCm (AMD) GPU acceleration of parallel operations (hash computation, encryption of large blocks, compression of independent chunks).

Purpose: Offload data-parallel operations to GPU for >5x-10x throughput improvement when GPU hardware is available. Supports both NVIDIA CUDA and AMD ROCm runtimes.

Output:
- `DataWarehouse.SDK/Hardware/Accelerators/GpuAccelerator.cs` — IGpuAccelerator implementation
- `DataWarehouse.SDK/Hardware/Accelerators/CudaInterop.cs` — CUDA P/Invoke
- `DataWarehouse.SDK/Hardware/Accelerators/RocmInterop.cs` — ROCm P/Invoke
</objective>

<execution_context>
@C:/Users/ddamien/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/ddamien/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/ROADMAP.md
@.planning/REQUIREMENTS-v3.md
@.planning/phases/35-hardware-accelerator-hypervisor/35-RESEARCH.md

@DataWarehouse.SDK/Hardware/IHardwareAcceleration.cs
@DataWarehouse.SDK/Hardware/IPlatformCapabilityRegistry.cs
</context>

<tasks>

<task type="auto">
  <name>Task 1: CUDA Interop</name>
  <files>
    DataWarehouse.SDK/Hardware/Accelerators/CudaInterop.cs
  </files>
  <action>
Create `DataWarehouse.SDK/Hardware/Accelerators/CudaInterop.cs` in namespace `DataWarehouse.SDK.Hardware.Accelerators`.

Define internal static class `CudaInterop` with minimal CUDA Runtime API surface:

**Constants**:
```csharp
private const string CudaLibrary = "cudart64_12"; // CUDA 12.x on Windows
// Linux: "libcudart.so.12"
internal const int CUDA_SUCCESS = 0;
```

**Enums**:
```csharp
internal enum cudaError
{
    cudaSuccess = 0,
    cudaErrorInvalidValue = 1,
    cudaErrorMemoryAllocation = 2,
    // ... (minimal set)
}
```

**Functions**:
```csharp
[LibraryImport(CudaLibrary, EntryPoint = "cudaGetDeviceCount")]
internal static partial int GetDeviceCount(out int count);

[LibraryImport(CudaLibrary, EntryPoint = "cudaSetDevice")]
internal static partial int SetDevice(int device);

[LibraryImport(CudaLibrary, EntryPoint = "cudaMalloc")]
internal static partial int Malloc(out IntPtr devPtr, nuint size);

[LibraryImport(CudaLibrary, EntryPoint = "cudaMemcpy")]
internal static partial int Memcpy(IntPtr dst, IntPtr src, nuint count, int kind);

[LibraryImport(CudaLibrary, EntryPoint = "cudaFree")]
internal static partial int Free(IntPtr devPtr);

[LibraryImport(CudaLibrary, EntryPoint = "cudaDeviceSynchronize")]
internal static partial int DeviceSynchronize();
```

**MemcpyKind constants**:
```csharp
internal const int cudaMemcpyHostToDevice = 1;
internal const int cudaMemcpyDeviceToHost = 2;
```

Add XML comments explaining this is a minimal CUDA surface for GPU acceleration. Full CUDA API is much larger.

Add `[SdkCompatibility("3.0.0", Notes = "Phase 35: CUDA interop (HW-02)")]` attribute.
  </action>
  <verify>
Build succeeds. File exists. Grep for `LibraryImport.*cuda` confirms declarations.
  </verify>
  <done>
CudaInterop.cs exists with minimal CUDA Runtime API declarations.
  </done>
</task>

<task type="auto">
  <name>Task 2: ROCm Interop</name>
  <files>
    DataWarehouse.SDK/Hardware/Accelerators/RocmInterop.cs
  </files>
  <action>
Create `DataWarehouse.SDK/Hardware/Accelerators/RocmInterop.cs` in namespace `DataWarehouse.SDK.Hardware.Accelerators`.

Define internal static class `RocmInterop` with minimal HIP Runtime API surface (ROCm's CUDA-compatible API):

**Constants**:
```csharp
private const string HipLibrary = "amdhip64"; // Windows
// Linux: "libamdhip64.so"
internal const int hipSuccess = 0;
```

**Functions** (mirror CUDA API):
```csharp
[LibraryImport(HipLibrary, EntryPoint = "hipGetDeviceCount")]
internal static partial int GetDeviceCount(out int count);

[LibraryImport(HipLibrary, EntryPoint = "hipSetDevice")]
internal static partial int SetDevice(int device);

[LibraryImport(HipLibrary, EntryPoint = "hipMalloc")]
internal static partial int Malloc(out IntPtr devPtr, nuint size);

[LibraryImport(HipLibrary, EntryPoint = "hipMemcpy")]
internal static partial int Memcpy(IntPtr dst, IntPtr src, nuint count, int kind);

[LibraryImport(HipLibrary, EntryPoint = "hipFree")]
internal static partial int Free(IntPtr devPtr);

[LibraryImport(HipLibrary, EntryPoint = "hipDeviceSynchronize")]
internal static partial int DeviceSynchronize();
```

**MemcpyKind constants**:
```csharp
internal const int hipMemcpyHostToDevice = 1;
internal const int hipMemcpyDeviceToHost = 2;
```

Add XML comments explaining ROCm/HIP compatibility layer.

Add `[SdkCompatibility("3.0.0", Notes = "Phase 35: ROCm/HIP interop (HW-02)")]` attribute.
  </action>
  <verify>
Build succeeds. File exists. Grep for `LibraryImport.*hip` confirms declarations.
  </verify>
  <done>
RocmInterop.cs exists with minimal HIP Runtime API declarations.
  </done>
</task>

<task type="auto">
  <name>Task 3: GpuAccelerator Implementation</name>
  <files>
    DataWarehouse.SDK/Hardware/Accelerators/GpuAccelerator.cs
  </files>
  <action>
Create `DataWarehouse.SDK/Hardware/Accelerators/GpuAccelerator.cs` in namespace `DataWarehouse.SDK.Hardware.Accelerators`.

Implement `public sealed class GpuAccelerator : IGpuAccelerator, IDisposable`.

**Fields**:
```csharp
private readonly IPlatformCapabilityRegistry _registry;
private GpuRuntime _runtime = GpuRuntime.None;
private int _deviceCount = 0;
private bool _isAvailable = false;
private bool _initialized = false;
private long _operationsCompleted = 0;
private readonly object _lock = new();
```

**Constructor**:
```csharp
public GpuAccelerator(IPlatformCapabilityRegistry registry)
```

Store registry. Do NOT initialize GPU in constructor.

**Properties**:
- `public AcceleratorType Type => _runtime switch { GpuRuntime.Cuda => AcceleratorType.NvidiaGpu, GpuRuntime.RoCm => AcceleratorType.AmdGpu, _ => AcceleratorType.None };`
- `public bool IsAvailable => _isAvailable;`
- `public GpuRuntime Runtime => _runtime;`
- `public int DeviceCount => _deviceCount;`

**InitializeAsync()**:
```csharp
public async Task InitializeAsync()
{
    lock (_lock)
    {
        if (_initialized) return;

        // 1. Try to load CUDA library via NativeLibrary.TryLoad("cudart64_12")
        //    If successful:
        //      - Call CudaInterop.GetDeviceCount(out int count)
        //      - If count > 0, set _runtime = GpuRuntime.Cuda, _deviceCount = count, _isAvailable = true
        //      - Register "gpu" and "gpu.cuda" capabilities

        // 2. If CUDA load fails, try ROCm via NativeLibrary.TryLoad("amdhip64")
        //    If successful:
        //      - Call RocmInterop.GetDeviceCount(out int count)
        //      - If count > 0, set _runtime = GpuRuntime.RoCm, _deviceCount = count, _isAvailable = true
        //      - Register "gpu" and "gpu.rocm" capabilities

        // 3. If both fail, set _runtime = None, _isAvailable = false

        _initialized = true;
    }
}
```

**VectorMultiplyAsync(float[] a, float[] b)**:
```csharp
public async Task<float[]> VectorMultiplyAsync(float[] a, float[] b)
{
    if (!_isAvailable)
        throw new InvalidOperationException("GPU is not available. Check IsAvailable before calling.");

    ArgumentNullException.ThrowIfNull(a);
    ArgumentNullException.ThrowIfNull(b);

    if (a.Length != b.Length)
        throw new ArgumentException("Vectors must have the same length");

    float[] result = new float[a.Length];

    // Strategy:
    // 1. Allocate GPU memory via Malloc (CUDA or ROCm based on _runtime)
    // 2. Copy a, b to GPU via Memcpy (Host->Device)
    // 3. Launch kernel (for demo, use CPU fallback with comment "TODO: launch GPU kernel")
    // 4. Copy result from GPU via Memcpy (Device->Host)
    // 5. Free GPU memory
    // 6. Return result

    // For Phase 35, SIMPLIFY: Use CPU fallback with comment explaining kernel launch is TODO
    // Production implementation would compile CUDA/HIP kernels or use CuBLAS/rocBLAS

    await Task.Run(() =>
    {
        for (int i = 0; i < a.Length; i++)
            result[i] = a[i] * b[i];
    });

    Interlocked.Increment(ref _operationsCompleted);

    return result;
}
```

**MatrixMultiplyAsync(float[,] a, float[,] b)**:
```csharp
public async Task<float[]> MatrixMultiplyAsync(float[,] a, float[,] b)
{
    if (!_isAvailable)
        throw new InvalidOperationException("GPU is not available.");

    // Validate dimensions: a is MxK, b is KxN, result is MxN
    int M = a.GetLength(0), K = a.GetLength(1);
    int K2 = b.GetLength(0), N = b.GetLength(1);

    if (K != K2)
        throw new ArgumentException("Matrix dimensions incompatible for multiplication");

    // For Phase 35: CPU fallback with TODO comment
    // Production: use CuBLAS (cublasSgemm) or rocBLAS

    float[] result = new float[M * N];
    await Task.Run(() =>
    {
        for (int i = 0; i < M; i++)
        for (int j = 0; j < N; j++)
        {
            float sum = 0;
            for (int k = 0; k < K; k++)
                sum += a[i, k] * b[k, j];
            result[i * N + j] = sum;
        }
    });

    Interlocked.Increment(ref _operationsCompleted);

    return result;
}
```

**ComputeEmbeddingsAsync(float[] input, float[,] weights)**:
```csharp
public async Task<float[]> ComputeEmbeddingsAsync(float[] input, float[,] weights)
{
    if (!_isAvailable)
        throw new InvalidOperationException("GPU is not available.");

    // Embedding: input (1 x D), weights (D x E) -> output (1 x E)
    // This is a matrix-vector multiply

    int D = input.Length;
    int D2 = weights.GetLength(0), E = weights.GetLength(1);

    if (D != D2)
        throw new ArgumentException("Input dimension must match weight rows");

    float[] result = new float[E];

    // CPU fallback for Phase 35
    await Task.Run(() =>
    {
        for (int j = 0; j < E; j++)
        {
            float sum = 0;
            for (int i = 0; i < D; i++)
                sum += input[i] * weights[i, j];
            result[j] = sum;
        }
    });

    Interlocked.Increment(ref _operationsCompleted);

    return result;
}
```

**ProcessAsync(byte[] data, AcceleratorOperation operation)** — IHardwareAccelerator:
```csharp
public Task<byte[]> ProcessAsync(byte[] data, AcceleratorOperation operation)
{
    // GPU accelerator does not support byte[] operations directly
    // Use VectorMultiply/MatrixMultiply instead
    throw new NotSupportedException("GPU accelerator requires float[] operations. Use VectorMultiplyAsync or MatrixMultiplyAsync.");
}
```

**GetStatisticsAsync()**:
```csharp
public Task<AcceleratorStatistics> GetStatisticsAsync()
{
    return Task.FromResult(new AcceleratorStatistics(
        Type: Type,
        OperationsCompleted: Interlocked.Read(ref _operationsCompleted),
        AverageThroughputMBps: 0.0, // TODO
        CurrentUtilization: 0.0, // TODO: query GPU via nvidia-smi or rocm-smi
        TotalProcessingTime: TimeSpan.Zero
    ));
}
```

**Dispose()**: (no cleanup needed for Phase 35, library handles device cleanup)

Add comprehensive XML documentation explaining:
- Detects CUDA or ROCm runtime at initialization
- IsAvailable returns false when no GPU detected
- Runtime property reports which GPU runtime is in use
- DeviceCount reports number of available GPUs
- >5x-10x speedup for parallel operations on GPU hardware
- Transparent fallback: caller checks IsAvailable

Add `[SdkCompatibility("3.0.0", Notes = "Phase 35: GPU acceleration via CUDA/ROCm (HW-02)")]` attribute.

**TODO comments**: Clearly mark that kernel launch and CuBLAS/rocBLAS integration is future work. Phase 35 provides detection + API contract + CPU fallback.
  </action>
  <verify>
Build succeeds. File exists. Grep for `class GpuAccelerator : IGpuAccelerator`. Grep for `RegisterCapability.*gpu` confirms capability registration. Grep for `IsAvailable` confirms detection logic. Grep for `TODO` confirms kernel launch is marked as future work.
  </verify>
  <done>
GpuAccelerator exists implementing IGpuAccelerator with CUDA/ROCm detection, capability registration, and CPU fallback for operations.
  </done>
</task>

</tasks>

<verification>
1. `dotnet build DataWarehouse.SDK/DataWarehouse.SDK.csproj` compiles with zero new errors
2. Three new files exist: GpuAccelerator.cs, CudaInterop.cs, RocmInterop.cs
3. GpuAccelerator implements IGpuAccelerator and IHardwareAccelerator
4. InitializeAsync tries CUDA first, then ROCm, sets Runtime accordingly
5. IsAvailable returns true when CUDA or ROCm detected with devices
6. DeviceCount reports number of GPU devices
7. VectorMultiplyAsync, MatrixMultiplyAsync, ComputeEmbeddingsAsync all compile
8. Platform capability registry receives "gpu.cuda" or "gpu.rocm" when detected
9. Zero external dependencies (CUDA/ROCm are runtime dependencies)
10. Code builds on Windows and Linux
</verification>

<success_criteria>
- GpuAccelerator compiles and implements IGpuAccelerator
- On machine without GPU: IsAvailable = false, Runtime = None
- On machine with NVIDIA GPU + CUDA: IsAvailable = true, Runtime = Cuda
- On machine with AMD GPU + ROCm: IsAvailable = true, Runtime = RoCm
- Platform capability registry correctly reports GPU presence
- Operations use CPU fallback with TODO comments for kernel launch
- Zero new NuGet dependencies
</success_criteria>

<output>
After completion, create `.planning/phases/35-hardware-accelerator-hypervisor/35-02-SUMMARY.md`
</output>
