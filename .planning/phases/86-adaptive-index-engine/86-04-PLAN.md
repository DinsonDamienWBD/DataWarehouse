---
phase: 86-adaptive-index-engine
plan: 04
type: execute
wave: 2
depends_on: ["86-01", "86-02"]
files_modified:
  - DataWarehouse.SDK/VirtualDiskEngine/AdaptiveIndex/BeTreeForest.cs
  - DataWarehouse.SDK/VirtualDiskEngine/AdaptiveIndex/HilbertPartitioner.cs
  - DataWarehouse.SDK/VirtualDiskEngine/AdaptiveIndex/LearnedShardRouter.cs
autonomous: true
must_haves:
  truths:
    - "Forest splits automatically at configurable threshold"
    - "Each shard can be at a different morph level independently"
    - "Hilbert curve partitioning provides better locality than Z-order"
    - "Learned shard routing achieves O(1) shard selection"
  artifacts:
    - path: "DataWarehouse.SDK/VirtualDiskEngine/AdaptiveIndex/BeTreeForest.cs"
      provides: "Level 5 sharded Be-tree forest"
      exports: ["BeTreeForest"]
    - path: "DataWarehouse.SDK/VirtualDiskEngine/AdaptiveIndex/HilbertPartitioner.cs"
      provides: "Hilbert space-filling curve for shard partitioning"
      exports: ["HilbertPartitioner"]
  key_links:
    - from: "BeTreeForest"
      to: "IAdaptiveIndex (per-shard)"
      via: "each shard is independently morphing IAdaptiveIndex"
      pattern: "_shards\\[.*\\]"
---

<objective>
Implement Level 5: Sharded Be-tree Forest with Hilbert curve partitioner, learned shard routing, auto-split/merge, and per-shard independent morph level tracking.

Purpose: At very large scale (100M+ objects), a single Be-tree becomes bottlenecked. The forest shards the key space across multiple independent Be-trees, each capable of morphing independently. Hilbert partitioning provides better spatial locality than Z-order for multi-dimensional keys.
Output: 3 files implementing the sharded forest as Level 5.
</objective>

<execution_context>
@C:/Users/ddamien/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/ddamien/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/ROADMAP.md
@.planning/phases/86-adaptive-index-engine/86-01-SUMMARY.md
@.planning/phases/86-adaptive-index-engine/86-02-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Hilbert curve partitioner and learned shard router</name>
  <files>
    DataWarehouse.SDK/VirtualDiskEngine/AdaptiveIndex/HilbertPartitioner.cs
    DataWarehouse.SDK/VirtualDiskEngine/AdaptiveIndex/LearnedShardRouter.cs
  </files>
  <action>
    1. **HilbertPartitioner.cs**: Hilbert space-filling curve implementation for shard partitioning.
       - `static long KeyToHilbertValue(byte[] key, int dimensions, int bitsPerDimension)`: Converts multi-dimensional key to 1D Hilbert index. For single-dimension keys, use first N bytes as coordinates.
       - Core algorithm: iterative Hilbert curve computation using the Butz algorithm (bit-interleaving with Gray code transforms). Support 2D and 3D curves. For arbitrary dimensions, use the generalized algorithm.
       - `static (long Start, long End)[] PartitionHilbertSpace(int numShards, int bitsPerDimension)`: Divides Hilbert space into equal-sized ranges for N shards.
       - `int GetShardId(byte[] key, int numShards)`: Maps key to shard via Hilbert value and range lookup.
       - `static byte[] HilbertValueToKey(long hilbertValue, int dimensions, int bitsPerDimension)`: Inverse mapping (for range queries).
       - All methods are static and pure (no state). Add `[MethodImpl(MethodImplOptions.AggressiveInlining)]` on hot path methods.

    2. **LearnedShardRouter.cs**: CDF-based shard routing for O(1) shard selection.
       - `LearnedShardRouter(int numShards)`: Initializes with shard count.
       - `TrainFromSample(IReadOnlyList<byte[]> sampleKeys)`: Builds piecewise linear CDF from sample of keys. Segments = numShards. Each segment maps a CDF range to a shard ID.
       - `int Route(byte[] key)`: Convert key to double (first 8 bytes as big-endian ulong normalized), apply CDF model, return shard ID. O(1) amortized.
       - `double[] ShardLoadFactors`: Track per-shard object counts for imbalance detection.
       - `bool IsImbalanced(double threshold = 2.0)`: True if max/min shard load ratio exceeds threshold.
       - `void Retrain(IReadOnlyList<byte[]> sampleKeys)`: Rebuild CDF model from fresh sample.
       - Thread-safe via `volatile` model reference swap (readers see consistent snapshot, writer builds new model then assigns).
  </action>
  <verify>Build: `dotnet build DataWarehouse.SDK/DataWarehouse.SDK.csproj --no-restore` compiles with zero errors.</verify>
  <done>HilbertPartitioner converts keys to Hilbert values for locality-preserving shard assignment. LearnedShardRouter provides O(1) routing via CDF model with imbalance detection.</done>
</task>

<task type="auto">
  <name>Task 2: Be-tree Forest (Level 5) with auto-split/merge and per-shard morph</name>
  <files>
    DataWarehouse.SDK/VirtualDiskEngine/AdaptiveIndex/BeTreeForest.cs
  </files>
  <action>
    **BeTreeForest.cs**: Implements `IAdaptiveIndex` as Level 5. Manages N shards, each an independent `IAdaptiveIndex`.

    Constructor: `IBlockDevice device, IBlockAllocator allocator, IWriteAheadLog wal, int blockSize, int initialShardCount = 4, long splitThreshold = 10_000_000`.

    Structure:
    - `List<ForestShard> _shards` â€” each shard: `{ IAdaptiveIndex Index, (byte[] Min, byte[] Max) KeyRange, MorphLevel Level, long ObjectCount }`
    - `LearnedShardRouter _router`
    - `HilbertPartitioner` for key-to-shard mapping

    Operations (all route to correct shard):
    - **LookupAsync**: `_router.Route(key)` -> shard -> shard.Index.LookupAsync(key)
    - **InsertAsync**: Route to shard, insert, increment shard count. If shard count > splitThreshold, trigger auto-split.
    - **DeleteAsync**: Route to shard, delete, decrement. If shard count < mergeThreshold (splitThreshold/4), trigger merge with adjacent shard.
    - **RangeQueryAsync**: Identify all shards overlapping [startKey, endKey) via key range comparison. Query each, merge-sort results.
    - **CountAsync**: Sum of all shard counts.
    - **ObjectCount**: Sum of shard counts (cached, updated on insert/delete).
    - **CurrentLevel**: `MorphLevel.BeTreeForest`

    Auto-split:
    - Find median key in oversized shard (via range query sampling)
    - Create new shard with keys > median
    - Transfer entries from old shard to new shard
    - Update router (retrain from shard boundaries)
    - WAL-protect the split operation

    Auto-merge:
    - Merge undersized shard into adjacent shard
    - Transfer all entries
    - Remove empty shard
    - Update router

    Per-shard independent morph:
    - Each shard has its own `AdaptiveIndexEngine` internally
    - Shard A can be at MorphLevel.BeTree while shard B is at MorphLevel.SortedArray
    - Shard morph decisions are independent based on per-shard object count

    Thread safety: per-shard `SemaphoreSlim(1,1)` for mutations. Reads are concurrent. Forest-level operations (split/merge) acquire a global `ReaderWriterLockSlim`.

    Add `[SdkCompatibility("6.0.0", Notes = "Phase 86: AIE-04 Be-tree Forest")]`.
  </action>
  <verify>Build: `dotnet build DataWarehouse.SDK/DataWarehouse.SDK.csproj --no-restore` compiles with zero errors.</verify>
  <done>Be-tree Forest shards key space across N independent indexes. Auto-split on threshold, auto-merge on underflow. Each shard morphs independently. Hilbert partitioning with learned routing.</done>
</task>

</tasks>

<verification>
- All 3 files exist under `DataWarehouse.SDK/VirtualDiskEngine/AdaptiveIndex/`
- `dotnet build DataWarehouse.SDK/DataWarehouse.SDK.csproj` succeeds with zero errors
- BeTreeForest implements IAdaptiveIndex with CurrentLevel = MorphLevel.BeTreeForest
- Per-shard morph levels can differ
</verification>

<success_criteria>
- Forest automatically splits shards at configurable threshold
- Each shard independently morphs based on its own object count
- Hilbert curve provides locality-preserving shard assignment
- Learned shard router achieves O(1) routing
- Auto-merge consolidates undersized shards
- Build passes with zero errors
</success_criteria>

<output>
After completion, create `.planning/phases/86-adaptive-index-engine/86-04-SUMMARY.md`
</output>
