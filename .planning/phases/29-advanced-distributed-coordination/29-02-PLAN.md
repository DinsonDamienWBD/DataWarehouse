---
phase: 29-advanced-distributed-coordination
plan: 02
type: execute
wave: 2
depends_on: ["29-01"]
files_modified:
  - DataWarehouse.SDK/Infrastructure/Distributed/Consensus/RaftConsensusEngine.cs
  - DataWarehouse.SDK/Infrastructure/Distributed/Consensus/RaftState.cs
  - DataWarehouse.SDK/Infrastructure/Distributed/Consensus/RaftLogEntry.cs
autonomous: true

must_haves:
  truths:
    - "Nodes start as Followers with randomized election timeout"
    - "Election timeout triggers Candidate transition with vote request to all peers"
    - "Candidate receiving majority vote becomes Leader"
    - "Leader sends periodic heartbeats to maintain authority"
    - "Leader replicates log entries to followers with commit tracking"
    - "Higher term from any message causes immediate step-down to Follower"
    - "GetLeader() on SwimClusterMembership returns the Raft-elected leader"
  artifacts:
    - path: "DataWarehouse.SDK/Infrastructure/Distributed/Consensus/RaftConsensusEngine.cs"
      provides: "Raft consensus engine with leader election and log replication"
      min_lines: 350
    - path: "DataWarehouse.SDK/Infrastructure/Distributed/Consensus/RaftState.cs"
      provides: "Raft persistent and volatile state types"
      min_lines: 40
    - path: "DataWarehouse.SDK/Infrastructure/Distributed/Consensus/RaftLogEntry.cs"
      provides: "Raft replicated log entry type"
      min_lines: 20
  key_links:
    - from: "RaftConsensusEngine"
      to: "IClusterMembership"
      via: "reads membership for peer list, sets leader via internal method"
      pattern: "IClusterMembership|_membership"
    - from: "RaftConsensusEngine"
      to: "IP2PNetwork"
      via: "sends RequestVote and AppendEntries RPCs"
      pattern: "IP2PNetwork|_network|SendToPeerAsync|RequestFromPeerAsync"
    - from: "RaftConsensusEngine"
      to: "IConsensusEngine"
      via: "implements IConsensusEngine for backward compatibility"
      pattern: "IConsensusEngine.*ProposeAsync|OnCommit|IsLeader"
    - from: "RaftConsensusEngine"
      to: "RaftState"
      via: "uses persistent + volatile state for term/vote/log tracking"
      pattern: "RaftPersistentState|RaftVolatileState"
---

<objective>
Implement Raft consensus for leader election (DIST-13) as an SDK-level class that integrates with SWIM membership.

Purpose: Provide exactly-one-leader guarantee in multi-node clusters with automatic re-election on leader failure. The Raft engine also implements `IConsensusEngine` for backward compatibility, allowing plugins that use the existing consensus contract to benefit from the new implementation.

Output:
- `RaftConsensusEngine` implementing `IConsensusEngine` with full Raft leader election and log replication
- `RaftPersistentState` and `RaftVolatileState` for term/vote/log tracking
- `RaftLogEntry` for replicated log entries
</objective>

<execution_context>
@C:/Users/ddamien/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/ddamien/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/29-advanced-distributed-coordination/29-RESEARCH.md
@.planning/phases/29-advanced-distributed-coordination/29-01-SUMMARY.md

@DataWarehouse.SDK/Contracts/Distributed/IClusterMembership.cs
@DataWarehouse.SDK/Contracts/Distributed/IP2PNetwork.cs
@DataWarehouse.SDK/Contracts/IConsensusEngine.cs
@DataWarehouse.SDK/Infrastructure/Distributed/Membership/SwimClusterMembership.cs
</context>

<tasks>

<task type="auto">
  <name>Task 1: Raft State Types and Log Entry</name>
  <files>
    DataWarehouse.SDK/Infrastructure/Distributed/Consensus/RaftState.cs
    DataWarehouse.SDK/Infrastructure/Distributed/Consensus/RaftLogEntry.cs
  </files>
  <action>
Create the `Distributed/Consensus/` directory under `DataWarehouse.SDK/Infrastructure/`.

**File 1: RaftState.cs**

Namespace: `DataWarehouse.SDK.Infrastructure.Distributed`

Create `RaftPersistentState` internal sealed class (Raft paper Section 5.2 persistent state):
- `long CurrentTerm` (get; set) -- latest term server has seen (init 0)
- `string? VotedFor` (get; set) -- candidateId that received vote in current term (null if none)
- `List<RaftLogEntry> Log` (get) = new() -- log entries

Create `RaftVolatileState` internal sealed class (Raft paper Section 5.2 volatile state):
- `long CommitIndex` (get; set) -- highest log entry known to be committed (init 0)
- `long LastApplied` (get; set) -- highest log entry applied to state machine (init 0)
- Leader-only state:
  - `ConcurrentDictionary<string, long> NextIndex` (get) = new() -- for each server, index of next log entry to send
  - `ConcurrentDictionary<string, long> MatchIndex` (get) = new() -- for each server, highest log entry known to be replicated

Create `RaftRole` internal enum: Follower, Candidate, Leader (separate from ClusterNodeRole to keep Raft internals clean)

Create `RaftConfiguration` record:
- `int ElectionTimeoutMinMs` (default 150) -- min election timeout
- `int ElectionTimeoutMaxMs` (default 300) -- max election timeout (randomized between min/max)
- `int HeartbeatIntervalMs` (default 50) -- leader heartbeat interval
- `int MaxLogEntries` (default 10000) -- bounded log size

Create `RaftMessageType` internal enum: RequestVote, RequestVoteResponse, AppendEntries, AppendEntriesResponse

Create `RaftMessage` internal sealed class (serializable via STJ):
- `RaftMessageType Type`
- `long Term`
- `string SenderId`
- For RequestVote: `string CandidateId`, `long LastLogIndex`, `long LastLogTerm`
- For RequestVoteResponse: `bool VoteGranted`, `string Reason`
- For AppendEntries: `long PrevLogIndex`, `long PrevLogTerm`, `List<RaftLogEntry>? Entries`, `long LeaderCommit`
- For AppendEntriesResponse: `bool Success`, `long MatchIndex`

**File 2: RaftLogEntry.cs**

Namespace: `DataWarehouse.SDK.Infrastructure.Distributed`

Create `RaftLogEntry` internal sealed class:
- `long Index` (get; set) -- position in the log
- `long Term` (get; set) -- term when entry was received by leader
- `string Command` (get; set) -- command name
- `byte[] Payload` (get; set) -- command data
- `DateTimeOffset Timestamp` (get; set) -- when entry was created

IMPORTANT: Do NOT reference existing `GeoRaftState`, `GeoRaftNode`, `RequestVoteMessage` types from SDK/Contracts/IConsensusEngine.cs. Create clean Phase 29 types as the research recommends. The existing types are v1.0 legacy with different semantics.
  </action>
  <verify>
Run `dotnet build DataWarehouse.SDK/DataWarehouse.SDK.csproj` -- zero new errors. Verify RaftState.cs and RaftLogEntry.cs exist. Grep confirms RaftPersistentState has CurrentTerm, VotedFor, Log. Grep confirms RaftLogEntry has Index, Term, Command, Payload.
  </verify>
  <done>
Raft state types defined: RaftPersistentState (term, votedFor, log), RaftVolatileState (commitIndex, lastApplied, nextIndex, matchIndex), RaftLogEntry (index, term, command, payload). Configuration type with randomized election timeout range. Message types for RequestVote and AppendEntries RPCs.
  </done>
</task>

<task type="auto">
  <name>Task 2: Raft Consensus Engine with Leader Election and Log Replication</name>
  <files>
    DataWarehouse.SDK/Infrastructure/Distributed/Consensus/RaftConsensusEngine.cs
  </files>
  <action>
**File: RaftConsensusEngine.cs**

Create `RaftConsensusEngine` sealed class implementing `IConsensusEngine`:
- Namespace: `DataWarehouse.SDK.Infrastructure.Distributed`
- Attribute: `[SdkCompatibility("2.0.0", Notes = "Phase 29: Raft consensus leader election")]`

Since `IConsensusEngine : IPlugin`, the class must satisfy IPlugin contract. It does NOT need to be a full plugin -- implement the minimum IPlugin surface:
- `string Name => "RaftConsensusEngine"`
- `string Version => "2.0.0"`
- Implement `IsLeader` property from IConsensusEngine
- Implement `ProposeAsync(Proposal)` -- append to log and replicate
- Implement `OnCommit(Action<Proposal>)` -- register commit handler

Constructor takes:
- `IClusterMembership membership` -- for peer discovery and leader reporting
- `IP2PNetwork network` -- for RPC communication
- `RaftConfiguration? config = null`

Internal state:
- `RaftPersistentState _persistent = new()`
- `RaftVolatileState _volatile = new()`
- `RaftRole _role = RaftRole.Follower`
- `string? _leaderId = null`
- `SemaphoreSlim _stateLock = new(1, 1)` -- all state mutations under lock
- `CancellationTokenSource _cts` -- for stopping background loops
- `DateTimeOffset _lastHeartbeatReceived` -- for election timeout detection
- `List<Action<Proposal>> _commitHandlers = new()`

**StartAsync(CancellationToken ct)** method to begin Raft operation:
1. Subscribe to `_network` for incoming Raft messages (OnPeerEvent or poll)
2. Start election timeout loop
3. Subscribe to `_membership.OnMembershipChanged` for membership-aware Raft

**Election Timeout Loop:**
- Compute random timeout: `RandomNumberGenerator.GetInt32(config.ElectionTimeoutMinMs, config.ElectionTimeoutMaxMs)` -- MUST be randomized (Pitfall 1)
- Use a loop that checks `(DateTimeOffset.UtcNow - _lastHeartbeatReceived).TotalMilliseconds > randomTimeout`
- When timeout fires AND role is Follower/Candidate: start election

**StartElection:**
1. Acquire _stateLock
2. Transition to Candidate
3. Increment `_persistent.CurrentTerm`
4. Vote for self: `_persistent.VotedFor = selfId`
5. Reset election timer
6. Release lock
7. Send RequestVote to all peers (from `_membership.GetMembers()` excluding self)
8. Collect responses with timeout
9. If majority (> members.Count / 2) grant vote: become Leader
10. If response contains term > currentTerm: step down to Follower

**BecomeLeader:**
1. Set `_role = RaftRole.Leader`, `_leaderId = selfId`
2. Initialize `NextIndex[peer] = lastLogIndex + 1` and `MatchIndex[peer] = 0` for all peers
3. Report leader to membership: cast `_membership` to `SwimClusterMembership` and call `SetLeader(selfId)` (or use internal interface)
4. Fire `OnMembershipChanged(LeaderChanged)`
5. Start heartbeat loop

**Heartbeat Loop (Leader only):**
- Use `PeriodicTimer(TimeSpan.FromMilliseconds(config.HeartbeatIntervalMs))`
- Send AppendEntries (empty if no new entries, or with pending log entries) to all peers
- On AppendEntries response: update MatchIndex, advance CommitIndex if majority replicated
- Apply committed entries to state machine (invoke _commitHandlers)
- Stop loop when no longer Leader

**HandleRequestVote (Section 5.2 of Raft paper):**
- If request.Term > currentTerm: update term, step down to Follower, clear votedFor
- Grant vote if: term matches, haven't voted (or voted for this candidate), candidate's log is at least as up-to-date
- Log up-to-date check: candidate's lastLogTerm > our lastLogTerm, OR same term and lastLogIndex >= our lastLogIndex
- If granting: set votedFor, reset election timer

**HandleAppendEntries (Section 5.3):**
- If request.Term > currentTerm: update term, step down to Follower
- If request.Term < currentTerm: reject (return false)
- Reset election timer (leader is alive)
- Set `_leaderId = request.SenderId`
- Log consistency check: if log doesn't contain entry at PrevLogIndex with PrevLogTerm, reject
- If existing entry conflicts with new one (same index, different term): delete it and all following
- Append new entries not already in the log
- If LeaderCommit > commitIndex: set commitIndex = min(LeaderCommit, index of last new entry)
- Apply newly committed entries to state machine

**ProposeAsync(Proposal):**
- If not Leader: throw InvalidOperationException("Not the leader")
- Create RaftLogEntry from Proposal, append to log
- Replicate via AppendEntries to all peers
- Wait until committed (majority have replicated) or timeout
- Return true if committed, false otherwise

**OnCommit(Action<Proposal> handler):**
- Add handler to `_commitHandlers` list

Message routing:
- Serialize RaftMessage to JSON byte[] via System.Text.Json
- Send via `_network.SendToPeerAsync` / `_network.RequestFromPeerAsync`
- Receive via `_network` subscription, deserialize, dispatch to HandleRequestVote or HandleAppendEntries based on Type field

Thread safety: All state reads/writes via `_stateLock`. Use `Task.WhenAny` for timeouts on vote collection. Implement IDisposable to cancel CTS, dispose semaphore.

Bound the log to `config.MaxLogEntries` -- when exceeded, compact oldest committed entries (keep last 1000).

IMPORTANT: Handle the IPlugin interface requirements minimally. RaftConsensusEngine is NOT a typical plugin -- it does not go through kernel loading. Stub the IPlugin lifecycle methods (Initialize/Execute/Shutdown return Task.CompletedTask). The essential contract is IConsensusEngine (IsLeader, ProposeAsync, OnCommit).
  </action>
  <verify>
Run `dotnet build DataWarehouse.SDK/DataWarehouse.SDK.csproj` -- zero new errors. Verify RaftConsensusEngine.cs exists. Grep for `IConsensusEngine` confirms interface implementation. Grep for `RandomNumberGenerator.GetInt32` confirms randomized election timeout. Grep for `PeriodicTimer` confirms heartbeat loop. Grep for `ProposeAsync` confirms proposal handling. Grep for `SemaphoreSlim` confirms thread-safe state access.
  </verify>
  <done>
RaftConsensusEngine implements IConsensusEngine with full Raft leader election (RequestVote), log replication (AppendEntries), and heartbeat mechanism. Election timeout is randomized per node. Higher terms cause immediate step-down. Leader reports itself to SwimClusterMembership. Log is bounded. All state access is thread-safe via SemaphoreSlim.
  </done>
</task>

</tasks>

<verification>
1. `dotnet build DataWarehouse.SDK/DataWarehouse.SDK.csproj` compiles with zero new errors
2. Three new files exist: RaftConsensusEngine.cs, RaftState.cs, RaftLogEntry.cs
3. RaftConsensusEngine implements IConsensusEngine (IsLeader, ProposeAsync, OnCommit)
4. Election timeout uses RandomNumberGenerator.GetInt32 (randomized, CSPRNG)
5. Leader heartbeat uses PeriodicTimer with configurable interval
6. All state mutations under SemaphoreSlim lock
7. Log bounded to configurable max entries
8. System.Text.Json used for RPC message serialization
</verification>

<success_criteria>
- Raft election: Followers timeout, become Candidates, request votes, majority wins
- Exactly one leader: higher term causes step-down, split votes retry with new random timeout
- Log replication: Leader sends AppendEntries, tracks MatchIndex, commits when majority replicated
- Integration: GetLeader() on cluster membership returns the Raft-elected leader
- IConsensusEngine backward compatibility: ProposeAsync and OnCommit work for existing plugin consumers
- Build passes with zero errors
</success_criteria>

<output>
After completion, create `.planning/phases/29-advanced-distributed-coordination/29-02-SUMMARY.md`
</output>
