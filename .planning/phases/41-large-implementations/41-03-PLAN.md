---
phase: 41-large-implementations
plan: 03
type: execute
wave: 1
depends_on: []
files_modified:
  - Plugins/DataWarehouse.Plugins.UltimateEdgeComputing/Strategies/FederatedLearning/FederatedLearningModels.cs
  - Plugins/DataWarehouse.Plugins.UltimateEdgeComputing/Strategies/FederatedLearning/ModelDistributor.cs
  - Plugins/DataWarehouse.Plugins.UltimateEdgeComputing/Strategies/FederatedLearning/LocalTrainingCoordinator.cs
  - Plugins/DataWarehouse.Plugins.UltimateEdgeComputing/Strategies/FederatedLearning/GradientAggregator.cs
  - Plugins/DataWarehouse.Plugins.UltimateEdgeComputing/Strategies/FederatedLearning/ConvergenceDetector.cs
  - Plugins/DataWarehouse.Plugins.UltimateEdgeComputing/Strategies/FederatedLearning/DifferentialPrivacyIntegration.cs
  - Plugins/DataWarehouse.Plugins.UltimateEdgeComputing/Strategies/FederatedLearning/FederatedLearningOrchestrator.cs
  - Plugins/DataWarehouse.Plugins.UltimateEdgeComputing/UltimateEdgeComputingPlugin.cs
autonomous: true

must_haves:
  truths:
    - "FederatedLearningOrchestrator distributes model weights to edge nodes without sending raw data"
    - "FedAvg aggregation correctly weighted-averages gradients by sample count"
    - "FedSGD aggregation simple-averages per-sample gradients"
    - "ConvergenceDetector stops training when loss delta < threshold for N consecutive rounds"
    - "Differential privacy adds calibrated Gaussian noise to gradients before aggregation"
    - "No raw training data leaves edge nodes -- only gradients are transmitted"
    - "Orchestrator handles stragglers with configurable timeout and minimum participation"
  artifacts:
    - path: "Plugins/DataWarehouse.Plugins.UltimateEdgeComputing/Strategies/FederatedLearning/FederatedLearningOrchestrator.cs"
      provides: "Central FL orchestrator managing rounds, distribution, aggregation, convergence"
      min_lines: 200
    - path: "Plugins/DataWarehouse.Plugins.UltimateEdgeComputing/Strategies/FederatedLearning/GradientAggregator.cs"
      provides: "FedAvg and FedSGD gradient aggregation with real weighted averaging math"
      min_lines: 100
    - path: "Plugins/DataWarehouse.Plugins.UltimateEdgeComputing/Strategies/FederatedLearning/ModelDistributor.cs"
      provides: "Model weight serialization and distribution to edge nodes"
      min_lines: 80
    - path: "Plugins/DataWarehouse.Plugins.UltimateEdgeComputing/Strategies/FederatedLearning/LocalTrainingCoordinator.cs"
      provides: "Edge-side training coordination with epoch management and gradient computation"
      min_lines: 100
    - path: "Plugins/DataWarehouse.Plugins.UltimateEdgeComputing/Strategies/FederatedLearning/ConvergenceDetector.cs"
      provides: "Global loss tracking with early stopping and patience mechanism"
      min_lines: 60
    - path: "Plugins/DataWarehouse.Plugins.UltimateEdgeComputing/Strategies/FederatedLearning/DifferentialPrivacyIntegration.cs"
      provides: "Gaussian noise mechanism for gradient privacy with epsilon budget tracking"
      min_lines: 80
  key_links:
    - from: "FederatedLearningOrchestrator"
      to: "ModelDistributor"
      via: "Orchestrator distributes model to nodes at start of each round"
      pattern: "_modelDistributor\\.Distribute|ModelDistributor"
    - from: "FederatedLearningOrchestrator"
      to: "GradientAggregator"
      via: "Orchestrator aggregates received gradients at end of each round"
      pattern: "_aggregator\\.Aggregate|GradientAggregator"
    - from: "FederatedLearningOrchestrator"
      to: "ConvergenceDetector"
      via: "Orchestrator checks convergence after each aggregation round"
      pattern: "_convergenceDetector\\.Check|HasConverged"
    - from: "GradientAggregator"
      to: "DifferentialPrivacyIntegration"
      via: "Noise added to gradients before aggregation for privacy"
      pattern: "_privacyModule\\.AddNoise|DifferentialPrivacy"
    - from: "UltimateEdgeComputingPlugin"
      to: "FederatedLearningOrchestrator"
      via: "Plugin exposes FL orchestrator via message bus commands"
      pattern: "FederatedLearningOrchestrator|federated"
---

<objective>
Build a production-ready Federated Learning orchestrator with model distribution, local training coordination, gradient aggregation (FedAvg/FedSGD), convergence detection, and differential privacy integration. Wire it into UltimateEdgeComputingPlugin.

Purpose: IMPL-09 requires a complete FL system. The codebase declares `SupportsFederatedLearning = true` in 7 files but has ZERO FL implementation (no FedAvg, no gradient aggregation, no model distribution). This plan builds the entire FL subsystem from scratch, ensuring no raw data ever leaves edge nodes.

Output: A FederatedLearningOrchestrator that manages multi-round FL training across edge nodes, with FedAvg and FedSGD aggregation strategies, differential privacy noise, and convergence detection. Integrated into UltimateEdgeComputingPlugin via message bus commands.
</objective>

<execution_context>
@C:/Users/ddamien/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/ddamien/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/REQUIREMENTS-v3.md
@.planning/phases/41-large-implementations/41-RESEARCH.md
@Plugins/DataWarehouse.Plugins.UltimateEdgeComputing/Strategies/SpecializedStrategies.cs
@Plugins/DataWarehouse.Plugins.UltimateEdgeComputing/UltimateEdgeComputingPlugin.cs
@DataWarehouse.SDK/Contracts/EdgeComputing/IEdgeComputingStrategy.cs
</context>

<tasks>

<task type="auto">
  <name>Task 1: Build Federated Learning Core (Models, Aggregation, Distribution, Privacy)</name>
  <files>
    Plugins/DataWarehouse.Plugins.UltimateEdgeComputing/Strategies/FederatedLearning/FederatedLearningModels.cs
    Plugins/DataWarehouse.Plugins.UltimateEdgeComputing/Strategies/FederatedLearning/ModelDistributor.cs
    Plugins/DataWarehouse.Plugins.UltimateEdgeComputing/Strategies/FederatedLearning/LocalTrainingCoordinator.cs
    Plugins/DataWarehouse.Plugins.UltimateEdgeComputing/Strategies/FederatedLearning/GradientAggregator.cs
    Plugins/DataWarehouse.Plugins.UltimateEdgeComputing/Strategies/FederatedLearning/ConvergenceDetector.cs
    Plugins/DataWarehouse.Plugins.UltimateEdgeComputing/Strategies/FederatedLearning/DifferentialPrivacyIntegration.cs
  </files>
  <action>
    Create `Plugins/DataWarehouse.Plugins.UltimateEdgeComputing/Strategies/FederatedLearning/` directory with all FL components. All classes in namespace `DataWarehouse.Plugins.UltimateEdgeComputing.Strategies.FederatedLearning`.

    **FederatedLearningModels.cs** (~120 lines):
    - `public sealed record ModelWeights(Dictionary<string, float[]> Layers, int Version, DateTime CreatedAt);`
      - Layers maps layer name (e.g., "dense_1.weights", "dense_1.bias") to weight array.
    - `public sealed record GradientUpdate(string NodeId, Dictionary<string, float[]> Gradients, int SampleCount, double LocalLoss, double LocalAccuracy, int Version);`
    - `public sealed record TrainingRound(int RoundNumber, int Version, ModelWeights GlobalModel, DateTime StartedAt, DateTime? CompletedAt, int ParticipatingNodes, double? GlobalLoss, double? GlobalAccuracy, AggregationResult? Result);`
    - `public sealed record AggregationResult(ModelWeights UpdatedModel, double WeightedLoss, double WeightedAccuracy, int ContributingNodes, int TotalSamples);`
    - `public sealed record FederatedLearningConfig(AggregationMethod Method = AggregationMethod.FedAvg, int MinParticipationNodes = 1, double MinParticipationRatio = 0.5, TimeSpan RoundTimeout = default, int MaxRounds = 100, int LocalEpochs = 5, double LearningRate = 0.01, bool EnableDifferentialPrivacy = false, double PrivacyEpsilon = 1.0, double PrivacyDelta = 1e-5, double ConvergenceThreshold = 0.001, int ConvergencePatience = 5);`
      - Default RoundTimeout in constructor body: `RoundTimeout == default ? TimeSpan.FromMinutes(5) : RoundTimeout`.
    - `public enum AggregationMethod { FedAvg, FedSGD }`
    - `public sealed record NodeTrainingStatus(string NodeId, NodeTrainingState State, DateTime LastUpdate, double? Progress);`
    - `public enum NodeTrainingState { Idle, ReceivingModel, Training, UploadingGradients, Completed, TimedOut, Failed }`

    **ModelDistributor.cs** (~100 lines):
    - `public sealed class ModelDistributor`.
    - `byte[] SerializeModel(ModelWeights model)`:
      - Serialize to binary format: [4-byte version][4-byte layer count][for each layer: 4-byte name length, name UTF8 bytes, 4-byte weight count, float[] weights as raw bytes].
      - Use `BinaryWriter` for efficiency.
    - `ModelWeights DeserializeModel(byte[] data)`:
      - Reverse of serialize. Use `BinaryReader`.
    - `Task<Dictionary<string, bool>> DistributeToNodesAsync(ModelWeights model, IReadOnlyList<string> nodeIds, Func<string, byte[], CancellationToken, Task<bool>> sendFunc, CancellationToken ct)`:
      - Serialize model once.
      - Send to each node via provided sendFunc (abstraction for message bus).
      - Track which nodes acknowledged receipt.
      - Return dictionary of nodeId -> success.
    - `ModelWeights CreateInitialModel(Dictionary<string, int> layerSizes)`:
      - Create random initial weights using `System.Security.Cryptography.RandomNumberGenerator` for seeding (CRYPTO-02 compliance), then `System.Random` with that seed for weight initialization.
      - Xavier/Glorot initialization: `weight = Random.NextDouble() * sqrt(2.0 / (fan_in + fan_out)) * 2 - sqrt(2.0 / (fan_in + fan_out))`.

    **LocalTrainingCoordinator.cs** (~120 lines):
    - `public sealed class LocalTrainingCoordinator`.
    - Represents edge-side training logic. In a real deployment, this runs on the edge node. In our implementation, it's a coordinator that can be called to simulate/manage local training.
    - `Task<GradientUpdate> TrainLocallyAsync(ModelWeights globalModel, float[][] localData, float[][] localLabels, FederatedLearningConfig config, CancellationToken ct)`:
      - Perform `config.LocalEpochs` epochs of mini-batch SGD on local data.
      - Mini-batch SGD loop:
        - Forward pass: `output = input * weights + bias` (simplified dense layer).
        - Loss: MSE = `mean((output - label)^2)`.
        - Backward pass: `gradient = 2 * (output - label) * input / batchSize`.
        - Weight update: `weight -= learningRate * gradient`.
      - Compute gradients as difference: `gradient_layer[i] = updated_weights[i] - original_weights[i]` (this is the FedAvg approach where "gradients" are actually weight deltas).
      - Return `GradientUpdate` with gradients, sample count, local loss, local accuracy.
    - `double ComputeLoss(ModelWeights model, float[][] data, float[][] labels)`:
      - Evaluate model on data, return MSE loss.
    - This is a simplified neural network for demonstration. Real models would use ONNX Runtime (Phase 36 WASI-NN). The math must still be real -- actual forward/backward passes, actual gradient computation. NOT a stub.

    **GradientAggregator.cs** (~130 lines):
    - `public sealed class GradientAggregator`.
    - `AggregationResult AggregateFedAvg(ModelWeights currentModel, IReadOnlyList<GradientUpdate> updates)`:
      - FedAvg: weighted average of weight deltas by sample count.
      - For each layer: `aggregated[i] = sum(n_k * delta_k[i]) / sum(n_k)` where n_k is sample count from node k.
      - Apply aggregated deltas to current model: `new_weight[i] = current_weight[i] + aggregated_delta[i]`.
      - Compute weighted loss: `sum(n_k * loss_k) / sum(n_k)`.
      - Compute weighted accuracy similarly.
      - Return new `ModelWeights` with incremented version.
    - `AggregationResult AggregateFedSGD(ModelWeights currentModel, IReadOnlyList<GradientUpdate> updates, double learningRate)`:
      - FedSGD: simple average of per-sample gradients, then apply with learning rate.
      - For each layer: `avg_gradient[i] = sum(gradient_k[i]) / num_nodes`.
      - Apply: `new_weight[i] = current_weight[i] - learningRate * avg_gradient[i]`.
      - Note: FedSGD uses actual gradients (not weight deltas), so the sign convention differs from FedAvg.
    - `AggregationResult Aggregate(ModelWeights currentModel, IReadOnlyList<GradientUpdate> updates, FederatedLearningConfig config)`:
      - Route to FedAvg or FedSGD based on config.Method.
    - Validate all updates have matching layer structure (same layer names, same dimensions). Throw `InvalidOperationException` if mismatch.

    **ConvergenceDetector.cs** (~80 lines):
    - `public sealed class ConvergenceDetector`.
    - Constructor takes `double threshold, int patience`.
    - Private state: `List<double> _lossHistory`, `int _stagnantRounds`.
    - `bool CheckConvergence(double currentLoss)`:
      - Add to history.
      - If history.Count < 2, return false (need at least 2 data points).
      - Compute delta: `Math.Abs(previousLoss - currentLoss)`.
      - If delta < threshold, increment `_stagnantRounds`.
      - If delta >= threshold, reset `_stagnantRounds = 0`.
      - Return `_stagnantRounds >= patience`.
    - `bool HasDiverged(double currentLoss, double divergenceThreshold = 10.0)`:
      - Return true if currentLoss > initialLoss * divergenceThreshold (training is diverging).
    - `ConvergenceReport GetReport()`:
      - Return `ConvergenceReport` with loss history, convergence status, rounds completed, best loss, trend.
    - `public sealed record ConvergenceReport(IReadOnlyList<double> LossHistory, bool IsConverged, bool IsDiverging, int RoundsCompleted, double BestLoss, double CurrentLoss);`
    - `void Reset()` -- clear history.

    **DifferentialPrivacyIntegration.cs** (~100 lines):
    - `public sealed class DifferentialPrivacyIntegration`.
    - Constructor takes `double epsilon, double delta, double? clipNorm = null`.
    - `Dictionary<string, float[]> AddNoise(Dictionary<string, float[]> gradients)`:
      - Step 1: Gradient clipping -- if `clipNorm` set, clip each layer's gradient norm to `clipNorm` via: `scale = min(1, clipNorm / norm)`, `clipped = gradient * scale`. L2 norm: `sqrt(sum(g_i^2))`.
      - Step 2: Add calibrated Gaussian noise to each gradient value.
      - Noise scale (sigma): `sigma = clipNorm * sqrt(2 * ln(1.25 / delta)) / epsilon` (Gaussian mechanism from Dwork & Roth).
      - For each value: `noised = value + Normal(0, sigma)`.
      - Use `System.Security.Cryptography.RandomNumberGenerator` to generate uniform random bytes, then Box-Muller transform for Gaussian: `z = sqrt(-2 * ln(u1)) * cos(2 * pi * u2)`.
    - `double TrackBudget(int round)`:
      - Track cumulative epsilon consumption using simple composition: `total_epsilon = round * epsilon`.
      - Advanced composition (Moments Accountant) would give tighter bounds but is complex; use simple composition with a note that Moments Accountant would be a future improvement.
      - Return remaining budget.
    - `bool IsBudgetExhausted(int round, double maxEpsilon)`:
      - Return `round * epsilon > maxEpsilon`.
    - All random number generation uses `RandomNumberGenerator` (CRYPTO-02 compliance), NOT `System.Random`.

    CRITICAL (Rule 13): Every class must contain real algorithmic logic. FedAvg must do actual weighted averaging of float arrays. Gaussian noise must use real Box-Muller transform. Gradient clipping must compute real L2 norms. No stubs, no placeholders, no `throw new NotImplementedException()`.

    CRITICAL: No external NuGet dependencies for FL core. Use only .NET BCL. Model serialization uses BinaryWriter/BinaryReader. Random uses RandomNumberGenerator + Box-Muller.
  </action>
  <verify>
    `dotnet build Plugins/DataWarehouse.Plugins.UltimateEdgeComputing/DataWarehouse.Plugins.UltimateEdgeComputing.csproj` compiles with zero new errors. Verify all 6 files exist in FederatedLearning directory. Grep for `NotImplementedException` in FederatedLearning directory -- zero matches. Grep for `sum(n_k` or similar weighted average pattern in GradientAggregator.cs -- must find real aggregation logic (comments or code).
  </verify>
  <done>
    All 6 FL core components implemented with real algorithms: ModelDistributor (binary serialization + Xavier init), LocalTrainingCoordinator (real mini-batch SGD with forward/backward passes), GradientAggregator (FedAvg weighted average + FedSGD with learning rate), ConvergenceDetector (loss delta tracking with patience), DifferentialPrivacyIntegration (Gaussian mechanism with gradient clipping and Box-Muller noise).
  </done>
</task>

<task type="auto">
  <name>Task 2: Build FederatedLearningOrchestrator and Integrate into Plugin</name>
  <files>
    Plugins/DataWarehouse.Plugins.UltimateEdgeComputing/Strategies/FederatedLearning/FederatedLearningOrchestrator.cs
    Plugins/DataWarehouse.Plugins.UltimateEdgeComputing/UltimateEdgeComputingPlugin.cs
  </files>
  <action>
    **FederatedLearningOrchestrator.cs** (~250 lines):
    - `public sealed class FederatedLearningOrchestrator`.
    - Composes ModelDistributor, GradientAggregator, ConvergenceDetector, DifferentialPrivacyIntegration.
    - Constructor takes `FederatedLearningConfig config, IMessageBus? messageBus = null`.
    - Private state: `ModelWeights _globalModel`, `List<TrainingRound> _rounds`, `int _currentRound`, `bool _isTraining`.

    - `Task<ModelWeights> InitializeModelAsync(Dictionary<string, int> layerSizes)`:
      - Create initial model via ModelDistributor.CreateInitialModel.
      - Set as `_globalModel`.

    - `Task<TrainingRound> RunRoundAsync(IReadOnlyList<string> nodeIds, Func<string, byte[], CancellationToken, Task<bool>> sendFunc, Func<string, TimeSpan, CancellationToken, Task<GradientUpdate?>> receiveFunc, CancellationToken ct)`:
      - This is the core round logic:
      - Step 1: Distribute current global model to all nodes via `_modelDistributor.DistributeToNodesAsync`.
      - Step 2: Wait for gradient updates from nodes:
        - Use `receiveFunc` to collect updates with timeout (`config.RoundTimeout`).
        - Track which nodes responded.
        - If node count < `config.MinParticipationNodes` or ratio < `config.MinParticipationRatio`, abort round.
      - Step 3: Apply differential privacy if enabled:
        - For each received gradient update, call `_privacyModule.AddNoise(update.Gradients)`.
        - Check budget exhaustion.
      - Step 4: Aggregate gradients via `_aggregator.Aggregate(_globalModel, updates, config)`.
      - Step 5: Update global model with aggregation result.
      - Step 6: Check convergence via `_convergenceDetector.CheckConvergence(result.WeightedLoss)`.
      - Step 7: Record round in `_rounds` list.
      - Step 8: Publish round summary to message bus if available (`edge.fl.round.completed`).
      - Return `TrainingRound` record.

    - `async Task<IReadOnlyList<TrainingRound>> TrainAsync(IReadOnlyList<string> nodeIds, Func<string, byte[], CancellationToken, Task<bool>> sendFunc, Func<string, TimeSpan, CancellationToken, Task<GradientUpdate?>> receiveFunc, CancellationToken ct)`:
      - Run rounds until convergence or max rounds reached.
      - Call `RunRoundAsync` in a loop.
      - Break on convergence, divergence, budget exhaustion, or cancellation.
      - Return all completed rounds.

    - `ModelWeights GetGlobalModel()` -- return current global model.
    - `IReadOnlyList<TrainingRound> GetHistory()` -- return all rounds.
    - `ConvergenceReport GetConvergenceReport()` -- delegate to detector.
    - `bool IsConverged` -- check convergence detector.
    - `void Reset()` -- reset all state for new training session.

    **UltimateEdgeComputingPlugin.cs modification:**
    Read the existing file first. Add FL integration without breaking existing functionality.

    1. Add private field: `private FederatedLearningOrchestrator? _flOrchestrator;`
    2. In the plugin's initialization or handshake, create the orchestrator with default config.
    3. Add message handler cases for FL commands in `OnMessageAsync`:
       - `"edge.fl.init"`: Initialize model with layer sizes from message payload. Create `_flOrchestrator` with config from payload.
       - `"edge.fl.round"`: Execute one training round. Uses message bus for distribution/collection.
       - `"edge.fl.status"`: Return current training status (round number, convergence report, global loss).
       - `"edge.fl.model"`: Return current global model weights.
    4. Ensure the `SupportsFederatedLearning = true` capability declaration now has backing implementation.

    IMPORTANT: Read UltimateEdgeComputingPlugin.cs first to understand its current structure (OnMessageAsync, OnHandshakeAsync, etc.). Only ADD the FL message handlers -- do NOT modify or break existing message handlers.

    The FL orchestrator uses `Func` delegates for send/receive, making it testable and transport-agnostic. The plugin wires these to message bus calls. This keeps the orchestrator independent of the specific transport mechanism.
  </action>
  <verify>
    `dotnet build Plugins/DataWarehouse.Plugins.UltimateEdgeComputing/DataWarehouse.Plugins.UltimateEdgeComputing.csproj` compiles with zero new errors. Grep for `FederatedLearningOrchestrator` in UltimateEdgeComputingPlugin.cs -- must find references. Grep for `"edge.fl.` in UltimateEdgeComputingPlugin.cs -- must find message handler cases. Verify FederatedLearningOrchestrator.cs exists with >200 lines.
  </verify>
  <done>
    FederatedLearningOrchestrator orchestrates complete FL training lifecycle: model initialization, multi-round training with configurable participation, FedAvg/FedSGD aggregation, differential privacy noise, convergence detection with patience. UltimateEdgeComputingPlugin exposes FL via message bus commands (edge.fl.init, edge.fl.round, edge.fl.status, edge.fl.model). The SupportsFederatedLearning capability now has real backing implementation. No raw data leaves edge nodes -- only gradient updates are transmitted.
  </done>
</task>

</tasks>

<verification>
1. `dotnet build Plugins/DataWarehouse.Plugins.UltimateEdgeComputing/DataWarehouse.Plugins.UltimateEdgeComputing.csproj` -- zero new errors
2. Grep all files in `FederatedLearning/` for `NotImplementedException` -- zero matches
3. Grep `GradientAggregator.cs` for `SampleCount` -- confirms real weighted averaging logic
4. Grep `DifferentialPrivacyIntegration.cs` for `RandomNumberGenerator` -- confirms CRYPTO-02 compliant noise
5. Grep `FederatedLearningOrchestrator.cs` for `RunRoundAsync` -- confirms round orchestration exists
6. File count in `FederatedLearning/` directory -- exactly 7 files
7. Total new LOC -- approximately 900-1,300 lines of real FL code
</verification>

<success_criteria>
- FederatedLearningOrchestrator manages complete FL training lifecycle
- FedAvg: correct weighted averaging of gradients by sample count (w_global = sum(n_k * w_k) / sum(n_k))
- FedSGD: correct simple averaging with learning rate application
- Differential privacy: Gaussian noise with calibrated sigma, gradient clipping, budget tracking
- Convergence detection: loss delta threshold with configurable patience
- Model distribution: binary serialization/deserialization of model weights
- Local training: real mini-batch SGD with forward/backward passes (simplified dense network)
- No raw data leaves edge nodes -- only GradientUpdate objects transmitted
- Straggler handling: configurable timeout and minimum participation ratio
- UltimateEdgeComputingPlugin wired with message bus commands (edge.fl.*)
- Zero external NuGet dependencies
- Zero stubs (Rule 13)
</success_criteria>

<output>
After completion, create `.planning/phases/41-large-implementations/41-03-SUMMARY.md`
</output>
