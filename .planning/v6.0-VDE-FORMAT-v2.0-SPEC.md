# DWVD v2.0 — Complete Virtual Disk Engine Format Specification

**Date:** 2026-02-20
**Status:** Draft (pre-milestone, design phase)
**Authored by:** Architecture research agent, based on full SDK codebase analysis

---

## Current Format Limitations (v1.0)

From `ContainerFormat.cs` (lines 19-87), current layout:
```
Block 0:   Primary Superblock (512 bytes)
Block 1:   Mirror Superblock
Block 2+:  Bitmap → Inode Table → WAL → Checksum Table → B-Tree Root → Data Region
```

Critical limitations identified:
1. Superblock 512 bytes, only 13 fields — no room for encryption, RAID, replication, policy
2. Inode 256 bytes with only 13 bytes reserved — no inline tags, encryption keys, compliance
3. Single flat data region — no sequential/random separation, no hot/cold, no WORM/mutable
4. No encryption metadata in format (IKeyStore exists but no on-disk anchoring)
5. No tag co-location (tags in overflow blocks = extra I/O)
6. No replication metadata (DottedVersionVector has no persistence model)
7. No RAID metadata (TamperProofManifest stores RaidRecord in separate JSON)
8. No compression awareness (no per-block flags or skip-decompression hints)
9. No policy enforcement at format level
10. Checksum table only 8-byte xxHash64 — no algorithm agility, no Merkle tree

---

## DWVD v2.0 Master Layout

### Design Principles
- Block-aligned everything (4 KiB default, max 64 KiB)
- Superblock group: 4 blocks (16 KiB metadata capacity)
- Metadata regions clustered at beginning for sequential prefetch
- Dual WAL: metadata WAL + data WAL for reduced contention
- Self-describing blocks: 16-byte trailer on EVERY block (type, generation, checksum)
- All regions located via pointers (not fixed offsets) — enables online resize/defrag

### Layout Diagram

```
BLOCK 0         [Primary Superblock Group — 4 blocks]
BLOCK 4         [Mirror Superblock Group — 4 blocks]           (crash recovery)
BLOCK 8         [Region Directory — 2 blocks]                  (dynamic region map)
BLOCK 10        [Policy Vault Header — 2 blocks]               (crypto-bound policies)
BLOCK 12        [Encryption Header — 2 blocks]                 (KEK wraps, algo params)
BLOCK 14        [Allocation Bitmap]                             (1 bit per block)
BLOCK B+0       [Inode Table]                                  (512-byte inodes v2.0)
BLOCK I+0       [Tag Index Region]                             (inverted B+-tree index)
BLOCK T+0       [Metadata WAL]                                 (journal for metadata ops)
BLOCK MW+0      [Integrity Tree]                               (Merkle tree checksums)
BLOCK MT+0      [B-Tree Index Forest]                          (multiple B-trees)
BLOCK BT+0      [Snapshot Table]                               (CoW snapshot registry)
BLOCK ST+0      [Replication State Region]                     (DVV, watermarks, dirty bmp)
BLOCK RS+0      [RAID Metadata Region]                         (shard maps, parity layout)
BLOCK RM+0      [Compliance Vault]                             (embedded passports)
BLOCK CV+0      [Intelligence Cache]                           (AI classification, heat map)
BLOCK IC+0      [Streaming Append Region]                      (append-optimized ring buffer)
BLOCK SA+0      [Cross-VDE Reference Table]                    (dw:// fabric links)
BLOCK XR+0      [WORM Immutable Region]                        (write-once, no overwrite)
BLOCK WR+0      [Compute Code Cache]                           (WASM bytecode directory)
BLOCK CC+0      [Data WAL]                                     (journal for data writes)
BLOCK DW+0      [Data Region]                                  (object data blocks)
```

**20+ regions** (vs 7 in v1.0), all indirectable via Region Directory.

---

## Universal Block Trailer (16 bytes, end of EVERY block)

```
Offset  Size  Field
-16     4     BlockTypeTag      (uint32: identifies region/purpose)
-12     4     GenerationNumber  (uint32: monotonic, detects torn writes)
-8      8     XxHash64          (ulong: checksum of bytes [0..BlockSize-16])
```

Effective payload: BlockSize - 16 = 4080 bytes for non-data-region blocks. Data Region blocks use the separated trailer architecture described below and carry a full 4096B payload.

Benefits: Every block self-verifiable, offline forensic recovery without metadata, torn-write detection.

Block Type Tags: SUPB, RMAP, POLV, ENCR, BMAP, INOD, TAGI, MWAL, MTRK, BTRE, SNAP, REPL, RAID, COMP, INTE, STRE, XREF, WORM, CODE, DWAL, DATA, TRLR, FREE

### Separated Trailer Architecture (Data Region)

Data Region blocks (`DATA` type) use a separated trailer layout rather than an embedded 16-byte trailer. This architectural decision is driven by hardware alignment requirements and zero-copy I/O compatibility.

**Layout:**
- **Data blocks**: Pure 4096B payload — no embedded trailer bytes. Database pages (PostgreSQL 8KB, SQLite 4KB), filesystem sectors, and NVMe-native 4096B LBAs all map directly without padding or truncation.
- **Trailer blocks (`TRLR`)**: For every 256 consecutive data blocks (one 1MB stripe), one dedicated 4KB trailer block immediately follows and holds 256 × 16B = 4096B of trailer records (one per data block in the stripe). The interleaving pattern is therefore: [data block 0 ... data block 255][trailer block][data block 256 ... data block 511][trailer block] ...

**Overhead:** 1 trailer block per 256 data blocks = 1/257 ≈ 0.39%. Identical to the embedded trailer overhead. The total overhead budget is unchanged.

**Benefits:**
- Zero-copy `Span<T>` mapping: data blocks map directly to typed structs with no trailer offset arithmetic.
- SPDK/io_uring DMA compatibility: DMA buffers are naturally 4096B-aligned and sector-exact.
- NVMe sector alignment: 4096B LBA granularity matches without padding.
- Database page compatibility: 4096B PostgreSQL and SQLite pages fit exactly with no wasted bytes.

**Trade-off:** Individual data blocks cannot self-verify without their corresponding trailer block. Recovery tools locate trailer blocks deterministically at block offset `floor(blockIndex / 256) * 257 + 256` relative to the start of the Data Region.

**Trailer record layout** (16 bytes, stored in TRLR blocks):
```
Offset  Size  Field
0       4     DataBlockTypeTag   (always DATA = 0x44415441 for data blocks)
4       4     GenerationNumber   (uint32: monotonic, detects torn writes)
8       8     XxHash64           (ulong: checksum of the corresponding 4096B data block)
```

---

## Superblock Group (4 blocks, mirrored)

### Block 0 — Primary Superblock (4080 usable bytes)
~50+ fields including: Magic, FormatVersion, BlockSize, TotalBlocks, FreeBlocks, VolumeUUID, ClusterNodeId, DefaultCompressionAlgo, DefaultEncryptionAlgo, DefaultChecksumAlgo, InodeSize(512), PolicyVersion, ReplicationEpoch, WormHighWaterMark, EncryptionKeyFingerprint, SovereigntyZoneId, FeatureFlags, VolumeLabel, LastScrubTimestamp, ErrorMapBlockCount

### Block 1 — Region Pointer Table
127 region pointer slots x 32 bytes per slot: `RegionTypeId(4), Flags(2), ShardId(2), StartBlock(8), BlockCount(8), UsedBlocks(8)` — 32 bytes total per slot.

`ShardId` replaces 2 bytes previously allocated as padding within the Flags field. Multiple slots may share the same `RegionTypeId` with different `ShardId` values — for example, a multi-shard WAL uses MWAL shard 0, MWAL shard 1, ..., MWAL shard N, each as a separate Region Pointer Table entry. Slot 64 may optionally contain an indirect pointer to an overflow Region Directory block (block index stored in `StartBlock`, `BlockCount = 1`), allowing more than 64 region entries and supporting up to 256 WAL shards on high-core-count machines. The overflow block uses the same 32B slot format and is identified by the reserved `RegionTypeId = 0xFFFF0000` (RDIR_OVERFLOW).

### Block 2 — Extended Metadata
DottedVersionVector(256B), SovereigntyZoneConfig(128B), RAIDLayoutSummary(128B), StreamingConfig(128B), FabricNamespaceRoot(256B), TierPolicyDigest(128B), AiMetadataSummary(128B), BillingMeterSnapshot(128B)

### Block 3 — Integrity Anchor
MerkleRootHash(32B), PolicyVaultHash(32B), InodeTableHash(32B), TagIndexHash(32B), HashChainCounter, HashChainHead(512-bit), BlockchainAnchorTxId(512-bit), SBOMDigest

### Feature Flags (bitfield)
ENCRYPTION_ENABLED, COMPRESSION_ENABLED, WORM_REGION_ACTIVE, RAID_ENABLED, REPLICATION_ENABLED, STREAMING_REGION_ACTIVE, POLICY_ENGINE_ACTIVE, TAMPERPROOF_CHAIN_ACTIVE, COMPUTE_CACHE_ACTIVE, COMPLIANCE_VAULT_ACTIVE, INTELLIGENCE_CACHE_ACTIVE, TAG_INDEX_ACTIVE, FUSE_COMPAT_MODE, AIRGAP_MODE, DIRTY,
COMPUTE_PUSHDOWN_ACTIVE, VOLATILE_KEYRING_ACTIVE, WAL_STREAMING_ACTIVE, DELTA_EXTENTS_ACTIVE, ZNS_AWARE, SPATIOTEMPORAL_ACTIVE,
SEMANTIC_DEDUP_ACTIVE (v7.0 reserve), ZKP_COMPLIANCE_ACTIVE (v7.0 reserve), HYPERSCALE_INODES,
GDPR_TOMBSTONES_ACTIVE, SEMANTIC_WEAR_LEVELING_ACTIVE, QUORUM_SEALED_ACTIVE
(Note: STEG is v7.0 reserve; bit 31 is expansion flag to 64-bit ModuleManifest)

---

## Inode v2.0 (512 bytes, up from 256)

```
Offset  Size   Field
0       8      InodeNumber
8       1      Type (File/Dir/SymLink)
9       1      InodeFlags (ENCRYPTED, COMPRESSED, WORM, INLINE_DATA)
10      2      Permissions (POSIX rwxrwxrwx)
12      4      LinkCount
16-31   16     OwnerId + GroupId
32-79   48     Size, AllocatedSize, timestamps (Created/Modified/Accessed/Changed)

--- Extent-based addressing (replaces 12 direct block pointers) ---
80-279  200    ExtentCount(4) + 6 extents x 32 bytes + 8 bytes spare
               Each: [StartBlock:8][BlockCount:4][Flags:4][ExpectedHash:16]
               Flags: COMPRESSED, PRECOMPRESSED, ENCRYPTED, HOLE(sparse), SHARED_COW,
                       MIRROR, EC_2_1, EC_4_2, EC_8_3 (RAID topology, 3 bits),
                       DELTA (sub-block delta extent), SPATIOTEMPORAL (4D extent reinterpretation)
               Bit 9:  TOMBSTONE       — Extent is a GDPR tombstone (ExpectedHash holds deletion proof)
               Bit 10: TTL_HINT        — Extent has temperature classification for wear-leveling
               Bit 11: QUORUM_SEALED   — Inode carries FROST threshold signature requiring quorum verification
               ExpectedHash: truncated BLAKE3 (first 16 bytes) of the extent's data blocks.
               Enables inline dedup detection (compare hashes before reading) and
               extent-level integrity verification without consulting the Integrity Tree.

--- Overflow pointers ---
280-303 24     IndirectExtentBlock, DoubleIndirectBlock, ExtendedAttributeBlock

--- Inline Tags (co-located, single I/O for 80% of tagged objects) ---
304-439 136    InlineTagCount(4) + TagOverflowBlock(4) + InlineTagArea(128)
               Up to ~4 compact tags: [NamespaceHash:4][NameHash:4][ValueType:1][ValueLen:1][Value:<=22B]

--- Security ---
440-463 24     EncryptionKeySlot(4), AclPolicyId(4), ContentHash(16, truncated SHA-256)

--- Compliance & Intelligence ---
464-475 12     CompliancePassportSlot(4), ClassificationLabel(4), SovereigntyZone(2), RetentionPolicyId(2)

--- Replication ---
476-483 8      ReplicationGeneration(4), DirtyFlag(4)

--- RAID ---
484-487 4      RaidShardId(2), RaidGroupId(2)

--- Streaming ---
488-495 8      StreamSequenceNumber

--- Format-Level Data OS Modules (composable, zero when inactive) ---
496-511 16     DataOsModuleArea — Packs up to 2 active modules from:
               EKEY(32B): [EphemeralKeyID:16][TTL_Epoch:8][KeyRingSlot:4][Flags:4]
               CPSH(48B): [WasmPredicateOffset:8][PredicateLen:4][PredicateFlags:4][InlinePredicate:32]
               DELT(8B):  [MaxDeltaDepth:2][CurrentDepth:2][CompactionPolicy:4]
               STEX(6B):  [CoordinateSystem:2][Precision:2][HilbertOrder:2]
               RAID(32B): [Scheme:1][DataShards:1][ParityShards:1][DeviceMap:29] (extends base 4B RAID)
               SDUP(266B): [EmbeddingDim:2][ModelID:4][Threshold:4][Embedding:256] (v7.0, overflow block)
               ZKPA(322B): [SchemeID:2][CircuitHash:32][Proof:288] (v7.0, overflow block)
               Modules >16B use IndirectExtentBlock overflow. When no Data OS modules active: SymLinkTarget
```

**Key improvements over v1.0:**
- Extent-based: 6 extents × 32B = 192B, each carrying a 16B ExpectedHash (truncated BLAKE3) for inline dedup detection and extent-level integrity without Integrity Tree lookup (vs 12 pointers = 48 KiB max inline in v1.0)
- Inline tags: 128 bytes for ~4 tags, eliminates overflow I/O for 80% of objects
- Encryption key slot: index into Encryption Header (supports rotation without inode rewrite)
- Content hash per-extent: enables dedup detection and quick integrity check at extent granularity
- Compliance/classification inline: no external service call needed
- Per-extent compression flags: PRECOMPRESSED skips re-compression for JPEG/MP4/etc.

---

## Specialized Regions

### Encryption Header (2 blocks)
KeySlotCount, ActiveKeySlot, KdfAlgorithm (PBKDF2/Argon2id/HKDF), MasterCipherAlgo, up to 63 key slots x 64 bytes (SlotId, Status, CreatedUtc, ExpiresUtc, WrappedKey, KeyFingerprint). Supports seamless key rotation.

### Integrity Tree (Merkle) — INTG Level 2+ Only
Allocated only at INTG Level 2 and above. At Level 0-1, integrity is handled by block trailers (XxHash64) or authenticated index pointers (hash-in-B-Tree-pointer, ZFS-style) with zero dedicated region overhead.

When allocated (Level 2+): Leaf: 1 BLAKE3 hash per data block. Internal: 128 hashes per block. Enables O(log N) subset verification, air-gap integrity proofs, tamper-proof chain anchoring. ~4x space vs flat table but dramatically superior capabilities. At Level 2, updates are epoch-batched (background thread, configurable interval) to avoid synchronous Merkle root bottleneck at high IOPS.

### Tag Index Region
B+-tree of [NamespaceHash:NameHash] → [InodeNumber:ValueHash]. Plus bloom filter for probabilistic negative lookups. Enables O(log N) tag queries vs O(N) inode scan.

### Replication State Region
DVV snapshot(256B), per-peer watermarks (64B each), dirty bitmap (1 bit per data block). Enables delta replication — only transmit dirty blocks.

### RAID Metadata Region
RAID level, shard counts, stripe width, erasure coding params, shard map with per-shard location/status/rebuild progress. Self-describing layout enables autonomous rebuild.

### Streaming Append Region
Ring buffer design: head/tail pointers, partition support, sequence counter. Append-only, sequential writes, zero fragmentation. Physically separate from random-access data region.

### WORM Immutable Region
Append-only with high-water mark. Block allocator NEVER frees WORM blocks until RetentionExpiry. Physical immutability (not just a flag). Works in air-gapped environments without cloud services.

### Compliance Vault
Serialized CompliancePassport records indexed by inode.CompliancePassportSlot. Includes digital signatures for format-level enforcement.

### Intelligence Cache
Per-inode classification, confidence score, heat score, predicted tier. Avoids re-running ML inference. Drives tiering decisions from on-disk metadata.

### Cross-VDE Reference Table
dw:// fabric links: LocalInode → (RemoteVolumeUUID, RemoteInode, FabricAddress). Enables cross-VDE foreign-key relationships.

### Compute Code Cache
WASM module directory: CodeHash, LinkedInode, runtime type. Enables fast dispatch for self-emulating objects.

---

## Feature Integration Matrix

| Region | Policy | Encrypt | RAID | Repl | TmprPrf | Intel | Tags | Comply | Stream | FUSE | CoW | Compute | AirGap | Fabric |
|--------|:------:|:-------:|:----:|:----:|:-------:|:-----:|:----:|:------:|:------:|:----:|:---:|:-------:|:------:|:------:|
| Superblock Group | Y | Y | Y | Y | Y | Y | Y | Y | Y | Y | Y | Y | Y | Y |
| Policy Vault | **P** | Y | | | Y | | Y | Y | | | | | Y | |
| Encryption Hdr | | **P** | | | Y | | | | | | | | Y | |
| Inode Table | Y | Y | Y | Y | Y | Y | **P** | Y | Y | **P** | Y | Y | Y | Y |
| Tag Index | | | | | | Y | **P** | | | | | | | |
| Integrity Tree | | | | | **P** | | | | | | | | **P** | |
| Snapshot Table | | | | Y | | | | | | | **P** | | | |
| Replication State | | | | **P** | | | | | | | | | | Y |
| RAID Metadata | | | **P** | | | | | | | | | | | |
| Compliance Vault | | | | | | | | **P** | | | | | Y | |
| Intelligence Cache | | | | | | **P** | Y | | | | | | | |
| Streaming Append | | | | Y | | | | | **P** | | | | | |
| Cross-VDE Ref | | | | Y | | | | | | | | | | **P** |
| WORM Region | | Y | | | **P** | | | Y | | | | | Y | |
| Compute Cache | | | | | | | | | | | | **P** | | |

**P** = Primary region. Y = participates.

---

## Overhead Analysis

| VDE Size | Total Blocks | v1.0 Overhead | v2.0 Overhead | v2.0 % | Usable Data |
|----------|-------------|---------------|---------------|--------|-------------|
| 1 MB | 256 | ~23% | ~39% | 39% | 624 KiB |
| 1 GB | 262,144 | ~1.6% | ~3.3% | 3.3% | 990 MiB |
| 1 TB | 268,435,456 | ~1.4% | ~2.8% | 2.8% | 996 GiB |
| 1 PB | 274,877,906,944 | ~1.4% | ~2.8% | 2.8% | 1019 TiB |

Overhead doubles from ~1.4% to ~2.8% at scale. Largest contributors: Data WAL (1%), Streaming Append (1%, configurable). Both configurable — setting streaming to 0% reduces to ~1.8%. Comparable to ZFS (3-5%) and Btrfs (3-4%).

### 1 TB Breakdown
| Region | Size | % |
|--------|------|---|
| Superblock Groups | 32 KiB | ~0% |
| Policy + Encryption | 16 KiB | ~0% |
| Allocation Bitmap | 32 MiB | 0.003% |
| Inode Table | 8 MiB | 0.001% |
| Tag Index | 2 MiB | 0.0002% |
| Metadata WAL | 5.1 GiB | 0.5% |
| Integrity Tree (Merkle, Level 2+ only) | 8.1 GiB | 0.79% |
| Streaming Append | 10.2 GiB | 1.0% |
| Data WAL | 10.2 GiB | 1.0% |
| Block Trailers | 4.0 GiB | 0.39% |
| Other (snapshot, repl, RAID, etc.) | ~35 MiB | ~0.003% |
| **TOTAL OVERHEAD** | **~28.6 GiB** | **2.8%** |

---

## Key Architectural Decisions

1. **Separated block trailers** — 0.39% cost, stored in dedicated TRLR blocks (1 per 256 data blocks). Data blocks are pure 4096B payload for zero-copy/SPDK/NVMe alignment
2. **Fixed 512B inode stride** — recovery tools can stride-scan raw disk without metadata. Composable modules packed inside fixed envelope. 1024B hyperscale tier optional
3. **32B extent pointers with ExpectedHash** — 6 extents × 32B with inline truncated BLAKE3 for dedup detection and extent-level integrity
4. **Inline tag storage** — converts 2+ I/O to 1 I/O for 80% of tagged objects
5. **Thread-affinity sharded WAL** — N WAL shards for N CPU cores, lock-free ring buffers, io_uring 1:1 mapping. Commit barrier on Shard 0
6. **6-level Adaptive Integrity Engine** — L0 trailers, L1 hash-in-index-pointer (ZFS-style), L2 epoch-batched Merkle, L3 learned scrubbing, L4 blockchain-anchored, L5 Merkle-CRDT (v7.0)
7. **Multi-entry Region Directory** — ShardId enables WAL/region sharding across CPU cores. Slot 64 indirect overflow for >64 regions
8. **WORM as physical region** — allocator-level immutability, not just a flag
9. **Region indirection** — pointer table enables online defrag, resize, feature toggling
10. **Streaming ring buffer** — zero fragmentation for append-only workloads
11. **Epoch-based vacuum** — MVCC dead version GC with reader epoch leases, SLA timeouts, WORM exemption
12. **Smart Extents (WASM pushdown)** — eBPF-free predicate pushdown via WASM bytecode anchored in inode. io_uring filtered reads on host, NVMe-side execution on computational drives. Zero overhead when off
13. **Cryptographic Ephemerality** — per-inode ephemeral keys with TTL_Epoch. Key drop = O(1) mathematical destruction of arbitrary data volumes. Volatile key ring in TPM/RAM, never persisted to VDE blocks
14. **WAL as pub/sub** — Thread-sharded WAL exposed to user-space subscribers via mmap/Span<T>. SubscriberCursor table enables zero-copy CDC. Eliminates Kafka/RabbitMQ for local microservices
15. **Polymorphic RAID** — per-inode erasure coding via extent flag topology bits. Temp files get no redundancy, financial ledgers get EC_4_2, on the same physical NVMe. No whole-disk RAID tax
16. **Sub-block delta extents** — binary patching (VCDIFF) instead of full 4KB CoW for <10% modifications. Write amplification → near zero. Background Vacuum compacts delta chains exceeding MaxDeltaDepth
17. **Epoch-to-ZNS symbiosis** — MVCC epochs mapped 1:1 to ZNS physical zones. Dead epoch = single ZNS_ZONE_RESET. Eliminates SSD garbage collection, 300% lifespan increase, 500μs→15μs write latency
18. **4D spatiotemporal extents** — Geohash+time extent addressing for IoT/LiDAR/drone telemetry. Hilbert curve spatial clustering enables DMA-level bounding box queries. 6×32B → 3×64B reinterpretation
19. **v7.0 format reservations** — Semantic dedup (SDUP) and zk-SNARK compliance (ZKPA) module slots reserved in ModuleManifest. Binary layout defined, activation gated behind v7.0 feature flags
20. **GDPR Tombstone Provable Erasure** — When Background Vacuum executes compliance hard-delete: (1) overwrite block with cryptographic zeros, (2) compute BLAKE3(zeros || secure_timestamp), (3) store proof hash in ExpectedHash:16, (4) flag extent as TOMBSTONE. Auditors verify deletion by re-computing the hash. Complements EKEY (fast crypto-shred) with auditable proof. DELT delta chains MUST be flattened before tombstoning.
21. **Semantic Wear-Leveling Gate** — SWLV activates ONLY when ZNS_AWARE is NOT set. On ZNS devices, epoch-based zone allocation (ZNSM) is strictly superior. SWLV segregates writes into AllocationGroups by Expected_TTL hint (Hot/Warm/Cold/Frozen). Background Vacuum reclaims entire temperature groups, eliminating write amplification on conventional NVMe. 2-bit temperature class packed into extent TTL_HINT flag.
22. **Quorum-Sealed Extent Verification** — Federated writes requiring Byzantine fault tolerance use FROST threshold signatures (already built in Phase 1). 79-byte overflow inode: [QuorumScheme:1][Threshold:1][TotalSigners:1][SignerBitmap:4][AggregateSignature:64][Nonce:8]. Requires allocation-free FROST verifier in VDE hot path (NOT BouncyCastle). QuorumDegradePolicy: REJECT / ACCEPT_UNSIGNED / QUEUE_FOR_SEALING. ReplicationGeneration:4 included in signed message to prevent replay.
23. **Forensic Necromancy (Runtime Tool)** — No format changes needed. Stride-scans TRLR region at deterministic 256-block intervals, reconstructs file ordering from GenerationNumber + BlockTypeTag. Implemented as SDK.VirtualDiskEngine.Recovery.ForensicNecromancer. Can resurrect entire VDE contents if physical blocks not overwritten.
24. **Temporal Point Queries (Runtime)** — No format changes needed. Exploits GenerationNumber in every TRLR record. Epoch-indexed extent resolver walks TRLR backward from CurrentEpoch to target epoch, reconstructing historical extent map. Enables "what did this file look like at epoch N?" queries.
25. **Metadata-Only Cold Analytics (Runtime)** — No format changes needed. Separated TRLR architecture (0.39% of data region) + 512B inodes + Tag Index enable full inventory analytics without touching data blocks. 16K inodes = 8MB scan.
26. **Content-Addressable Extent Dedup (Runtime)** — No format changes needed. Uses existing ExpectedHash:16 for O(1) dedup candidate detection. Matching extents verified via data read, then deduplicated via SHARED_COW flag + SNAP refcount. Background scanner, no application awareness needed.
27. **Instant Clone via Metadata-Only Copy (Runtime)** — No format changes needed. Allocate inode, memcpy 512B, set SHARED_COW on all extent pointers, increment SNAP refcounts. 1TB file with 1000 extents = 32KB metadata writes. Requires SNAP module active.
28. **Probabilistic Corruption Radar (Runtime)** — No format changes needed. Statistical sampling of TRLR blocks + corresponding data blocks. 100 random TRLR blocks covers 25,600 data blocks. 400 TRLR blocks = 99.99% detection probability for 0.001% corruption rate.
29. **Epoch-Gated Lazy Deletion (Runtime)** — No format changes needed. Advance OldestActiveEpoch to logically delete all pre-epoch data. Background Vacuum reclaims lazily via TRLR scan. Converts millions of individual deletes into single superblock write for time-series workloads.
30. **Cross-Extent Integrity Chain (Runtime)** — No format changes needed. Three-level hash chain: XxHash64 per block (trailer) → BLAKE3 per extent (extent pointer) → SHA-256 per file (inode ContentHash). Detects block-swap attacks at INTG Level 0-1 without Merkle Tree region.
31. **Heat-Driven Allocation Group Tiering (Runtime)** — No format changes needed. Region Directory already supports multiple Data Region shards via ShardId. Hot shard on NVMe, cold shard on HDD. Background migration based on Intelligence HeatScore. Requires INTL module active.
32. **Extent-Level Integrity Caching (Runtime)** — No format changes needed. In-memory cache: {ExtentStartBlock → (LastVerifiedEpoch, ExpectedHash)}. If max GenerationNumber in extent ≤ LastVerifiedEpoch, skip per-block XxHash64 verification. Converts O(N) per-block checks into O(1) TRLR read.
33. **Proof of Physical Custody (Runtime)** — No format changes needed. At mount time, VDE engine queries TPM Platform Configuration Registers (PCR), XORs with MerkleRootHash from Superblock Block 3. ExpectedHash:16 validations fail on any other physical machine. Gated by Policy Vault flag `HARDWARE_BINDING_ACTIVE` (not a ModuleManifest bit). Leverages existing EKEY `Tpm2Provider` for PCR operations. Caveats: TPM PCR replay with physical access, VM migration requires vTPM PCR migration, backup needs escrow recovery key.
34. **O(log N) Time-Travel Bisection (Optimization of TPQR)** — Enhancement to AD-24 (Temporal Point Queries). Instead of backward TRLR walk (O(N)), binary search on ExpectedHash across MVCC epochs yields O(log N). For 10M epochs, finds exact tampered transaction in ~23 lookups without reading data blocks. Requires sorted epoch-to-hash index built at mount (10M epochs × 24B = 240MB). The bisection operates on Inode version chains, not on the AIE index — immune to AIE morphing.
35. **Entropy-Triggered Panic Fork (Runtime)** — No format changes needed. Shannon entropy already computed per-block by UltimateCompression for compressibility decisions. If sustained WAL write burst exceeds entropy threshold (>0.99 for N consecutive blocks), triggers Panic Fork: (1) create immutable snapshot via SNAP module, (2) mark pre-current-epoch extents SHARED_COW, (3) freeze OldestActiveEpoch to prevent vacuum reclamation, (4) sever write access to historical data. Configurable threshold to avoid false positives from legitimate high-entropy writes (encrypted uploads, compressed archives).
36. **Cross-Tenant Convergent Encryption (Extension of CAED)** — Enhancement to AD-26 (Content-Addressable Extent Dedup). For multi-tenant VDEs, uses hash of plaintext as encryption key — same file from different tenants produces identical ciphertext and identical ExpectedHash:16. Enables cross-tenant dedup via existing SHARED_COW + SNAP refcounting. Security caveat: confirmation-of-file attack possible; mitigated via server-aided Message-Locked Encryption (Bellare 2013). Pure convergent mode available as opt-in for non-sensitive data.
37. **Radioactive Parity Decay (Extension of Polymorphic RAID)** — Enhancement to AD-19 (Polymorphic RAID). Background Vacuum policy degrades RAID level of cold extents (Epoch Delta > configurable threshold). Flips extent RAID topology bits (EC_4_2 → Standard), frees parity blocks to Allocation Bitmap. Hard constraints: WORM region data NEVER eligible, compliance-labeled inodes exempt, minimum 2 replicas before any parity discard. Gated by explicit Policy Vault opt-in — never default-on.
38. **Ghost Enclaves — v7.0 Reserve** — Plausible deniability via duress password deriving "Chaff Key" that causes ExpectedHash failures for classified files. Deferred to v7.0 due to: (1) allocation bitmap leakage reveals hidden blocks, (2) Superblock metadata statistics mismatch detectable, (3) requires dual allocation bitmaps — reintroducing hidden volume complexity. Correct implementation needs dedicated cryptographic design review.
39. **Quine VDE — v7.0 Reserve / Separate Project** — Self-booting VDE via WASI unikernel in Block 0. Deferred: (1) magic signature conflict (DWVD vs WASM at byte 0), (2) 4080 usable bytes insufficient for bootloader, (3) UEFI requires FAT32 ESP partition, (4) embedding executable code in Block 0 creates bootkit attack surface. If desired, implement as separate UEFI application (dw-boot.efi) rather than format change.

---

## Format-Level Data OS Features (v2.1 Additions)

The DWVD v2.1 format embeds intelligence directly into the binary layout. Every feature below exploits the composable 512B inode + separated 4096B payload + feature flags architecture. When a feature is disabled, the corresponding bits are zeroed and overhead is exactly **0 bytes, 0 CPU cycles**.

These features turn DWVD from a "place to put bytes" into a **Semantic, Cryptographic, and Hardware-Symbiotic Data Operating System**.

### Feature 1: Smart Extents — Computational Storage & Predicate Pushdown

**Module:** `CPSH` (bit 19) | **Inode:** 48B overflow | **Gate:** `COMPUTE_PUSHDOWN_ACTIVE`

When the database engine writes a file, it compiles a predefined filter (e.g., `WHERE ErrorCode = 500`) into a tiny WASM bytecode snippet and stores it in the inode's CPSH module. On read, the DW Engine sends the WASM bytecode alongside extent addresses via:
- **Standard NVMe:** io_uring submits read + WASM filter sqe. Host CPU executes WASM after DMA but before user-space copy — saves memory bandwidth.
- **Computational NVMe:** (Samsung SmartSSD, etc.) Drive's embedded ARM processor executes WASM directly on flash. Only matching rows cross PCIe bus.

**Trade-off:** Off = standard DMA read, 0% overhead. On = 50GB scan returns 5MB of matching rows. Trades milliseconds of WASM execution to save gigabytes of RAM bandwidth.

**Industry comparison:** Oracle Exadata (appliance-level), AWS S3 Select (API-level). DWVD is format-level — engine-agnostic, works on any io_uring-capable host.

### Feature 2: Cryptographic Ephemerality — Zero-I/O Data Shredding

**Module:** `EKEY` (bit 20) | **Inode:** 32B overflow | **Gate:** `VOLATILE_KEYRING_ACTIVE`

Instead of the global VDE master key, a file encrypted with an ephemeral key stores `[EphemeralKeyID:16][TTL_Epoch:8][KeyRingSlot:4][Flags:4]` in its inode. The key lives in a volatile Key Management Ring (RAM-only or TPM-sealed — NEVER persisted to VDE blocks).

When `TTL_Epoch` expires, the Background Vacuum does NOT delete the file or overwrite data. It deletes the ephemeral key. Forensic recovery is mathematically impossible the millisecond the key drops. The vacuum reclaims blocks at leisure.

**Trade-off:** Off = standard VDE master key, 0% overhead. On = instantaneous, O(1) destruction of arbitrary data volumes (exabytes) with zero disk I/O.

**Critical constraint:** Ephemeral keys MUST be in volatile ring (RAM/TPM via `Tpm2Provider`). If persisted to VDE blocks, the key is forensically recoverable — defeating the purpose.

**Industry comparison:** AWS KMS crypto-shred (service-level), Apple APFS per-file keys. DWVD adds native TTL — the filesystem self-destructs mathematically. Zero-trust data lifecycle management.

### Feature 3: Native Event Streaming — Filesystem as Kafka

**Module:** `WALS` (bit 21) | **Region:** WalSubscriberCursorTable | **Gate:** `WAL_STREAMING_ACTIVE`

The Thread-Affinity Sharded WALs (LMAX Disruptor ring buffers) are already the filesystem's intent log. By adding a `SubscriberCursor` table and a `SUBSCRIBE` flag on the WAL region directory entry, external services can attach directly to the VDE via memory-mapping (`mmap` / `Span<T>`) and read WAL shards as a zero-copy append-only event stream.

**Cursor format:** `[SubscriberID:8][LastEpoch:8][LastSequence:8][Flags:8]` — 32B per subscriber, 128 subscribers per 4KB cursor block.

**Trade-off:** Off = WAL operates as standard crash-recovery mechanism, 0% overhead. On = eliminates Kafka/RabbitMQ for local or tightly-coupled microservices. Subscribers read mutations at zero-copy RAM speed (nanoseconds).

**Security:** WAL contains raw mutations including potential PII. The `SUBSCRIBE` flag is gated by the Policy Engine authority chain. Per-subscriber ACLs on the cursor table.

**Industry comparison:** PostgreSQL logical replication (database-level), Apache BookKeeper (distributed log). No standard filesystem exposes its WAL to user-space subscribers. DWVD is the first.

### Feature 4: Polymorphic RAID — Per-Inode Erasure Coding

**Module:** `RAID` (extends bit 5) | **Inode:** extends base 4B to 32B | **Gate:** Extent flag bits

The 32B extent pointer's `Flags:4` field carries 3 bits for topology:
```
000 = Standard (no redundancy, maximum speed)
001 = Mirror   (1:1 copy on separate device)
010 = EC_2_1   (Reed-Solomon 2+1 erasure coding)
011 = EC_4_2   (Reed-Solomon 4+2)
100 = EC_8_3   (Reed-Solomon 8+3)
101-111 = Reserved
```

A massive ML training dataset gets `Standard` (zero overhead). A financial ledger gets `EC_4_2`. Both sit side-by-side on the same physical NVMe. The RAID inode module `[Scheme:1][DataShards:1][ParityShards:1][DeviceMap:29]` provides the full erasure coding descriptor.

**Trade-off:** Off = standard extents, 0% overhead. On = CPU pays minor AVX-512 Reed-Solomon parity penalty, but saves terabytes of storage by not mirroring the entire disk array for a subset of critical files.

**Industry comparison:** Ceph (pool-level), VMware vSAN (VM-level). ZFS/hardware RAID forces redundancy at vdev/disk level. DWVD pushes Ceph's distributed logic down to per-file granularity in a local format.

### Feature 5: Sub-Block Delta Extents — Filesystem Rsync

**Module:** `DELT` (bit 22) | **Inode:** 8B | **Gate:** `DELTA_EXTENTS_ACTIVE`

When a write modifies <10% of a block, the filesystem generates a binary patch (VCDIFF/bsdiff) instead of allocating a new 4KB block. The delta is stored in a `DELTA`-flagged extent: `[BaseExtentPtr:32][PatchOffset:4][PatchLength:4][PatchData:inline_or_block]`.

The inode tracks `[MaxDeltaDepth:2][CurrentDepth:2][CompactionPolicy:4]`. When `CurrentDepth > MaxDeltaDepth` (default 8), the Background Vacuum flattens the delta chain by applying all patches to a fresh base block.

**Trade-off:** Off = standard CoW block replacement, 0% overhead. On = read performance degrades slightly (CPU applies patch in RAM, ~nanoseconds with AVX-512), but write IOPS skyrocket and write amplification drops to near zero. Massive win for snapshot-heavy systems, VM hypervisors, version-controlled data.

**Industry comparison:** Git packfiles (application-level). ZFS/Btrfs do full 4KB CoW — no sub-block deltas. DWVD exploits the modern hardware inversion: CPU is nanoseconds, NVMe flash wear is the real enemy.

### Feature 6: Latent-Space Semantic Deduplication — The Fuzzy Block (v7.0)

**Module:** `SDUP` (bit 25, v7.0 reserve) | **Inode:** 266B overflow | **Gate:** `SEMANTIC_DEDUP_ACTIVE`

When writing unstructured data (video, audio, telemetry), a WASM autoencoder generates a latent embedding stored in the inode's `SemanticHash`. If two files have >99.9% cosine similarity, the filesystem stores a Latent Delta Extent (vector difference) instead of full data blocks.

**Format reservation only in v6.0.** Prerequisite: Phase 93 NativeHnswVectorStore. Risks: model upgrade invalidates hashes (requires ModelVersion), false positive = silent data loss (conservative 0.999 threshold), embedding may leak data (encrypt semantic hash).

**Industry comparison:** No competitor has this. Standard dedup (ZFS/Windows Server) uses exact byte-matching — yields 0% on noisy real-world data. Semantic dedup achieves 90%+ on CCTV, medical imaging, seismic sensors.

### Feature 7: Epoch-to-ZNS Hardware Symbiosis — The Immortal Flash

**Module:** `ZNSM` (bit 23) | **Region:** ZnsZoneMapRegion | **Gate:** `ZNS_AWARE`

Maps MVCC Epochs 1:1 to physical ZNS Zones. During Epoch N, all cores write sequentially into Zone N (matching ZNS sequential-write requirement). When the Background Vacuum determines Epoch N is dead, it issues a single `ZNS_ZONE_RESET` hardware command instead of millions of per-block TRIMs.

**Zone map entry:** `[EpochID:8][ZoneID:4][State:2][Flags:2]` — 16B per entry.

Detection: VDE queries NVMe Identify Namespace at mount. If ZNS capable, activates sequential-zone allocation. If conventional NVMe, standard behavior.

**Trade-off:** Off = standard NVMe with TRIM, 0% overhead. On = NVMe never performs garbage collection. Write latency drops from 500μs to 15μs. Physical SSD lifespan increases ~300%.

**Industry comparison:** No filesystem maps MVCC epochs to ZNS zones. This is the cleanest feature — minimal format change, maximum performance win.

### Feature 8: zk-SNARK Compliance Anchors — Zero-Knowledge Filesystem (v7.0)

**Module:** `ZKPA` (bit 26, v7.0 reserve) | **Inode:** 322B overflow | **Gate:** `ZKP_COMPLIANCE_ACTIVE`

A WASM module generates a 288B Groth16 zk-SNARK proof embedded in the inode. The proof mathematically demonstrates a compliance statement (e.g., "this VDE contains no unencrypted SSNs") without exposing the underlying data.

Auditor verification: ~5ms per proof. Proof generation: seconds to minutes (writer's burden). Batch aggregation via recursive SNARKs at hyperscale.

**Format reservation only in v6.0.** Prerequisite: UltimateCompliance plugin + WASM runtime.

**Industry comparison:** No filesystem embeds ZKP. Auditors currently must scan plaintext data — which is itself a security risk. A hospital can mathematically prove HIPAA compliance on a 50TB DWVD without the regulator seeing a single byte of patient data.

### Feature 9: Spatiotemporal (4D) Extent Addressing

**Module:** `STEX` (bit 24) | **Inode:** 6B | **Gate:** `SPATIOTEMPORAL_ACTIVE`

When `IS_4D_EXTENT` is set, the inode's 6 × 32B extent slots are reinterpreted as 3 × 64B spatiotemporal extents:
```
[SpatialGeohash:16][TimeEpochStart:8][TimeEpochEnd:8][StartBlock:8][BlockCount:4][Flags:4][ExpectedHash:16]
```

16-byte Geohash = ~10cm global resolution. Hilbert curve ordering ensures spatially adjacent data lands on physically adjacent blocks, enabling NVMe scatter-gather DMA prefetch.

**Trade-off:** Off = standard 1D extents, 0% overhead. On = spatial bounding box queries resolved at DMA level. NVMe skips blocks outside the GPS coordinate/timeframe window.

**Industry comparison:** PostGIS (database-level spatial indexing). No filesystem has native 4D addressing. Eliminates spatial databases for raw telemetry storage. Enables real-time analytics on exabytes of drone/LiDAR/autonomous vehicle data.

---

### The Architecture Summary

By baking these 9 features into the v2.1 format:
1. **Smart Extents** — pushes compute to the drive
2. **Crypto-Ephemerality** — O(1) instant data destruction
3. **WAL Streaming** — eliminates middleware messaging queues
4. **Polymorphic RAID** — eliminates wasted disk redundancy
5. **Delta Extents** — annihilates write amplification on small updates
6. **Semantic Dedup** — deduplicates meaning, not just bytes (v7.0)
7. **ZNS Symbiosis** — eliminates SSD garbage collection entirely
8. **zk-SNARK Anchors** — mathematically proves compliance without data exposure (v7.0)
9. **4D Extents** — filesystem understands physical space and time

Because every feature relies on a bit-flag in the Superblock, Inode, or Extent Pointer, a VDE on a laptop storing PDF invoices ignores them all: **0 bytes, 0 CPU cycles overhead**.

---

## Format-Implicit Runtime Features

These features require zero format changes — the DWVD v2.0 binary layout already produces all necessary metadata. They need only higher-layer C# implementation in SDK or plugins.

| Code | Feature | Exploits | Implementation Home |
|------|---------|----------|---------------------|
| TPQR | Temporal Point Queries | TRLR GenerationNumber, MVCC epochs | SDK: VirtualDiskEngine.Query |
| MCDA | Metadata-Only Cold Analytics | Separated TRLR, 512B inodes, Tag Index | SDK: VirtualDiskEngine.Analytics |
| CAED | Content-Addressable Extent Dedup | ExpectedHash:16, SHARED_COW, SNAP | SDK: VirtualDiskEngine.Dedup |
| ICLN | Instant Clone | SHARED_COW, inode copy, SNAP refcounts | SDK: VirtualDiskEngine.Clone |
| PCRD | Probabilistic Corruption Radar | TRLR sampling, XxHash64 | SDK: VirtualDiskEngine.Diagnostics |
| EGLD | Epoch-Gated Lazy Deletion | GenerationNumber, OldestActiveEpoch, Vacuum | SDK: VirtualDiskEngine.Retention |
| PFAB | Progressive Feature A/B Testing | ModuleManifest, lazy init, inode padding | SDK: VirtualDiskEngine.FeatureGates |
| CEIC | Cross-Extent Integrity Chain | ExpectedHash + ContentHash + XxHash64 | SDK: VirtualDiskEngine.Integrity |
| HDAG | Heat-Driven Allocation Tiering | Region Directory ShardId, HeatScore | SDK: VirtualDiskEngine.Allocation |
| ITPS | Inline Tag Predicate Scans | InlineTagArea fixed offset, 512B stride | Plugin: UltimateIntelligence |
| ELIC | Extent-Level Integrity Caching | ExpectedHash, TRLR GenerationNumber | SDK: VirtualDiskEngine.Cache |
| SPSE | Self-Describing Portable Export | Superblock, Region Directory, self-describing inodes | SDK: VirtualDiskEngine.Export |
| NECR | Forensic Necromancy | TRLR stride-scan, GenerationNumber + BlockTypeTag | SDK: VirtualDiskEngine.Recovery |
| PPOC | Proof of Physical Custody | TPM PCR + MerkleRootHash XOR | SDK: VirtualDiskEngine.Security |
| ETPF | Entropy-Triggered Panic Fork | Shannon entropy + SNAP + OldestActiveEpoch | SDK: VirtualDiskEngine.Protection |

Three structural properties that disproportionately enable these features:
1. **Separated Trailer Architecture** — 0.39% overhead creates a metadata index over the entire data corpus (enables MCDA, PCRD, ELIC, EGLD, TPQR)
2. **Per-Extent ExpectedHash:16** — Content-addressable index in extent pointers (enables CAED, CEIC, ELIC)
3. **Region Directory ShardId** — Multiple data shards within one VDE (enables HDAG, multi-device tiering)

---

## Source File References

- Current Superblock: `DataWarehouse.SDK/VirtualDiskEngine/Container/Superblock.cs`
- Current Layout: `DataWarehouse.SDK/VirtualDiskEngine/Container/ContainerFormat.cs`
- Current Inode: `DataWarehouse.SDK/VirtualDiskEngine/Metadata/InodeStructure.cs`
- Current Checksum: `DataWarehouse.SDK/VirtualDiskEngine/Integrity/ChecksumTable.cs`
- Current B-Tree: `DataWarehouse.SDK/VirtualDiskEngine/Index/BTreeNode.cs`
- Current CoW: `DataWarehouse.SDK/VirtualDiskEngine/CopyOnWrite/CowBlockManager.cs`
- Tag System: `DataWarehouse.SDK/Tags/TagTypes.cs`
- Compliance: `DataWarehouse.SDK/Compliance/CompliancePassport.cs`
- TamperProof: `DataWarehouse.SDK/Contracts/TamperProof/TamperProofManifest.cs`
- Replication: `DataWarehouse.SDK/Replication/DottedVersionVector.cs`
- Streaming: `DataWarehouse.SDK/Contracts/Streaming/StreamingStrategy.cs`
- Compute: `DataWarehouse.SDK/Contracts/Compute/ComputeTypes.cs`
- Fabric: `DataWarehouse.SDK/Storage/Fabric/IStorageFabric.cs`

*This specification will be refined after the feature-storage requirements catalog is completed and cross-referenced.*

---

## DWVD Identity & Namespace Signature

The VDE file must be self-identifying as a DataWarehouse native image. No ambiguity — any tool opening the file can immediately determine whether it is a valid DWVD container, which spec revision created it, and which namespace it belongs to.

### Magic Signature (16 bytes at file offset 0x00)

The first 16 bytes of every `.dwvd` file form the magic signature. These bytes are checked before ANY other parsing occurs.

```
Offset  Size  Value (hex)               Interpretation
──────  ────  ────────────────────────  ──────────────────────────────────────
0x00    4     44 57 56 44               "DWVD" ASCII — format identifier
0x04    1     02                        Format major version (2)
0x05    1     00                        Format minor version (0)
0x06    2     00 01                     Spec revision (uint16 LE: 1)
0x08    5     64 77 3A 2F 2F            "dw://" ASCII — namespace anchor
0x0D    3     00 00 00                  Padding (zero-filled)
```

**Total: 16 bytes.** This signature occupies the first 16 bytes of Superblock Block 0, before the remaining superblock fields.

**Validation rules:**
- Bytes 0x00-0x03 MUST be `44 57 56 44`. Any mismatch → not a DWVD file, refuse to open with DW engine.
- Bytes 0x04-0x05 determine format version compatibility. Major version mismatch → refuse. Minor version mismatch → warn but attempt open.
- Bytes 0x06-0x07 are the spec revision counter, incremented for non-breaking spec amendments within a major.minor version.
- Bytes 0x08-0x0C MUST be `64 77 3A 2F 2F`. This anchors the `dw://` namespace and distinguishes DWVD from any other format that might coincidentally start with "DWVD".
- Bytes 0x0D-0x0F MUST be zero. Non-zero values in padding indicate corruption or future use.

### Namespace Registration Block (Superblock Block 2, Extended Metadata)

Located within Superblock Block 2 (offset 0x2000 from file start at default 4 KiB block size), the Namespace Registration Block embeds the VDE's identity within the `dw://` namespace system.

```
Offset  Size  Field                Description
──────  ────  ───────────────────  ──────────────────────────────────────────────────
+0x00   32    NamespacePrefix      UTF-8, null-padded. The dw:// URI prefix for this
                                   VDE (e.g., "dw://cluster01/vde-mil-alpha")
+0x20   16    NamespaceUUID        UUID v7 (time-sortable), globally unique namespace
                                   identifier. Binary encoding (RFC 9562).
+0x30   64    NamespaceAuthority   UTF-8, null-padded. Organization identifier of the
                                   authority that issued this namespace
                                   (e.g., "org.example.defense.hq")
+0x70   64    NamespaceSignature   Ed25519 signature (64 bytes) over the concatenation:
                                   SHA-512(NamespacePrefix || NamespaceUUID)
                                   Signed by the authority's Ed25519 private key.
```

**Total: 176 bytes** within the Extended Metadata area of Block 2.

**Design rationale:**
- **Self-contained identity**: The VDE knows its own URI. No external registry lookup required.
- **Cross-VDE resolution**: When a Cross-VDE Reference Table entry references `dw://cluster01/vde-mil-alpha`, the target VDE can confirm it IS that namespace by comparing its NamespacePrefix + NamespaceUUID.
- **Cryptographic authority verification**: The NamespaceSignature allows any node to verify that the namespace was legitimately issued by the claimed authority, without contacting the authority. The authority's public key is distributed via the cluster trust store.
- **Forgery detection**: Creating a VDE that claims to be `dw://cluster01/vde-mil-alpha` but with a different NamespaceUUID or authority requires forging an Ed25519 signature — computationally infeasible.

**Example:**
```
NamespacePrefix:    "dw://prod-east/financial-core\0\0\0"   (32 bytes, null-padded)
NamespaceUUID:      0192A3B4-C5D6-7E8F-9A0B-1C2D3E4F5A6B   (16 bytes binary)
NamespaceAuthority: "org.acme-bank.infrastructure\0..."      (64 bytes, null-padded)
NamespaceSignature: <64-byte Ed25519 sig over SHA-512 of prefix||uuid>
```

### Format Fingerprint (Superblock Block 3, Integrity Anchor)

Located in Superblock Block 3 alongside the MerkleRootHash and other integrity fields:

```
Offset  Size  Field              Description
──────  ────  ─────────────────  ──────────────────────────────────────────────────
+0x00   32    FormatFingerprint  BLAKE3 hash of the canonical format specification
                                 document (UTF-8 encoded) used to create this VDE
```

**Purpose:**
- A DW engine opening a VDE computes the BLAKE3 of its own spec revision and compares to FormatFingerprint.
- **Match**: VDE was created by the same spec revision. Full compatibility guaranteed.
- **Mismatch**: VDE was created by a different spec revision. The engine consults its version migration table to determine what changed.
- **Unknown fingerprint**: The engine has never seen this spec revision. Opens in read-only mode and logs a warning.

**Interaction with MinReaderVersion/MinWriterVersion** (see Forward Compatibility Markers): FormatFingerprint provides exact identification, while version fields provide coarse-grained compatibility gating.

---

## VDE External Tamper Detection

External tamper detection addresses a critical threat: modification of the `.dwvd` file by tools, processes, or actors outside the DataWarehouse engine. This includes hex editors, other storage engines, filesystem-level corruption, bit rot, malicious modification, and file truncation.

### Header Integrity Seal (Superblock Block 0, last 32 bytes)

The final 32 bytes of Superblock Block 0 (offset `BlockSize - 16 - 32` to `BlockSize - 16`, i.e., before the Universal Block Trailer) contain an HMAC-BLAKE3 seal over the entire block.

```
Offset from Block 0 start  Size  Field
────────────────────────── ────  ─────────────────────
+0x0FB0                    32    HeaderIntegritySeal
+0x0FF0                    16    Universal Block Trailer (BlockTypeTag + Gen + XxHash64)
```

**Computation:**
```
key = HKDF-BLAKE3(
    ikm = VDE_MasterKey,
    salt = VDE_UUID,
    info = "vde-header-seal",
    len = 32
)
seal = HMAC-BLAKE3(
    key = key,
    message = Block0[0x0000 .. 0x0FAF]   // all bytes BEFORE the seal
)
```

**Verification protocol:**
1. On every VDE open, read Superblock Block 0.
2. Derive the seal key from the VDE master key.
3. Compute HMAC-BLAKE3 over bytes `[0x0000 .. 0x0FAF]`.
4. Compare to stored HeaderIntegritySeal.
5. **Match** → proceed normally.
6. **Mismatch** → tamper detected. Apply configured TamperResponse (see below).

**Note:** The Universal Block Trailer's XxHash64 provides corruption detection (non-keyed). The HeaderIntegritySeal provides tamper detection (keyed). Both are checked, in order: XxHash64 first (fast, detects accidental corruption), then HMAC-BLAKE3 (slower, detects intentional modification).

### Metadata Chain Hash (Superblock Block 3)

A rolling hash chain links all metadata regions into a single verifiable chain. Stored in Superblock Block 3, Integrity Anchor area:

```
Offset  Size  Field               Description
──────  ────  ──────────────────  ──────────────────────────────────────────────────
+0xA0   32    MetadataChainHash   BLAKE3 of the ordered concatenation of per-region hashes
+0xC0   8     ChainGeneration     Monotonic counter, incremented on every chain update
+0xC8   8     ChainTimestamp      UTC nanosecond timestamp of last chain update
```

**Chain computation:**
```
MetadataChainHash = BLAKE3(
    SuperblockHash              // BLAKE3 of Superblock Blocks 0-3
    || RegionDirectoryHash      // BLAKE3 of Region Directory (2 blocks)
    || PolicyVaultHash          // BLAKE3 of Policy Vault Header (2 blocks)
    || EncryptionHeaderHash     // BLAKE3 of Encryption Header (2 blocks)
    || BitmapHash               // BLAKE3 of Allocation Bitmap (first 4 KiB only — partial)
    || InodeTableMerkleRoot     // Merkle root of Inode Table (from Integrity Tree)
    || TagIndexRootHash         // BLAKE3 of Tag Index root block
    || ReplicationStateHash     // BLAKE3 of Replication State region header
)
```

**Properties:**
- Any external modification to ANY metadata region changes that region's hash, which changes the MetadataChainHash.
- The chain is updated atomically via the Metadata WAL: new hashes are computed, written to WAL, then committed.
- ChainGeneration is monotonically increasing. A rollback (lower generation) indicates tampering or corruption.
- Partial BitmapHash (first 4 KiB) provides a spot check without hashing the entire bitmap on every metadata write.

### Data Region Integrity (Merkle Tree)

Data block integrity is already covered by the `MerkleRootHash` in Superblock Block 3 and the Integrity Tree region. Any modification to any data block is detectable via O(log N) Merkle path verification from leaf to root. This section confirms the tamper detection guarantees:

- **Single block modification**: Detectable by recomputing the Merkle path (log2(TotalDataBlocks) hash operations).
- **Multiple block modification**: Detectable by Merkle root mismatch. Affected blocks identifiable by tree traversal.
- **Block insertion/removal**: Detectable by Merkle root mismatch + TotalBlocks / ExpectedFileSize mismatch.

### File Size Sentinel (Superblock Block 0)

```
Offset  Size  Field               Description
──────  ────  ──────────────────  ──────────────────────────────────────────────────
+0x60   8     ExpectedFileSize    uint64 LE: exact expected size of the .dwvd file in bytes
+0x68   8     TotalAllocatedBlks  uint64 LE: total blocks allocated (incl. metadata + data)
```

**Invariant:**
```
ExpectedFileSize == TotalAllocatedBlks * BlockSize
```

**Verification:**
1. On VDE open, stat() the file to get actual size.
2. Compare to ExpectedFileSize.
3. `actual < expected` → file truncated. Data loss likely. Open in FORENSIC mode.
4. `actual > expected` → file extended. Appended data is not part of the VDE. Ignore excess, warn.
5. `actual == expected` → size consistent. Proceed to further checks.

**Note:** For thin-provisioned VDEs (see Thin Provisioning Support), ExpectedFileSize reflects the logical size, not the physical allocation. The filesystem's `stat()` may report a smaller physical size due to sparse file holes. In this case, `PhysicalAllocatedBlocks` is used for the physical size check.

### Last Writer Identity (Superblock Block 0)

```
Offset  Size  Field                Description
──────  ────  ───────────────────  ──────────────────────────────────────────────────
+0x70   16    LastWriterSessionId  UUID v7: unique session ID of the last DW engine
                                   instance that performed a write operation
+0x80   8     LastWriterTimestamp  uint64 LE: UTC nanoseconds since epoch of last write
+0x88   16    LastWriterNodeId     UUID: cluster node ID of the writing node
```

**Detection logic:**
1. On VDE open, read LastWriterSessionId, LastWriterTimestamp, LastWriterNodeId.
2. Query the cluster session registry: was this session ID ever issued by this cluster?
3. **Yes, and session is closed**: Normal — a previous DW session wrote and closed cleanly.
4. **Yes, and session is still active**: Another DW instance is concurrently writing. Multi-writer conflict detection triggered.
5. **No, session ID unknown**: The VDE was modified by something that is NOT a DW engine (or a DW engine not part of this cluster). External modification suspected. Apply TamperResponse.
6. **Timestamp newer than last known DW write**: Confirms external modification occurred after DW's last write.

### Tamper Response Levels (Policy Vault)

Configured in the Policy Vault (Block 10-11), the TamperResponse policy determines how the DW engine reacts when tamper is detected.

```
TamperResponse (uint8):
  0x00  WARN_AND_CONTINUE      Log warning to audit log and system log.
                                Open VDE normally. Do not restrict operations.
                                Use case: development, non-sensitive data.

  0x01  READ_ONLY_FORENSIC     Open VDE in read-only mode.
                                Log detailed forensic report (which regions affected).
                                Alert admin via configured notification channel.
                                User can read data but cannot write until admin clears.
                                Use case: general enterprise data.

  0x02  REQUIRE_ADMIN_AUTH     Refuse to open until an admin authenticates via MFA.
                                Admin reviews tamper report, decides to:
                                  - Clear tamper flag and open normally
                                  - Open in forensic mode for investigation
                                  - Refuse and escalate
                                Use case: regulated industries (HIPAA, SOX).

  0x03  REFUSE_TO_OPEN         Refuse to open the VDE entirely.
                                Log tamper event.
                                Require recovery from known-good backup.
                                Use case: classified data, critical infrastructure.

  0x04  AUTO_QUARANTINE        Refuse to open.
                                Notify security team via webhook/email/pager.
                                Move VDE file to quarantine directory.
                                Lock VDE at filesystem level (remove write permissions).
                                Create incident ticket if integration configured.
                                Use case: military, intelligence agencies, nuclear facilities.
```

**Policy Vault encoding:**
```
Offset in Policy Vault  Size  Field
──────────────────────  ────  ─────────────────────
+0x80                   1     TamperResponseLevel (enum value 0x00-0x04)
+0x81                   1     TamperResponseFlags:
                                Bit 0: NotifyAdmin (0=no, 1=yes)
                                Bit 1: NotifySecurityTeam (0=no, 1=yes)
                                Bit 2: CreateIncidentTicket (0=no, 1=yes)
                                Bit 3: PreserveTamperEvidence (0=no, 1=yes)
                                Bit 4: AllowAdminOverride (0=no, 1=yes)
                                Bits 5-7: Reserved
+0x82                   2     MaxConsecutiveTamperEvents (uint16 LE)
                              After this many consecutive tamper detections without
                              admin clearance, escalate to next TamperResponse level.
```

---

## Composable VDE Architecture

### Design Philosophy

The DWVD v2.0 specification defines the **maximum envelope** — the theoretical ceiling of all possible regions, inode fields, and metadata structures. This is the superset. At deployment, the user selects which modules to integrate into the VDE format. The DW engine creates the VDE with exactly those modules at runtime.

**Core principles:**

1. **No wasted bytes.** A VDE contains ONLY the regions and inode fields for its selected modules. A minimal VDE has zero optional regions and a compact inode.

2. **Tier-based fallback.** Features that are NOT module-integrated still work through the plugin pipeline (Tier 2 performance). Module integration provides the "last drops" of performance (Tier 1 — VDE-native, zero-overhead per operation).

3. **Additive evolution.** Modules can be added to an existing VDE via online region addition, inode padding reclamation, or background migration. Removing modules requires VDE migration (create new VDE without the module, copy data).

4. **Self-describing.** The VDE's `ModuleManifest` and `InodeLayoutDescriptor` allow any DW engine to parse the VDE correctly, regardless of which modules are active.

**Overhead range:**
- VDE with zero modules = minimal overhead core format (~1.4%)
- VDE with ALL modules = maximum envelope (~3.2%)
- The user controls where they land on this spectrum

### Module Registry

Each module is assigned a bit position in a 32-bit `ModuleManifest` field stored in the Superblock (Block 0).

```
Bit  Module Name        Abbrev  Block Type Tags Added          Regions Added                    Inode Bytes
───  ─────────────────  ──────  ───────────────────────────── ──────────────────────────────── ──────────
0    Security           SEC     POLV (0x504F4C56)             PolicyVault +                    24
                                ENCR (0x454E4352)             EncryptionHeader                 (KeySlot:4, AclId:4, ContentHash:16)

1    Compliance         CMPL    CMVT (0x434D5654)             ComplianceVault +                12
                                ALOG (0x414C4F47)             AuditLog                         (PassportSlot:4, ClassLabel:4,
                                                                                                SovZone:2, RetentionId:2)

2    Intelligence       INTL    INTE (0x494E5445)             IntelligenceCache                12
                                                                                               (ClassLabel:4, ValueScore:4,
                                                                                                HeatScore:4)

3    Tags               TAGS    TAGI (0x54414749)             TagIndexRegion                   136
                                                                                               (InlineTagCount:4,
                                                                                                TagOverflowBlock:4,
                                                                                                InlineTagArea:128)

4    Replication        REPL    REPL (0x5245504C)             ReplicationState                 8
                                                                                               (ReplGen:4, DirtyFlag:4)

5    RAID               RAID    RAID (0x52414944)             RAIDMetadata                     4
                                                                                               (ShardId:2, GroupId:2)

6    Streaming          STRM    STRE (0x53545245)             StreamingAppend +                8
                                DWAL (0x4457414C)             DataWAL                          (StreamSeqNum:8)

7    Compute            COMP    CODE (0x434F4445)             ComputeCodeCache                 0
                                                                                               (uses existing extent flags)

8    Fabric             FABR    XREF (0x58524546)             CrossVDEReferenceTable           0
                                                                                               (uses existing extent flags)

9    Consensus          CNSS    CLOG (0x434C4F47)             ConsensusLogRegion               0
                                                                                               (region-only, no inode fields)

10   Compression        CMPR    DICT (0x44494354)             DictionaryRegion                 4
                                                                                               (DictId:2, ComprAlgo:2 — in
                                                                                                extent flags overlay)

11   Integrity          INTG    MTRK (0x4D54524B)             IntegrityTree (Merkle)           0
                                                              (MTRK allocated at Level 2+ only;
                                                               Level 0-1 use trailers/index ptrs;
                                                               no inode fields needed)

12   Snapshot           SNAP    SNAP (0x534E4150)             SnapshotTable                    0
                                                                                               (CoW via extent SHARED_COW flag;
                                                                                                no inode fields needed)

13   Query              QURY    BTRE (0x42545245)             BTreeIndexForest (extended)      4
                                                                                               (ContentType:2, SchemaId:2)

14   Privacy            PRIV    ANON (0x414E4F4E)             AnonymizationTable               2
                                                                                               (PIIMarker:1, SubjectRef:1)

15   Sustainability     SUST    —                             (metadata in superblock only)    4
                                                                                               (CarbonScore:4)

16   Transit            TRNS    —                             (metadata in superblock only)    1
                                                                                               (QoSPriority:1)

17   Observability      OBSV    MLOG (0x4D4C4F47)            MetricsLogRegion                 0
                                                                                               (region-only, no inode fields)

18   AuditLog           ALOG    ALOG (0x414C4F47)            AuditLogRegion                   0
                                                                                               (region-only, no inode fields)

19   ComputePushdown    CPSH    —                             (uses ComputeCodeCache region)   48 (overflow)
                                                                                               (WasmPredicateOffset:8, Len:4,
                                                                                                Flags:4, InlinePredicate:32)
                                                              Smart Extents: WASM predicate
                                                              baked into inode for io_uring/
                                                              computational NVMe filtered reads

20   EphemeralKey       EKEY    —                             (uses EncryptionHeader key slots) 32 (overflow)
                                                                                               (EphemeralKeyID:16, TTL_Epoch:8,
                                                                                                KeyRingSlot:4, Flags:4)
                                                              Crypto-shredding: per-inode key
                                                              with TTL. Key drop = O(1) destroy.
                                                              Keys in volatile ring (RAM/TPM only)

21   WalSubscribers     WALS    WALS (0x57414C53)            WalSubscriberCursorTable          0
                                                                                               (region-only; 32B per subscriber:
                                                                                                SubscriberID:8, LastEpoch:8,
                                                                                                LastSequence:8, Flags:8)
                                                              Filesystem-as-Kafka: WAL exposed
                                                              to user-space via mmap/Span<T>

22   DeltaExtents       DELT    —                             (extent flag + inode module)      8
                                                                                               (MaxDeltaDepth:2, CurrentDepth:2,
                                                                                                CompactionPolicy:4)
                                                              Sub-block binary patching via
                                                              VCDIFF. Background Vacuum compacts

23   ZnsZoneMap         ZNSM    ZNSM (0x5A4E534D)            ZnsZoneMapRegion                  0
                                                                                               (region-only; per-entry:
                                                                                                EpochID:8, ZoneID:4, State:2,
                                                                                                Flags:2)
                                                              Epoch→Zone 1:1 mapping. Dead
                                                              epoch = single ZNS_ZONE_RESET

24   SpatioTemporal     STEX    —                             (extent reinterpretation)         6
                                                                                               (CoordSystem:2, Precision:2,
                                                                                                HilbertOrder:2)
                                                              4D extents: 6×32B → 3×64B with
                                                              [Geohash:16][TimeStart:8][TimeEnd:8]

25   SemanticDedup      SDUP    —                             (v7.0 reserve — uses overflow)    266 (overflow)
                                                                                               (EmbeddingDim:2, ModelID:4,
                                                                                                Threshold:4, Embedding:256)
                                                              Latent-space fuzzy dedup via
                                                              WASM autoencoder cosine similarity

26   ZkpCompliance      ZKPA    —                             (v7.0 reserve — uses overflow)    322 (overflow)
                                                                                               (SchemeID:2, CircuitHash:32,
                                                                                                Proof:288)
                                                              zk-SNARK proof embedded in inode.
                                                              5ms verification, zero data exposure

27   GdprTombstone      TOMB    —                             0    Provable erasure: crypto-zeros + hash proof in extent pointer

28   SemanticWearLevel  SWLV    —                             2    TTL-aware block placement for non-ZNS conventional NVMe/SSD

29   QuorumSeal         QSIG    —                            79*   FROST threshold signature per inode. Byzantine federated integrity. (*overflow)

30   StegoWatermark     STEG    —                             8*   v7.0 RESERVE — traitor tracing via Reed-Solomon parity manipulation (*overflow)

31   (Reserved)         —       —                             —    Expansion flag to 64-bit ModuleManifest
```

**ModuleManifest encoding** (uint32 LE, stored at Superblock Block 0 offset +0x40):

| Value | Binary (lower 20 bits) | Modules Active |
|-------|----------------------|----------------|
| `0x00000000` | `0000 0000 0000 0000 0000` | Minimal (core only) |
| `0x0000000F` | `0000 0000 0000 0000 1111` | SEC + CMPL + INTL + TAGS (enterprise basics) |
| `0x0000FFFF` | `0000 1111 1111 1111 1111` | All 16 modules with inode fields |
| `0x0007FFFF` | `0111 1111 1111 1111 1111` | All 19 defined modules (maximum envelope) |
| `0x00000801` | `0000 0000 1000 0000 0001` | SEC + INTG (security + integrity only) |
| `0x00000C09` | `0000 0000 1100 0000 1001` | SEC + CNSS + INTG + SNAP |

### Compact Module Configuration (Nibble-Encoded)

Beyond on/off, each module has a configuration level (0-15) packed into a 4-bit nibble. Two 64-bit fields pack the full configuration:

```
ModuleConfig (uint64 LE, Superblock Block 0 offset +0x44):
  Bits [3:0]    SEC config level   (0=off, 1=basic, ..., 0xF=maximum)
  Bits [7:4]    CMPL config level
  Bits [11:8]   INTL config level
  Bits [15:12]  TAGS config level
  Bits [19:16]  REPL config level
  Bits [23:20]  RAID config level
  Bits [27:24]  STRM config level
  Bits [31:28]  COMP config level
  Bits [35:32]  FABR config level
  Bits [39:36]  CNSS config level
  Bits [43:40]  CMPR config level
  Bits [47:44]  INTG config level
  Bits [51:48]  SNAP config level
  Bits [55:52]  QURY config level
  Bits [59:56]  PRIV config level
  Bits [63:60]  SUST config level

ModuleConfigExt (uint64 LE, Superblock Block 0 offset +0x4C):
  Bits [3:0]    TRNS config level
  Bits [7:4]    OBSV config level
  Bits [11:8]   ALOG config level
  Bits [63:12]  Reserved (zero)
```

**Total: 16 bytes** for full configuration of 32 modules at 16 levels each.

**Config level semantics per module:**

```
SEC (Security) levels:
  0x0: Disabled
  0x1: Basic — PolicyVault only, no EncryptionHeader, no per-block encryption
  0x2: Standard — PolicyVault + EncryptionHeader, 16 key slots, AES-256-CTR
  0x3: Enhanced — Standard + AEAD tags in Integrity Tree, key rotation enabled
  0x4: High — Enhanced + per-extent encryption algo selection, 32 key slots
  0x5-0xE: (reserved for future differentiation)
  0xF: Maximum — all security regions, 63 key slots, full AEAD, full audit trail,
       per-block authentication tags, quantum-resistant key wrapping

CMPL (Compliance) levels:
  0x0: Disabled
  0x1: Basic — ClassificationLabel in inode only, no vault
  0x2: Standard — ComplianceVault with passport records, retention enforcement
  0x3: Enhanced — Standard + AuditLog region, digital signatures on passports
  0x4-0xE: (reserved)
  0xF: Maximum — full compliance suite, real-time audit streaming, sovereign zone enforcement

INTL (Intelligence) levels:
  0x0: Disabled
  0x1: Basic — ClassificationLabel only (inode field, no cache region)
  0x2: Standard — IntelligenceCache region, classification + heat score
  0x3: Enhanced — Standard + predictive tiering, value scoring
  0xF: Maximum — full AI pipeline integration, model versioning in cache

TAGS (Tags) levels:
  0x0: Disabled
  0x1: Basic — InlineTagArea in inode only (128B, ~4 tags), no index region
  0x2: Standard — InlineTagArea + TagIndexRegion (B+-tree)
  0x3: Enhanced — Standard + bloom filter for negative lookups
  0xF: Maximum — full index with range queries, tag inheritance, cross-VDE tag sync

INTG (Integrity) levels — 6-Level Adaptive Integrity Engine:
  0x0: Level 0 — Block Trailers Only. No dedicated integrity region allocated.
       Integrity relies solely on the 16-byte Universal Block Trailer (XxHash64)
       stored in TRLR blocks. Minimal overhead, suitable for IoT/embedded.

  0x1: Level 1 — Authenticated Index Pointers (ZFS-style). No MTRK region allocated.
       The hash of each data block is embedded directly into the B-Tree/index pointer
       that references it. The index IS the integrity structure — traversal automatically
       verifies integrity (one read path, not two). Scales with index, not block count.
       MTRK region is NOT needed at this level. Aligns with composable module philosophy.

  0x2: Level 2 — Epoch-Batched Merkle Forest (BLAKE3). MTRK region allocated.
       Dedicated IntegrityTree region with Merkle tree (32B per leaf). Hot write path
       skips synchronous root updates — a background thread computes cryptographic
       hashes every configurable interval (default 5 seconds), amortizing cost over
       ~2.5M operations at 500K IOPS. Crash window = batch interval, recoverable via
       WAL replay of uncommitted hashes. Solves the synchronous Merkle bottleneck.

  0x3: Level 3 — Learned Integrity Filtering (Merkle + AEAD + AI-driven scrubbing).
       Merkle tree + AEAD authentication tags on data blocks. AI-driven scrub
       prioritization predicts which allocation groups are prone to bit-rot and
       selectively scrubs them first. Smart for operational efficiency — use to
       PRIORITIZE scrubbing, never to skip it. Correctness is still Merkle-based.

  0x4: Level 4 — Blockchain-Anchored (Merkle + AEAD + blockchain + air-gap proofs).
       Full Merkle tree + AEAD + periodic anchoring of MerkleRootHash to an external
       blockchain or tamper-proof chain. Enables air-gap integrity proofs: a detached
       VDE can prove its integrity state at a specific point in time via the chain anchor.

  0x5: Level 5 — Merkle-CRDT (Forward Compatibility Marker — deferred to v7.0).
       INTERFACE ONLY in v6.0. Cryptographic hashes merge with Dotted Version Vectors
       (DVV) to mathematically prove cross-datacenter consistency without scanning data.
       When two federated VDEs have divergent Merkle trees after a partition, the trees
       carry CRDT semantics enabling deterministic merge. Implementation deferred until
       Merkle-CRDT research matures (Protocol Labs). The ICrdtIntegrityProvider interface
       is defined but throws NotSupportedException in v6.0.

  NOTE: MTRK region is only allocated at Level 2+. At Level 0-1, integrity is handled
  by block trailers (L0) or index pointers (L1) with zero dedicated region overhead.

CMPR (Compression) levels:
  0x0: Disabled
  0x1: Basic — LZ4 only, no dictionary, per-extent compression flag
  0x2: Standard — LZ4 + Zstd, trained dictionary support, DictionaryRegion
  0x3: Enhanced — Standard + Brotli, per-content-type algorithm selection
  0xF: Maximum — all algorithms, 256 dictionaries, adaptive compression, PRECOMPRESSED detection
```

### Inode Layout Composition

The inode size is FIXED within a VDE but VARIES between VDEs based on selected modules. This is determined at VDE creation time and recorded in the InodeLayoutDescriptor.

**Physical inode size is fixed at 512 bytes for all standard deployments.** This constraint is non-negotiable: recovery tools perform stride scanning across the Inode Table region, and stride scanning requires a known, fixed stride width. The composable module system still works within this 512B envelope — unused module space becomes zero-padded reserved bytes that are available for future module addition without inode migration. For hyperscale deployments that genuinely require more than 512B of inode data (e.g., extremely wide inline tag areas or multi-hundred-field extended attributes), a second tier of 1024B inodes is available, configured at VDE creation time via the `HYPERSCALE_INODES` feature flag in the Superblock. Standard and hyperscale VDEs are not interchangeable — a hyperscale VDE cannot be opened by an engine that does not recognise 1024B stride.

#### InodeLayoutDescriptor (Superblock Block 1, after Region Pointer Table)

Stored immediately after the 127 Region Pointer slots (127 x 32 = 4064 bytes, leaving 16 bytes in Block 1 — the descriptor continues into the reserved area of Block 2 if needed).

```
Offset  Size  Field              Description
──────  ────  ─────────────────  ──────────────────────────────────────────────────
+0x00   2     InodeSize          uint16 LE: actual inode size in bytes.
                                 Must be power of 2 or multiple of 64:
                                 256, 320, 384, 448, 512, 576, 640, 768, 1024
+0x02   2     CoreFieldsEnd      uint16 LE: byte offset where core fields end.
                                 Always 304 in v2.0 (identity + timestamps +
                                 8 extents + overflow pointers).
+0x04   1     ModuleFieldCount   uint8: number of module field blocks present.
                                 Range: 0 (minimal) to 19 (maximum envelope).
+0x05   1     PaddingBytes       uint8: number of reserved/padding bytes at end
                                 of inode. Available for future module addition.
+0x06   2     Reserved           Zero.

+0x08   N     ModuleFields[]     Array of ModuleFieldCount entries, each 7 bytes:
```

**ModuleField entry (7 bytes each):**

```
Offset  Size  Field              Description
──────  ────  ─────────────────  ──────────────────────────────────────────────────
+0x00   1     ModuleId           uint8: module bit position from ModuleManifest
                                 (0=SEC, 1=CMPL, 2=INTL, ..., 18=ALOG)
+0x01   2     FieldOffset        uint16 LE: byte offset within the inode where
                                 this module's fields begin
+0x03   2     FieldSize          uint16 LE: size of this module's field block in bytes
+0x05   1     FieldVersion       uint8: layout version for this module's fields.
                                 Allows module field layout to evolve independently.
+0x06   1     Flags              uint8:
                                   Bit 0: ACTIVE (1=fields are populated, 0=zeroed/lazy)
                                   Bit 1: MIGRATING (1=background migration in progress)
                                   Bits 2-7: Reserved
```

**Maximum descriptor size:** 8 (header) + 19 * 7 (module entries) = 141 bytes.

This descriptor allows the VDE engine to parse inodes correctly regardless of which modules are active. A VDE with SEC+TAGS+REPL has a different inode layout than one with SEC+INTL+RAID, but both are fully self-describing.

#### Inode Size Calculation Algorithm

The algorithm determines how modules fit within the fixed 512B inode budget and computes the remaining padding. If the selected modules exceed 512B of raw inode data, the inode size is promoted to 1024B (hyperscale tier).

```
function CalculateInodeSize(selectedModules: Module[]): InodeSpec {
    base = 304  // core: identity(32) + timestamps(48) + extents(200) + overflow(24)

    module_bytes = 0
    for each module in selectedModules:
        module_bytes += module.InodeFieldSize

    raw_size = base + module_bytes

    // Fixed-size tier selection: standard (512B) or hyperscale (1024B)
    if raw_size <= 512:
        inode_size = 512
    elif raw_size <= 1024:
        inode_size = 1024   // hyperscale — requires HYPERSCALE_INODES feature flag
    else:
        error("Module combination exceeds 1024B inode limit — reduce selected modules")

    padding = inode_size - raw_size

    return { InodeSize: inode_size, PaddingBytes: padding }
}
```

**Worked examples:**

```
Example 1: Minimal (no modules)
  Core:             304 bytes
  Modules:            0 bytes
  Raw:              304 bytes
  Fixed tier:       512 bytes (standard)
  Padding:          208 bytes (available for future module addition without migration)
  InodeSize = 512

Example 2: SEC + TAGS + REPL + INTL
  Core:             304 bytes
  + SEC:             24 bytes → 328
  + TAGS:           136 bytes → 464
  + REPL:             8 bytes → 472
  + INTL:            12 bytes → 484
  Raw:              484 bytes
  Fixed tier:       512 bytes (standard — 484 ≤ 512, fits)
  Padding:           28 bytes (available for future modules)
  InodeSize = 512

Example 3: SEC + CMPL + RAID + CMPR + QURY
  Core:             304 bytes
  + SEC:             24 bytes → 328
  + CMPL:            12 bytes → 340
  + RAID:             4 bytes → 344
  + CMPR:             4 bytes → 348
  + QURY:             4 bytes → 352
  Raw:              352 bytes
  Fixed tier:       512 bytes (standard — 352 ≤ 512, fits)
  Padding:          160 bytes
  InodeSize = 512

Example 4: All 19 modules (maximum envelope)
  Core:             304 bytes
  + SEC:             24 → 328
  + CMPL:            12 → 340
  + INTL:            12 → 352
  + TAGS:           136 → 488
  + REPL:             8 → 496
  + RAID:             4 → 500
  + STRM:             8 → 508
  + COMP:             0 → 508
  + FABR:             0 → 508
  + CNSS:             0 → 508
  + CMPR:             4 → 512
  + INTG:             0 → 512
  + SNAP:             0 → 512
  + QURY:             4 → 516
  + PRIV:             2 → 518
  + SUST:             4 → 522
  + TRNS:             1 → 523
  + OBSV:             0 → 523
  + ALOG:             0 → 523
  Raw:              523 bytes
  Fixed tier:      1024 bytes (hyperscale — 523 > 512, promoted; requires HYPERSCALE_INODES flag)
  Padding:          501 bytes
  InodeSize = 1024
```

The padding bytes are marked as RESERVED in the InodeLayoutDescriptor and can be claimed by future module additions WITHOUT inode migration (see Online Module Addition).

### Three-Tier Feature Performance Model

Every feature in DataWarehouse operates at one of three performance tiers depending on whether its module is integrated into the VDE format.

| Tier | Name | Mechanism | Format Overhead | Runtime Overhead | When Used |
|------|------|-----------|-----------------|------------------|-----------|
| **Tier 1** | VDE-Integrated | Dedicated region + inline inode fields | 0.1-3.2% (depends on module count) | Near-zero per-operation | Module enabled at VDE creation or added online |
| **Tier 2** | Pipeline-Optimized | Feature works through plugin pipeline with SDK caching | None | Memory (caches), 1-2 extra I/O per session | Module not in VDE, feature enabled in DW config |
| **Tier 3** | Basic | Feature works with no optimization | None | Higher I/O, CPU per operation | Feature enabled, no tuning applied |

#### Per-Feature Tier Mapping

The following table exhaustively maps every major feature across all three tiers:

```
Feature              Tier 1 (VDE-Integrated)                    Tier 2 (Pipeline-Optimized)          Tier 3 (Basic)
─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
Encryption           KeySlot in inode (4B), EncryptionHeader     IKeyStore lookup per session,        Encrypt/decrypt via
                     region with 63 key slots, per-block algo    cached session keys in memory.       pipeline with external
                     in extent flags, AEAD tags in Integrity     ~1 extra lookup per session start.   key store. ~2 extra
                     Tree. ~0 extra I/O per data operation.                                           lookups per operation.

Compression          DictId (2B) + ComprAlgo (2B) in extent      Dictionary loaded from external      Compress/decompress per
                     flags, DictionaryRegion for Zstd trained    store per session, cached in         operation with default
                     dicts, PRECOMPRESSED extent flag for skip.  memory. ~1 external lookup per       algo (LZ4), no dictionary
                     ~0 extra I/O per data operation.            session start.                       support, no skip for
                                                                                                      pre-compressed data.

Access Control       AclPolicyId (4B) in inode, PolicyVault      ACL evaluated from config database,  ACL loaded from config
                     sealed in VDE. Single inode read provides   cached per user session.              file at startup. No
                     full ACL context. ~0 extra I/O.             ~1 extra DB query per session.       per-object ACL support.

Tags                 InlineTagArea (128B) in inode,              Tags stored in external inverted     Tags as key-value pairs
                     TagIndexRegion (B+-tree + bloom filter).    index, sharded in-memory index.      in object metadata
                     Single I/O for 80% of tagged objects.       1-2 extra I/O per tag query.         properties. Full inode
                     O(log N) tag queries via B+-tree.           O(log N) via external index.         scan for any tag query.

Compliance           PassportSlot (4B) + ClassLabel (4B) +       CompliancePassport loaded from       Compliance checked at
                     SovZone (2B) + RetentionId (2B) in inode,  external compliance store per        API boundary only. No
                     ComplianceVault region, AuditLog region.    object access. ~1 extra I/O per      format-level enforcement.
                     Zero extra I/O for classification check.    object operation.                    No embedded passports.

Replication          ReplGen (4B) + DirtyFlag (4B) in inode,    Replication tracked via message      Replication via full
                     ReplicationState region with DVV snapshot   bus events, change log in external   object comparison. Must
                     + dirty bitmap. Delta replication: scan     DB. Must diff dataset to find        transfer entire dataset
                     dirty bitmap only. O(N/8) scan.             changes. O(N) for change detection. for every sync cycle.

RAID                 ShardId (2B) + GroupId (2B) in inode,      RAID managed by storage strategy,    RAID at OS/hardware
                     RAIDMetadata region with shard maps,        shard info in external manifest.     level only, no DW
                     parity layout, rebuild progress.            Rebuild requires external manifest   awareness. Manual
                     Self-describing autonomous rebuild.         lookup. ~1 extra I/O per shard op.  rebuild, no auto-heal.

Integrity            Merkle tree in IntegrityTree region,        Flat checksum table (8B per block,   CRC32 in Universal
                     replaces flat checksums. O(log N) subset   xxHash64). Full O(N) verification    Block Trailer only.
                     proofs, air-gap verification, tamper-       required. No subset proofs. No       No independent
                     proof chain anchoring. 32B per leaf.        Merkle path verification.            verification capability.

Streaming            StreamSeqNum (8B) in inode, ring buffer    Streaming via message bus append     Standard write path,
                     region with head/tail pointers, partition   log, external offset tracking.       no append optimization,
                     support. Zero-fragmentation sequential      ~1 extra I/O per append for offset  normal block allocation
                     writes. O(1) append.                        management.                          (fragmentation likely).

Snapshot             SnapshotTable region, CoW extent flags      Snapshot via external metadata       No snapshot support.
                     (SHARED_COW). O(1) snapshot creation        store tracking shared blocks.        Full copy required for
                     via metadata-only operation. Instant.       O(N) for large snapshots (must       any point-in-time copy.
                                                                 scan + record all extents).

Intelligence         ClassLabel (4B) + ValueScore (4B) +        AI classification scores in          No AI-driven storage
                     HeatScore (4B) in inode, IntelligenceCache external cache (Redis/DB), scores    optimization. All
                     region with model metadata.                 recomputed or fetched on access.     objects treated equally.
                     Tiering decisions from inode scan only.     ~1 extra I/O per object for score.  No heat-based tiering.

Consensus            ConsensusLogRegion with per-Raft-group      Raft log persisted in external       No built-in consensus.
                     append-only log. Term + Index per entry.   file or database. Single Raft        External coordination
                     Multiple Raft groups via separate region   group only. ~2 extra I/O per log     service required (etcd,
                     directory slots. Integrated crash recovery. entry (write + fsync).               ZooKeeper, etc.).

Query                ContentType (2B) + SchemaId (2B) in inode, Schema loaded from external catalog, No predicate pushdown.
                     extended BTreeIndexForest with multiple     column stats in external DB.         Full scan for all
                     indexes. Predicate pushdown to block level. Predicate pushdown at pipeline       queries. No index
                     O(log N) index lookups.                     level. ~1-2 extra I/O per query.    support beyond B-tree.

Privacy              PIIMarker (1B) + SubjectRef (1B) in inode, PII detection results stored in      PII detection on-demand
                     AnonymizationTable in VDE. GDPR right-to-  external privacy database.           only. GDPR erasure
                     erasure via inode scan for SubjectRef.      GDPR erasure requires external DB    requires full scan of
                     O(N) scan, but with small constant.         query + object scan. ~2 extra I/O.  all objects. Very slow.

Fabric               CrossVDE Reference Table in VDE.            Fabric routing via SDK FabricClient  No cross-VDE reference
                     O(1) cross-VDE link resolution via local   lookup. O(log N) via message bus     support. Cross-VDE
                     table lookup. No network round-trip for     query. ~1 network round-trip per     operations require
                     reference metadata.                         reference resolution.                manual configuration.

Compute              ComputeCodeCache region in VDE.             WASM modules loaded from object      No WASM caching. Modules
                     Memory-mapped fast module load. Code hash  store on demand. Standard read       loaded from external
                     → cached bytecode in <1ms.                  path. ~2-5 I/O per module load.     source every invocation.

Sustainability       CarbonScore (4B) in inode.                  Carbon footprint tracked in          No carbon awareness.
                     Per-object carbon footprint available from  external sustainability service.     No per-object footprint
                     inode scan. O(1) per object.                ~1 extra service call per object.    tracking.

Transit              QoSPriority (1B) in inode.                  QoS priority from external config    No per-object QoS.
                     Block-level priority scheduling based on    per connection/tenant. Connection-   Best-effort delivery
                     per-object QoS. Fine-grained control.       level QoS only. ~0 extra I/O.       for all objects.

Observability        MetricsLogRegion in VDE.                    Metrics exported to external         Basic logging to
                     Self-contained time-series metrics.         observability system (Prometheus,    stdout/file. No
                     VDE is its own observability source.        Grafana, etc.). Requires external    structured metrics.
                     Queryable without external systems.         infrastructure. ~0 extra I/O.        No self-contained data.

AuditLog             AuditLogRegion in VDE.                      Audit events written to external     No persistent audit
                     Append-only, monotonically sequenced,       database or log file. Truncation     trail. Audit events
                     NEVER truncated, indexed by timestamp.      risk. ~1 extra I/O per event.        logged to system log
                     Tamper-evident hash chain.                   No hash chain guarantee.             only. Volatile.
```

### Online Module Addition (Option 1: Modify Existing VDE)

When a user enables a new feature and wants VDE integration on an existing VDE:

#### Step 1: Region Addition (always possible, zero downtime)

Region addition is straightforward because the Region Directory has 127 slots (of which typically 7-15 are used):

1. Identify free blocks in the Allocation Bitmap for the new region.
2. Allocate the required number of blocks.
3. Initialize the new region (write region header, zero data area).
4. Write a Metadata WAL entry recording the allocation.
5. Add a Region Directory entry in an unused slot:
   ```
   RegionPointer {
       RegionTypeId:  <new region type>
       Flags:         ACTIVE
       StartBlock:    <allocated start>
       BlockCount:    <region size>
       UsedBlocks:    0
   }
   ```
6. Update Superblock `ModuleManifest` bit (set the module's bit).
7. Update Superblock `FeatureFlags` if applicable.
8. Commit WAL entry. All changes are atomic.

**Crash safety:** If the system crashes between steps, the WAL replay either completes the addition or rolls back to the previous state. No partial additions.

#### Step 2: Inode Field Addition (depends on reserved padding)

**Case A: Padding bytes available in inode (COMMON)**

This is the expected case. The inode size calculation intentionally rounds up to the next multiple of 64, creating padding bytes that serve as reserved space for future module additions.

1. Read the InodeLayoutDescriptor from Superblock Block 1.
2. Verify `PaddingBytes >= module.InodeFieldSize`.
3. Add a new ModuleField entry:
   ```
   ModuleField {
       ModuleId:      <module bit position>
       FieldOffset:   InodeSize - PaddingBytes  // claim from end of padding
       FieldSize:     <module's inode bytes>
       FieldVersion:  1
       Flags:         ACTIVE
   }
   ```
4. Update `PaddingBytes -= module.InodeFieldSize`.
5. Increment `ModuleFieldCount`.
6. Write updated descriptor via WAL.
7. New module fields are initialized to zero — lazy initialization populates them on first access to each inode.

**Zero downtime, no inode migration required.** Existing inodes already have the bytes (as padding); we simply reinterpret them.

**Case B: No padding bytes available (RARE)**

Only occurs when the inode is fully packed (zero padding remaining) and a new module requires inode fields.

1. Calculate new InodeSize: `ceil((currentInodeSize + module.InodeFieldSize) / 64) * 64`.
2. Allocate new Inode Table region with the expanded inode size.
3. Begin background migration:
   - Read each inode from old table.
   - Copy core fields + existing module fields.
   - Initialize new module fields to zero.
   - Write to new table at the corresponding inode number.
4. During migration, reads check old table first, writes go to new table.
5. When all inodes are migrated, atomically swap the Region Directory pointer.
6. Free old Inode Table blocks.
7. Update InodeLayoutDescriptor with new InodeSize and module fields.

**The VDE remains fully operational during migration.** This process is analogous to ext4's online inode resize.

**Crash safety:** Migration progress is tracked in the Metadata WAL. On crash, resume from the last committed inode number.

#### Step 3: Update ModuleManifest and ModuleConfig

A single Superblock write updates:
- `ModuleManifest`: set the new module's bit.
- `ModuleConfig` or `ModuleConfigExt`: set the module's configuration level nibble.
- `HeaderIntegritySeal`: recompute.
- `MetadataChainHash`: recompute with updated region directory hash.

### New VDE Migration (Option 2: Create Fresh VDE)

When the user prefers a clean VDE with the desired module configuration:

1. **Create** a new VDE with the updated `ModuleManifest` using the desired creation profile or custom module selection.
2. **Bulk copy** data from old VDE to new VDE:
   - Extent-aware copy: preserves extent layout for sequential access patterns.
   - CoW-aware: shared extents (snapshots) are re-shared in the new VDE, not duplicated.
   - Inode transformation: core fields copied directly, module fields populated from old VDE or initialized to zero.
   - Bandwidth: limited by storage I/O. Typical: ~70 MB/s for HDD, ~500 MB/s for SSD, ~2 GB/s for NVMe.
3. **Atomic cutover**:
   - Option A (standalone): rename files (`mv old.dwvd old.dwvd.bak && mv new.dwvd production.dwvd`).
   - Option B (fabric): update the Fabric routing table to point to the new VDE. Zero downtime with fabric hot-swap.
4. **Verification**: compare MerkleRootHash of new VDE against expected value computed during copy.
5. **Cleanup**: old VDE kept as backup until admin confirms the new VDE is operating correctly.

**Estimated migration times:**

| VDE Size | HDD (~70 MB/s) | SSD (~500 MB/s) | NVMe (~2 GB/s) |
|----------|----------------|-----------------|-----------------|
| 1 GB | 15 seconds | 2 seconds | <1 second |
| 50 GB | 12 minutes | 100 seconds | 25 seconds |
| 1 TB | 4 hours | 35 minutes | 9 minutes |
| 10 TB | 40 hours | 6 hours | 85 minutes |

### Tier 2 Fallback (Option 3: No VDE Changes)

The feature works immediately through the plugin pipeline. No VDE format changes. No downtime. No risk.

- The DW engine routes feature operations through the standard SDK plugin pipeline.
- Caching strategies (in-memory LRU, session-scoped, tenant-scoped) minimize redundant I/O.
- Performance is "very good" — typically 1-2 extra I/O operations per session for feature metadata.
- The user can upgrade to Option 1 or Option 2 at any time in the future.

**This is ALWAYS available, ALWAYS instant, ZERO risk.** Recommended as the default for users who want to evaluate a feature before committing to format integration.

### User Choice Flow

When a user enables a feature, the DW engine presents the integration options:

```
╔══════════════════════════════════════════════════════════════════════════════╗
║  Feature "Encryption" enabled for VDE "production-01"                      ║
╠══════════════════════════════════════════════════════════════════════════════╣
║                                                                            ║
║  VDE Integration Options:                                                  ║
║                                                                            ║
║  [1] Add to current VDE (online modification)                              ║
║      • Adds EncryptionHeader region (2 blocks, 8 KiB)                      ║
║      • Adds PolicyVault region (2 blocks, 8 KiB)                           ║
║      • Adds KeySlot+AclId+ContentHash to inodes (24 bytes)                 ║
║        → From reserved padding: 28 bytes available, 24 consumed,           ║
║          4 bytes remaining. No inode migration needed.                      ║
║      • Performance: Tier 1 (maximum — ~0 extra I/O per operation)          ║
║      • Downtime: None                                                      ║
║      • Risk: Low (atomic via WAL)                                          ║
║                                                                            ║
║  [2] Create new VDE with Encryption module                                 ║
║      • New VDE built with SEC module from scratch                          ║
║      • Data migrated from current VDE                                      ║
║      • Estimated time: 12 minutes for 50 GB (SSD)                          ║
║      • Performance: Tier 1 (maximum)                                       ║
║      • Downtime: ~12 minutes (or zero with fabric hot-swap)                ║
║      • Risk: Very low (old VDE preserved as backup)                        ║
║                                                                            ║
║  [3] Use without VDE integration (recommended for quick start)             ║
║      • Feature works immediately through plugin pipeline                   ║
║      • Performance: Tier 2 (very good — ~1 extra lookup per session)       ║
║      • Downtime: None                                                      ║
║      • Risk: Zero                                                          ║
║      • Note: You can upgrade to Option 1 or 2 anytime later               ║
║                                                                            ║
║  Current VDE:                                                              ║
║    ModuleManifest = 0x0000000C (INTL + TAGS)                               ║
║    InodeSize      = 512 bytes, 28 bytes reserved padding                   ║
║                                                                            ║
║  After Option 1:                                                           ║
║    ModuleManifest = 0x0000000D (SEC + INTL + TAGS)                         ║
║    InodeSize      = 512 bytes, 4 bytes reserved padding                    ║
║                                                                            ║
╚══════════════════════════════════════════════════════════════════════════════╝
```

### VDE Creation Profiles (Pre-Built Templates)

Pre-built profiles simplify VDE creation for common use cases. Users can select a profile or create a custom configuration.

#### Profile: Minimal

```
Name:             Minimal
Use case:         Development, testing, ephemeral data, throwaway containers
ModuleManifest:   0x00000000
Modules:          None (core format only)
InodeSize:        320 bytes (304 core + 16 padding)
Regions:          Superblock Group (x2) + Region Directory + Allocation Bitmap +
                  Inode Table + Metadata WAL + Data Region
Overhead:         ~1.4%
Block Size:       4 KiB (default)
```

#### Profile: Standard

```
Name:             Standard
Use case:         General-purpose enterprise, business data, standard compliance
ModuleManifest:   0x00000C01 (SEC + INTG + SNAP + CMPR — bits 0, 10, 11, 12)
                  Binary: 0000 0000 0000 0000 0001 1100 0000 0001
Modules:          Security + Integrity (Merkle) + Snapshot (CoW) + Compression
InodeSize:        384 bytes (304 + 24 SEC + 4 CMPR + 52 padding)
ModuleConfig:     SEC=0x2 (standard), INTG=0x2 (Merkle), SNAP=0x1, CMPR=0x2 (Zstd)
Regions:          Core + PolicyVault + EncryptionHeader + IntegrityTree +
                  SnapshotTable + DictionaryRegion
Overhead:         ~2.0%
Block Size:       4 KiB (default)
```

#### Profile: Enterprise

```
Name:             Enterprise
Use case:         Regulated industries (HIPAA, SOX, PCI-DSS), healthcare, financial
ModuleManifest:   0x00040C0F (SEC + CMPL + INTL + TAGS + INTG + SNAP + CMPR + ALOG)
                  Binary: 0000 0000 0000 0100 0000 1100 0000 1111
                  Bits:   0 (SEC) + 1 (CMPL) + 2 (INTL) + 3 (TAGS) +
                          10 (CMPR) + 11 (INTG) + 18 (ALOG)
Modules:          Security + Compliance + Intelligence + Tags + Compression +
                  Integrity + AuditLog
InodeSize:        512 bytes (304 + 24 + 12 + 12 + 136 + 4 + 20 padding)
ModuleConfig:     SEC=0x3, CMPL=0x3, INTL=0x2, TAGS=0x3, CMPR=0x2, INTG=0x2, ALOG=0x1
Regions:          Core + PolicyVault + EncryptionHeader + ComplianceVault +
                  IntelligenceCache + TagIndexRegion + DictionaryRegion +
                  IntegrityTree + AuditLogRegion
Overhead:         ~2.5%
Block Size:       4 KiB (default)
```

#### Profile: Maximum Security

```
Name:             Maximum Security
Use case:         Military, intelligence agencies, nuclear facilities, classified data
ModuleManifest:   0x0007FFFF (ALL 19 modules active)
                  Binary: 0000 0000 0000 0111 1111 1111 1111 1111
Modules:          All 19 defined modules enabled
InodeSize:        576 bytes (304 + 219 module fields + 53 padding)
ModuleConfig:     All modules at maximum level (0xF)
ModuleConfig val: 0xFFFFFFFFFFFFFFFF
ModuleConfigExt:  0x0000000000000FFF
Regions:          All 20+ regions active
Overhead:         ~3.2%
Block Size:       4 KiB (default)
TamperResponse:   AUTO_QUARANTINE (0x04)
```

#### Profile: Edge/IoT

```
Name:             Edge/IoT
Use case:         IoT gateways, edge nodes, embedded systems, constrained devices
ModuleManifest:   0x00000840 (STRM + INTG — bits 6, 11)
                  Binary: 0000 0000 0000 0000 0000 1000 0100 0000
Modules:          Streaming + Integrity
InodeSize:        320 bytes (304 + 8 STRM + 8 padding)
ModuleConfig:     STRM=0x1 (basic ring buffer), INTG=0x1 (flat checksum)
Regions:          Core + StreamingAppend + DataWAL + flat checksum table
Overhead:         ~1.8%
Block Size:       512 bytes (sub-4K for tiny telemetry payloads)
Note:             512B block size with 16B trailer = 496 usable bytes/block.
                  Optimized for high-frequency small writes (sensor data, telemetry).
```

#### Profile: Analytics

```
Name:             Analytics
Use case:         Data warehousing, analytics workloads, ML pipelines, data lakes
ModuleManifest:   0x00002C04 (INTL + CMPR + SNAP + QURY — bits 2, 10, 12, 13)
                  Binary: 0000 0000 0000 0000 0010 1100 0000 0100
Modules:          Intelligence + Compression + Snapshot + Query
InodeSize:        384 bytes (304 + 12 INTL + 4 CMPR + 4 QURY + 60 padding)
ModuleConfig:     INTL=0x2, CMPR=0x3 (enhanced, multi-algo), SNAP=0x1, QURY=0x2
Regions:          Core + IntelligenceCache + DictionaryRegion + SnapshotTable +
                  BTreeIndexForest (extended)
Overhead:         ~2.0%
Block Size:       64 KiB (large blocks for sequential scan workloads)
Note:             64 KiB blocks optimize for columnar scans and large sequential reads.
                  Predicate pushdown via BTreeIndexForest reduces unnecessary block reads.
```

#### Profile: Custom

```
Name:             Custom
Use case:         When no preset profile fits the workload
ModuleManifest:   User-selected (any combination of bits 0-18)
Modules:          User picks from module registry menu
InodeSize:        Calculated from selection via CalculateInodeSize()
ModuleConfig:     User sets per-module configuration levels (0-15)
Overhead:         Calculated and displayed before confirmation
Block Size:       User-selected (512, 1024, 2048, 4096, 8192, 16384, 32768, 65536)
```

---

## Gap Closure: Additional Regions from Cross-Reference Analysis

The following regions were identified as missing in the cross-reference analysis between the feature storage requirements catalog and the v2.0 layout. They fill gaps where existing regions do not adequately cover the required functionality.

### Audit Log Region (ALOG)

**Block Type Tag:** `0x414C4F47` ("ALOG" ASCII)
**Module:** AuditLog (bit 18) — also used by Compliance module (bit 1) when CMPL level >= 3

**Purpose:** Append-only, monotonically sequenced, NEVER truncated log of all security-relevant and compliance-relevant events. Distinguished from:
- **Streaming Append Region** (ring buffer, overwrites old entries when full)
- **WORM Immutable Region** (bulk immutable data storage, not event-structured)
- **Metadata WAL** (crash recovery journal, regularly truncated after checkpoint)

**Region header (first block):**

```
Offset  Size  Field                Description
──────  ────  ───────────────────  ──────────────────────────────────────────────────
+0x00   8     RegionMagic          0x41554449544C4F47 ("AUDITLOG")
+0x08   8     FirstSequenceNum     uint64 LE: sequence number of first entry in region
+0x10   8     LastSequenceNum      uint64 LE: sequence number of last entry written
+0x18   8     EntryCount           uint64 LE: total entries in this region
+0x20   8     HeadOffset           uint64 LE: byte offset of oldest entry (always grows)
+0x28   8     TailOffset           uint64 LE: byte offset past newest entry (write cursor)
+0x30   8     IndexRootBlock       uint64 LE: block number of B-Tree index root
+0x38   32    ChainHash            BLAKE3 hash: hash of previous ChainHash + last entry
+0x58   8     CreatedTimestamp     uint64 LE: UTC nanoseconds, region creation time
+0x60   8     OldestEntryTimestamp uint64 LE: UTC nanoseconds, oldest entry timestamp
+0x68   8     NewestEntryTimestamp uint64 LE: UTC nanoseconds, newest entry timestamp
```

**Entry format (variable length, packed sequentially):**

```
Offset  Size     Field           Description
──────  ───────  ──────────────  ──────────────────────────────────────────────────
+0x00   8        SequenceNumber  uint64 LE: monotonically increasing, never reused
+0x08   8        Timestamp       uint64 LE: UTC nanoseconds since epoch
+0x10   2        EventType       uint16 LE: event type code (see below)
+0x12   16       ActorId         UUID: identity of the actor (user, service, system)
+0x22   8        TargetInode     uint64 LE: inode number of affected object (0 if N/A)
+0x2A   2        PayloadSize     uint16 LE: size of variable payload (0-65535 bytes)
+0x2C   var      Payload         Event-specific payload (JSON-encoded or binary)
+0x2C+P 32       EntryHash       BLAKE3(previous_entry_hash || this_entry_bytes[0..0x2C+P])
```

**Event type codes:**

| Code | Event | Description |
|------|-------|-------------|
| 0x0001 | OBJECT_READ | Object read access |
| 0x0002 | OBJECT_WRITE | Object write/modify |
| 0x0003 | OBJECT_DELETE | Object deletion |
| 0x0004 | OBJECT_CREATE | Object creation |
| 0x0010 | ACL_CHANGE | Access control list modified |
| 0x0011 | KEY_ROTATE | Encryption key rotated |
| 0x0012 | KEY_REVOKE | Encryption key revoked |
| 0x0020 | POLICY_CHANGE | Policy vault entry modified |
| 0x0021 | TAMPER_DETECTED | External tamper detection triggered |
| 0x0030 | COMPLIANCE_CHECK | Compliance passport verified |
| 0x0031 | RETENTION_EXPIRE | Retention period expired |
| 0x0040 | REPL_SYNC | Replication sync event |
| 0x0041 | REPL_CONFLICT | Replication conflict detected |
| 0x0050 | ADMIN_LOGIN | Administrative access |
| 0x0051 | ADMIN_ACTION | Administrative action performed |
| 0x0060 | VDE_MOUNT | VDE opened/mounted |
| 0x0061 | VDE_UNMOUNT | VDE closed/unmounted |
| 0x0070 | SNAPSHOT_CREATE | Snapshot created |
| 0x0071 | SNAPSHOT_DELETE | Snapshot deleted |
| 0x0080 | PRIVACY_ERASURE | GDPR right-to-erasure executed |
| 0x0081 | PII_DETECTED | PII detected in object |
| 0xFFFF | CUSTOM | Custom event (payload contains event definition) |

**B-Tree index:** A secondary B-Tree indexes entries by timestamp range, enabling efficient queries like "all events between T1 and T2" without scanning the entire log.

**Immutability guarantee:** The Audit Log region is append-only. The block allocator NEVER frees Audit Log blocks. The ChainHash provides tamper evidence — any modification to any entry invalidates all subsequent hashes.

### Consensus Log Region (CLOG)

**Block Type Tag:** `0x434C4F47` ("CLOG" ASCII)
**Module:** Consensus (bit 9)

**Purpose:** Persistent storage for distributed consensus protocol logs. Supports Raft, Paxos, and PBFT log persistence within the VDE format.

**Region header (first block):**

```
Offset  Size  Field                Description
──────  ────  ───────────────────  ──────────────────────────────────────────────────
+0x00   8     RegionMagic          0x434F4E534C4F4700 ("CONSLOG\0")
+0x08   1     ProtocolType         0x01=Raft, 0x02=Paxos, 0x03=PBFT
+0x09   1     GroupCount           uint8: number of consensus groups (multi-Raft)
+0x0A   2     Reserved             Zero
+0x0C   8     CurrentTerm          uint64 LE: current term/epoch (Raft)
+0x14   8     CommitIndex          uint64 LE: highest committed log index
+0x1C   8     LastApplied          uint64 LE: highest applied log index
+0x24   16    VotedFor             UUID: candidate voted for in current term (Raft)
+0x34   16    LeaderId             UUID: current leader ID (if known)
+0x44   4     LogEntryCount        uint32 LE: total log entries in this group
+0x48   8     FirstLogIndex        uint64 LE: index of first log entry (after compaction)
+0x50   8     LastLogIndex         uint64 LE: index of last log entry
+0x58   8     SnapshotIndex        uint64 LE: index of last snapshot (for log compaction)
+0x60   8     SnapshotTerm         uint64 LE: term of last snapshot
```

**Log entry format:**

```
Offset  Size     Field           Description
──────  ───────  ──────────────  ──────────────────────────────────────────────────
+0x00   8        Term            uint64 LE: term in which entry was created
+0x08   8        Index           uint64 LE: log position (monotonically increasing)
+0x10   1        EntryType       uint8: 0x01=Command, 0x02=ConfigChange,
                                        0x03=NoOp, 0x04=Snapshot
+0x11   3        Reserved        Zero
+0x14   4        PayloadSize     uint32 LE: size of payload in bytes
+0x18   var      Payload         Command payload (state machine input)
+0x18+P 4        CRC32           CRC-32C of bytes [0x00 .. 0x18+P-1]
```

**Multi-Raft support:** Multiple consensus groups are supported by allocating separate Region Directory slots for each group. The GroupCount field in the header indicates how many groups exist. Each group's CLOG region is independent, with its own Term, CommitIndex, and log entries.

### Compression Dictionary Region (DICT)

**Block Type Tag:** `0x44494354` ("DICT" ASCII)
**Module:** Compression (bit 10)

**Purpose:** Stores trained compression dictionaries (Zstd, Brotli) that are referenced by the 2-byte `DictId` in extent flags. Trained dictionaries dramatically improve compression ratios for small objects with shared structure (e.g., JSON documents, log entries, protocol buffers).

**Region header (first block):**

```
Offset  Size  Field                Description
──────  ────  ───────────────────  ──────────────────────────────────────────────────
+0x00   8     RegionMagic          0x434D505244494354 ("CMPRDICT")
+0x08   2     DictionaryCount      uint16 LE: number of dictionaries stored (max 256)
+0x0A   2     MaxDictionaries      uint16 LE: maximum dictionaries (always 256)
+0x0C   4     TotalDictBytes       uint32 LE: total bytes used by all dictionaries
+0x10   8     LastUpdated          uint64 LE: UTC nanoseconds, last dictionary change
```

**Dictionary directory (immediately after header, 256 entries x 27 bytes = 6912 bytes):**

```
Offset  Size  Field                Description
──────  ────  ───────────────────  ──────────────────────────────────────────────────
+0x00   2     DictionaryId         uint16 LE: ID referenced by extent DictId field
+0x02   1     AlgorithmId          uint8: 0x00=None, 0x01=Zstd, 0x02=Brotli,
                                          0x03=LZ4, 0x04=Snappy
+0x03   4     DictSize             uint32 LE: dictionary size in bytes
+0x07   8     DictOffset           uint64 LE: byte offset within region to dict data
+0x0F   4     TrainedFromSamples   uint32 LE: number of samples used for training
+0x13   8     CreatedUtc           uint64 LE: UTC nanoseconds, dictionary creation time
+0x1B   2     ContentTypeHint      uint16 LE: MIME type hint (index into string table)
                                   0x0000=generic, 0x0001=application/json,
                                   0x0002=text/plain, 0x0003=application/protobuf, etc.
```

**Dictionary data:** Stored sequentially after the directory. Each dictionary is a raw Zstd/Brotli dictionary blob, loaded into memory on first use and cached for the VDE session lifetime.

**Usage flow:**
1. Object written with compression enabled.
2. DW engine selects best dictionary based on ContentType (from QURY module) or content analysis.
3. Extent flags record `DictId` + `ComprAlgo`.
4. On read, extent flags → DictId → dictionary directory → load dictionary → decompress.

### Metrics Log Region (MLOG)

**Block Type Tag:** `0x4D4C4F47` ("MLOG" ASCII)
**Module:** Observability (bit 17)

**Purpose:** Time-series append-only region for internal VDE observability metrics. Provides self-contained monitoring without requiring external observability infrastructure.

**Region header (first block):**

```
Offset  Size  Field                Description
──────  ────  ───────────────────  ──────────────────────────────────────────────────
+0x00   8     RegionMagic          0x4D45545249435300 ("METRICS\0")
+0x08   8     EntryCount           uint64 LE: total metric entries
+0x10   8     OldestTimestamp      uint64 LE: UTC nanoseconds
+0x18   8     NewestTimestamp      uint64 LE: UTC nanoseconds
+0x20   2     MetricIdCount        uint16 LE: number of distinct metric IDs registered
+0x22   4     RetentionSeconds     uint32 LE: retention period (auto-compact after this)
+0x26   2     DownsampleFactor     uint16 LE: downsampling factor for compacted entries
```

**Metric entry format (18 bytes, fixed-width for fast sequential scan):**

```
Offset  Size  Field           Description
──────  ────  ──────────────  ──────────────────────────────────────────────────
+0x00   8     Timestamp       uint64 LE: UTC nanoseconds since epoch
+0x08   2     MetricId        uint16 LE: metric identifier (see well-known IDs below)
+0x0A   8     Value           int64 LE or float64 (interpretation depends on MetricId)
```

**Well-known MetricId values:**

| ID | Name | Unit | Type |
|----|------|------|------|
| 0x0001 | ReadOps | count | int64 |
| 0x0002 | WriteOps | count | int64 |
| 0x0003 | ReadBytes | bytes | int64 |
| 0x0004 | WriteBytes | bytes | int64 |
| 0x0005 | ReadLatencyP50 | nanoseconds | int64 |
| 0x0006 | ReadLatencyP99 | nanoseconds | int64 |
| 0x0007 | WriteLatencyP50 | nanoseconds | int64 |
| 0x0008 | WriteLatencyP99 | nanoseconds | int64 |
| 0x0010 | FreeBlocks | blocks | int64 |
| 0x0011 | FragmentationRatio | ratio (0.0-1.0) | float64 |
| 0x0012 | InodeUtilization | ratio (0.0-1.0) | float64 |
| 0x0020 | CacheHitRatio | ratio (0.0-1.0) | float64 |
| 0x0021 | CompressionRatio | ratio | float64 |
| 0x0030 | ReplicationLag | milliseconds | int64 |
| 0x0031 | DirtyBlocks | blocks | int64 |
| 0x0040 | TamperEvents | count | int64 |
| 0x0041 | ErrorCount | count | int64 |

**Auto-compaction:** When entries older than `RetentionSeconds` exist, the DW engine runs a background compaction:
1. Group old entries by MetricId in windows of `DownsampleFactor` entries.
2. Replace each window with a single entry containing the average value.
3. Free compacted blocks.
4. This keeps the region from growing unboundedly while preserving historical trends.

---

## Additional Design Recommendations

### Emergency Recovery Block

**Block Type Tag:** `0x52435652` ("RCVR" ASCII)
**Location:** Block 9 (FIXED OFFSET, always, regardless of module configuration or Region Directory)

The Emergency Recovery Block exists at a known, hardcoded position so that recovery tools can locate it even if the Superblock, Region Directory, and all metadata are corrupted or encrypted with lost keys.

**Layout (single block, 4080 usable bytes after trailer):**

```
Offset  Size  Field                Description
──────  ────  ───────────────────  ──────────────────────────────────────────────────
+0x00   8     RecoveryMagic        0x4457564452435652 ("DWVDRCVR")
+0x08   16    VdeUUID              UUID: unique identifier for this VDE
+0x18   8     CreationTimestamp    uint64 LE: UTC nanoseconds, VDE creation time
+0x20   2     FormatMajorVersion   uint16 LE: format major version (2)
+0x22   2     FormatMinorVersion   uint16 LE: format minor version (0)
+0x24   4     ModuleManifest       uint32 LE: copy of ModuleManifest at creation time
+0x28   8     TotalBlocks          uint64 LE: total blocks in VDE
+0x30   4     BlockSize            uint32 LE: block size in bytes
+0x34   2     InodeSize            uint16 LE: inode size in bytes
+0x36   2     Reserved             Zero

+0x38   128   AdminContact         UTF-8, null-padded: admin email address and/or phone
                                   number for human contact during disaster recovery
+0xB8   128   OrganizationName     UTF-8, null-padded: organization that owns this VDE
+0x138  64    VolumeLabel          UTF-8, null-padded: human-readable VDE name
+0x178  32    NamespacePrefix      UTF-8, null-padded: dw:// namespace of this VDE

+0x198  256   RecoveryNotes        UTF-8, null-padded: free-form text for recovery
                                   instructions (e.g., "Contact SOC at +1-555-0100.
                                   Backup location: vault-7, tape set DW-2026-042.")

+0x298  32    RecoveryHMAC         HMAC-BLAKE3 over bytes [0x00 .. 0x297]
                                   Key: HKDF(hardware_serial || recovery_passphrase,
                                            salt="dwvd-recovery", info="rcvr-hmac")
```

**Critical properties:**
- **PLAINTEXT**: This block is NEVER encrypted, even when the SEC module encrypts all other regions. It contains no sensitive data — only identification and contact information.
- **FIXED OFFSET**: Always block 9. Recovery tools hardcode this offset. Even if the Superblock is corrupted, the recovery block is locatable.
- **HMAC protection**: The RecoveryHMAC prevents modification by unauthorized parties. The key is derived from hardware-specific information and a recovery passphrase known to the admin.
- **Human-readable**: Contains email, phone, organization, and notes in UTF-8 so that a human examining the raw bytes can identify the VDE owner and recovery procedures.

### Thin Provisioning Support

Thin provisioning allows a VDE to have a logical capacity much larger than its physical allocation. Blocks that have never been written do not consume physical disk space (sparse file holes).

**Superblock fields:**

```
Offset  Size  Field                     Description
──────  ────  ────────────────────────  ──────────────────────────────────────────────────
+0x90   1     ThinProvisioningEnabled   uint8: 0x00=disabled (thick), 0x01=enabled (thin)
+0x91   1     SparseFileHolesEnabled    uint8: 0x00=no, 0x01=yes (OS sparse file support)
+0x92   2     Reserved                  Zero
+0x94   8     PhysicalAllocatedBlocks   uint64 LE: blocks actually written to disk
+0x9C   8     LogicalTotalBlocks        uint64 LE: logical capacity in blocks
```

**Invariants:**
- `PhysicalAllocatedBlocks <= LogicalTotalBlocks`
- `PhysicalAllocatedBlocks <= TotalAllocatedBlks` (from File Size Sentinel)
- When ThinProvisioningEnabled=0: `PhysicalAllocatedBlocks == LogicalTotalBlocks`

**Allocation Bitmap behavior:**
- Bit = 1: block is allocated (has data or metadata)
- Bit = 0: block is free AND may be a sparse hole on the underlying filesystem
- The DW engine uses `fallocate()` / `FSCTL_SET_SPARSE` to create holes for freed blocks
- On read of an unallocated block: return zero-filled block (no disk I/O)

**Use cases:**
- Development: create a 1 TB VDE that initially consumes only ~10 MB of physical disk
- Staging: allocate production-sized VDEs on smaller staging hardware
- Multi-tenant: overcommit capacity across tenants, with alerts when physical utilization exceeds thresholds

**Overcommit monitoring:**
- `OvercommitRatio = LogicalTotalBlocks / PhysicalAllocatedBlocks`
- DW engine alerts when ratio exceeds configurable threshold (default: 10:1)
- Critical alert when physical disk free space < 10% of remaining logical capacity

### Forward Compatibility Markers

Following ext4's proven three-tier feature flag model, these fields enable graceful version evolution.

**Superblock fields:**

```
Offset  Size  Field                          Description
──────  ────  ─────────────────────────────  ──────────────────────────────────────────
+0xA4   2     MinReaderVersion               uint16 LE: minimum DW engine version that
                                              can READ this VDE. Encoded as major*100+minor
                                              (e.g., 200 = v2.0, 215 = v2.15)
+0xA6   2     MinWriterVersion               uint16 LE: minimum DW engine version that
                                              can WRITE to this VDE.
+0xA8   4     IncompatibleFeatureFlags       uint32 LE: features that MUST be understood
                                              to access this VDE. If a reader encounters
                                              an unknown bit set → REFUSE to open.
+0xAC   4     CompatibleFeatureFlags         uint32 LE: features that can be safely IGNORED
                                              by older readers. Unknown bits are harmless.
+0xB0   4     ReadOnlyCompatibleFeatureFlags uint32 LE: features that can be ignored for
                                              READ-ONLY access but MUST be understood for
                                              write access. Unknown bit → open read-only.
```

**IncompatibleFeatureFlags bit assignments:**

| Bit | Flag | Description |
|-----|------|-------------|
| 0 | INCOMPAT_LARGE_INODES | Inode size > 256 bytes (v2.0 always sets this) |
| 1 | INCOMPAT_EXTENT_BASED | Extent-based addressing (v2.0 always sets this) |
| 2 | INCOMPAT_MERKLE_TREE | Integrity Tree uses Merkle (not flat checksum) |
| 3 | INCOMPAT_INLINE_TAGS | Inodes contain InlineTagArea |
| 4 | INCOMPAT_COMPOSABLE | ModuleManifest + InodeLayoutDescriptor present |
| 5 | INCOMPAT_AEAD_BLOCKS | Blocks contain AEAD authentication tags |
| 6 | INCOMPAT_MULTI_RAFT | Multiple consensus log regions |
| 7-31 | Reserved | Future incompatible features |

**CompatibleFeatureFlags bit assignments:**

| Bit | Flag | Description |
|-----|------|-------------|
| 0 | COMPAT_CARBON_SCORE | CarbonScore field in inode (safe to ignore) |
| 1 | COMPAT_QOS_PRIORITY | QoSPriority field in inode (safe to ignore) |
| 2 | COMPAT_METRICS_LOG | MetricsLogRegion present (safe to ignore) |
| 3-31 | Reserved | Future compatible features |

**ReadOnlyCompatibleFeatureFlags bit assignments:**

| Bit | Flag | Description |
|-----|------|-------------|
| 0 | ROCOMPAT_SNAPSHOT | SnapshotTable present (reads OK, writes need CoW awareness) |
| 1 | ROCOMPAT_REPLICATION | ReplicationState present (reads OK, writes need DVV update) |
| 2 | ROCOMPAT_COMPRESSION | DictionaryRegion present (reads need decompression support) |
| 3-31 | Reserved | Future read-only compatible features |

**Decision matrix for opening a VDE:**

```
IncompatibleFeatureFlags has unknown bit?
  └─ Yes → REFUSE TO OPEN. Log error: "VDE requires feature X (bit N) which this
            engine version does not support. Upgrade to DW >= MinReaderVersion."
  └─ No  → Continue.

ReadOnlyCompatibleFeatureFlags has unknown bit?
  └─ Yes → OPEN READ-ONLY. Log warning: "VDE has feature X (bit N) not supported
            for write. Opening read-only. Upgrade to DW >= MinWriterVersion for
            full access."
  └─ No  → Continue.

CompatibleFeatureFlags has unknown bit?
  └─ Yes → OPEN NORMALLY. Log info: "VDE has optional feature X (bit N) which this
            engine version does not utilize. Safe to ignore."
  └─ No  → OPEN NORMALLY.
```

### VDE Health & Lifecycle Metadata

Stored in Superblock Block 0, these fields track the health and lifecycle of the VDE, enabling preventive maintenance and error tracking.

```
Offset  Size  Field                Description
──────  ────  ───────────────────  ──────────────────────────────────────────────────
+0xB4   8     CreationTimestamp    uint64 LE: UTC nanoseconds, VDE creation time
+0xBC   8     LastMountTimestamp   uint64 LE: UTC nanoseconds, last time VDE was opened
+0xC4   4     MountCount           uint32 LE: total number of times this VDE has been opened
+0xC8   4     MaxMountCount        uint32 LE: after this many mounts, force integrity scrub.
                                   Default: 1000. Set to 0 to disable.
+0xCC   8     LastScrubTimestamp   uint64 LE: UTC nanoseconds, last integrity scrub completion
+0xD4   8     LastDefragTimestamp  uint64 LE: UTC nanoseconds, last defragmentation completion
+0xDC   4     ErrorCount           uint32 LE: cumulative correctable errors detected.
                                   Incremented on: checksum mismatch (corrected from mirror),
                                   Merkle path verification failure (corrected from parity),
                                   torn write recovery (via WAL replay).
+0xE0   1     VdeState             uint8: current VDE state:
                                     0x00 = CLEAN    — last unmount was clean
                                     0x01 = DIRTY    — VDE is currently mounted or was not
                                                       cleanly unmounted (crash recovery needed)
                                     0x02 = ERROR    — errors detected but VDE is operational
                                     0x03 = RECOVERING — WAL replay or scrub in progress
                                     0x04 = FORENSIC — tamper detected, read-only forensic mode
+0xE1   3     Reserved             Zero
+0xE4   4     ScrubIntervalHours   uint32 LE: recommended scrub interval in hours.
                                   Default: 168 (weekly). 0 = no auto-scrub.
```

**Mount/unmount protocol:**
1. **On mount**: Set `VdeState = DIRTY`, increment `MountCount`, update `LastMountTimestamp`. Flush superblock.
2. **On clean unmount**: Set `VdeState = CLEAN`. Flush superblock.
3. **On next mount if VdeState == DIRTY**: Previous session crashed. Trigger WAL replay, then integrity scrub.
4. **On MountCount >= MaxMountCount**: Force full integrity scrub before allowing normal operations. Reset MountCount to 0 after scrub.

**Error escalation:**
- `ErrorCount < 10`: normal operation, log info.
- `ErrorCount 10-99`: log warnings, recommend scrub.
- `ErrorCount >= 100`: set VdeState = ERROR, alert admin, recommend backup and rebuild.

### VDE Nesting (VDE within VDE)

VDE nesting enables multi-tenant isolation by embedding a complete VDE as an object inside an outer VDE. This provides defense-in-depth: each layer has its own encryption, access control, and integrity verification.

**Superblock fields for nesting:**

```
Offset  Size  Field                Description
──────  ────  ───────────────────  ──────────────────────────────────────────────────
+0xE8   16    ParentVdeUUID        UUID: if this VDE is nested inside another VDE,
                                   this is the parent's VdeUUID. All zeros if top-level.
+0xF8   1     NestingDepth         uint8: 0 = top-level, 1 = nested inside one VDE,
                                   2 = nested two levels deep. Maximum: 3.
+0xF9   1     NestingFlags         uint8:
                                     Bit 0: INHERIT_ENCRYPTION (inner VDE inherits
                                            outer VDE's encryption key as additional
                                            key wrap layer)
                                     Bit 1: INHERIT_ACL (inner VDE's ACL is AND'd
                                            with outer VDE's ACL — never more permissive)
                                     Bit 2: ISOLATED_NAMESPACE (inner VDE has its own
                                            dw:// namespace, independent of outer)
                                     Bits 3-7: Reserved
+0xFA   2     Reserved             Zero
+0xFC   4     InnerVdeInodeNumber  uint32 LE: inode number of this VDE's object in the
                                   parent VDE (for reverse lookup). 0 if top-level.
```

**Nesting model:**

```
┌─────────────────────────────────────────────────────────────┐
│  Outer VDE (top-level, NestingDepth=0)                      │
│  VdeUUID: aaaa-aaaa                                         │
│  Encryption: AES-256-GCM (key A)                            │
│                                                             │
│  ┌───────────────────────────────────────────────────────┐  │
│  │  Inner VDE (NestingDepth=1)                           │  │
│  │  VdeUUID: bbbb-bbbb                                   │  │
│  │  ParentVdeUUID: aaaa-aaaa                             │  │
│  │  Encryption: ChaCha20-Poly1305 (key B)                │  │
│  │  Stored as: inode #4721 in outer VDE (type=FILE)      │  │
│  │                                                       │  │
│  │  ┌─────────────────────────────────────────────────┐  │  │
│  │  │  Innermost VDE (NestingDepth=2)                 │  │  │
│  │  │  VdeUUID: cccc-cccc                             │  │  │
│  │  │  ParentVdeUUID: bbbb-bbbb                       │  │  │
│  │  │  Encryption: AES-256-XTS (key C)                │  │  │
│  │  │  Data encrypted with: key A → key B → key C     │  │  │
│  │  │  (triple envelope encryption)                   │  │  │
│  │  └─────────────────────────────────────────────────┘  │  │
│  └───────────────────────────────────────────────────────┘  │
└─────────────────────────────────────────────────────────────┘
```

**Use cases:**
- **Tenant isolation**: Each tenant's data lives in an inner VDE. The outer VDE provides infrastructure-level encryption and access control. Even if the outer VDE's admin key is compromised, tenant data remains encrypted with the tenant's own keys.
- **Defense-in-depth**: Military/intelligence deployments with multi-layer classification. TOP SECRET outer, SECRET inner, CONFIDENTIAL innermost.
- **Portable workspaces**: An inner VDE can be extracted from the outer VDE and transferred as a standalone file. It remains a valid, fully self-describing DWVD.

**Constraints:**
- Maximum nesting depth: 3 (to prevent performance degradation from recursive I/O amplification).
- Each nesting level adds ~1 I/O amplification factor (read inner block = read outer extent → locate inner block → read inner block).
- Inner VDEs MUST have `NestingDepth = parent.NestingDepth + 1`. Violation → refuse to mount.
- Inner VDE's `ParentVdeUUID` MUST match the outer VDE's `VdeUUID`. Mismatch → inner VDE was moved to a different outer VDE, which may be legitimate (portability) or suspicious (tampering). Log warning.

---

---

## File Extension & OS Registration

### Primary Extension: `.dwvd`

**DataWarehouse Virtual Disk** — matches the format acronym (DWVD), parallels the industry convention where the extension matches the format name (.vhd → VHD, .vmdk → VMDK, .vhdx → VHDX, .dwvd → DWVD).

### File Identification Stack

| Layer | Identifier | Value | Purpose |
|-------|-----------|-------|---------|
| **File extension** | `.dwvd` | — | OS-level file type association |
| **Magic bytes** | Offset 0x00-0x03 | `44 57 56 44` ("DWVD") | Binary format detection (libmagic, `file` command) |
| **Namespace anchor** | Offset 0x08-0x0C | `64 77 3A 2F 2F` ("dw://") | DataWarehouse ecosystem identification |
| **MIME type** | — | `application/x-datawarehouse-vdisk` | HTTP, email, browser, and API content negotiation |
| **IANA registration** | — | `application/vnd.datawarehouse.dwvd` | Formal vendor MIME type (submit to IANA) |
| **macOS UTI** | — | `com.datawarehouse.dwvd` | macOS Uniform Type Identifier (conforms to `public.disk-image`) |
| **Windows ProgID** | — | `DataWarehouse.VirtualDisk.2` | Windows shell integration (versioned) |
| **Linux desktop** | — | `application-x-datawarehouse-vdisk.xml` | freedesktop.org shared MIME info |

### OS Registration Details

#### Windows
```xml
<!-- Registry entries for .dwvd file association -->
HKEY_CLASSES_ROOT\.dwvd
  (Default) = "DataWarehouse.VirtualDisk.2"
  Content Type = "application/vnd.datawarehouse.dwvd"
  PerceivedType = "system"

HKEY_CLASSES_ROOT\DataWarehouse.VirtualDisk.2
  (Default) = "DataWarehouse Virtual Disk"
  FriendlyTypeName = "@dw.dll,-100"

  \DefaultIcon
    (Default) = "dw.dll,0"

  \shell\open\command
    (Default) = "\"C:\Program Files\DataWarehouse\dw.exe\" mount \"%1\""

  \shell\info\command
    (Default) = "\"C:\Program Files\DataWarehouse\dw.exe\" inspect \"%1\""

  \shell\verify\command
    (Default) = "\"C:\Program Files\DataWarehouse\dw.exe\" verify \"%1\""
```

Hyper-V integration: Register as a recognized virtual disk format via `virtdisk.dll` provider interface. Enables "Attach VHD" in Disk Management to recognize `.dwvd` files when DW is installed.

#### Linux
```xml
<!-- /usr/share/mime/packages/datawarehouse-dwvd.xml -->
<?xml version="1.0" encoding="UTF-8"?>
<mime-info xmlns="http://www.freedesktop.org/standards/shared-mime-info">
  <mime-type type="application/x-datawarehouse-vdisk">
    <comment>DataWarehouse Virtual Disk</comment>
    <comment xml:lang="en">DataWarehouse Virtual Disk Image</comment>
    <magic priority="60">
      <match type="string" offset="0" value="DWVD"/>
    </magic>
    <glob pattern="*.dwvd"/>
    <sub-class-of type="application/octet-stream"/>
  </mime-type>
</mime-info>
```

`/etc/magic` entry for `file` command:
```
0       string  DWVD    DataWarehouse Virtual Disk
>4      leshort x       (format version %d
>6      leshort x       .%d)
```

FUSE auto-mount: `udev` rule to auto-invoke `dw mount` when a `.dwvd` file is detected on a removable device (USB air-gap scenario).

#### macOS
```xml
<!-- Info.plist UTI declaration -->
<key>UTImportedTypeDeclarations</key>
<array>
  <dict>
    <key>UTTypeIdentifier</key>
    <string>com.datawarehouse.dwvd</string>
    <key>UTTypeDescription</key>
    <string>DataWarehouse Virtual Disk</string>
    <key>UTTypeConformsTo</key>
    <array>
      <string>public.disk-image</string>
      <string>public.data</string>
    </array>
    <key>UTTypeTagSpecification</key>
    <dict>
      <key>public.filename-extension</key>
      <array><string>dwvd</string></array>
      <key>public.mime-type</key>
      <string>application/vnd.datawarehouse.dwvd</string>
    </dict>
  </dict>
</array>
```

### Secondary Extensions

| Extension | Purpose | When Used |
|-----------|---------|-----------|
| `.dwvd` | Primary VDE container | Always (production data) |
| `.dwvd.tmp` | In-progress VDE creation | During `dw create` (renamed to `.dwvd` on completion) |
| `.dwvd.bak` | Backup copy before migration | During `dw migrate` (auto-cleaned after verification) |
| `.dwvd.wal` | External WAL overflow | Only if WAL exceeds VDE-internal allocation (rare) |
| `.dwvd.snap` | Exported snapshot | `dw snapshot export` (standalone, mountable) |
| `.dwvd.delta` | Delta/diff for replication | `dw replicate export-delta` (applied via `dw replicate apply-delta`) |
| `.dwvd.meta` | Detached metadata sidecar | Only for VDEs with separated metadata (VDE1=data, VDE2=metadata) |
| `.dwvd.lock` | Mount lock file | Prevents concurrent mount from multiple DW instances |

### Content Detection Priority

When DW encounters a file, it identifies the format in this order:

```
1. Read bytes 0x00-0x03: Must be "DWVD" (0x44575644)
   → If not: reject ("Not a DataWarehouse Virtual Disk")

2. Read bytes 0x04-0x05: Format version (major.minor)
   → If major > supported: reject ("VDE format version %d not supported, upgrade DW")
   → If major == supported but minor > supported: open read-only with warning

3. Read bytes 0x08-0x0C: Namespace anchor "dw://"
   → If not present: warn ("Legacy DWVD without namespace, limited functionality")

4. Read IncompatibleFeatureFlags from Superblock:
   → If any unknown bits set: reject ("VDE uses features not supported by this DW version")

5. Read ReadOnlyCompatibleFeatureFlags:
   → If unknown bits set: open read-only

6. Read CompatibleFeatureFlags:
   → If unknown bits set: open normally (safe to ignore)

7. Verify Header Integrity Seal:
   → If fails: respond per TamperResponse policy
```

### Non-DWVD File Handling

When someone tries to mount a non-DWVD file (e.g., VHDX, VMDK) with DW:

```
$ dw mount image.vhdx

ERROR: "image.vhdx" is not a DataWarehouse Virtual Disk.
  Detected format: Microsoft VHDX (Virtual Hard Disk v2)

  To use this file with DataWarehouse:
    dw import --from vhdx --to production.dwvd image.vhdx

  This will create a new DWVD container and import all data.
  Original file will not be modified.

  Supported import formats: VHD, VHDX, VMDK, QCOW2, VDI, RAW, IMG
```

Conversely, when someone tries to mount a `.dwvd` with Hyper-V/VMware:

- Without DW installed: OS shows "Unknown format" or "Cannot mount"
- With DW installed: DW's registered shell handler intercepts and mounts via DW engine
- The DWVD file is NOT a valid VHDX/VMDK — it won't accidentally be interpretable as another format due to the unique magic bytes

---

*End of DWVD v2.0 format specification. This document defines the complete format, composable architecture, OS integration, and will be cross-referenced with the implementation roadmap during milestone planning.*
