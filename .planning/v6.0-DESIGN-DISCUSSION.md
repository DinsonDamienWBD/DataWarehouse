# v6.0 Intelligent Policy Engine — Design Discussion

**Date:** 2026-02-20
**Status:** Pre-milestone discussion (requirements not yet formalized)

---

## 1. Core Concept

Transform every applicable feature (94 identified across 8 categories) into a multi-level, cascade-aware, AI-tunable policy with performance optimization. Not a dumb policy enforcer — an intelligent engine that auto-tunes, predicts, and optimizes.

## 2. Multi-Level Hierarchy

```
VDE Instance          <- drive-level (BitLocker analogy)
  +-- Container       <- partition/volume-level
       +-- Object/File <- file-level
            +-- Chunk  <- extent-level
                 +-- Block <- sector-level
```

### VDE Architecture Context

Each DW instance runs a VDE — a single flat binary container file (.dwvd format, DWVD magic). The container has a fixed block layout:
Superblock -> Bitmap -> Inode Table -> WAL -> Checksum Table -> B-Tree Index -> Data Region

### Index/Metadata Separation Across VDEs

Currently, the B-Tree index and inode table (metadata) live inside the same VDE container as the data. The IMetadataIndex (semantic search) lives outside the VDE as a separate plugin.

**Proposal:** Make index/metadata location user-configurable:
- Default: all in same VDE (traditional)
- Separation options: data+index on VDE1, metadata on VDE2 / data on VDE1, index on VDE2, metadata on VDE3 / any combination

**Pros:** Performance (NVMe index + spinning data), scaling (index replicated independently), DR (faster recovery)
**Cons:** Cross-VDE consistency (2PC/saga needed), latency (network hops), atomicity (multi-VDE transactions)

### VDE-Level Governance

All features (tags, ACLs, metadata, encryption, etc.) can be applied to the VDE itself, not just its contents. A VDE tagged `region:eu-only` physically cannot operate outside EU. This enables:
- Sovereignty at infrastructure level
- Simplified compliance auditing
- Zero-trust at container level
- Portable governance

---

## 3. Feature Inventory (94 Features, 8 Categories)

### Summary Matrix

| Feature | Block | Chunk | Object | Container | VDE | Multi-Level Value |
|---------|:-----:|:-----:|:------:|:---------:|:---:|:-----------------:|
| **A. SECURITY** | | | | | | |
| A1. Encryption algorithm | YES | - | YES | YES | YES | HIGH |
| A2. Key management | - | - | YES | YES | YES | HIGH |
| A3. Access control (ACL) | - | - | YES | YES | YES | HIGH |
| A4. Audit trail config | - | - | YES | YES | YES | HIGH |
| A5. Auth requirements | - | - | - | YES | YES | MEDIUM |
| A6. Transit encryption | - | - | - | YES | YES | MEDIUM |
| A7. Data integrity (hash) | YES | YES | YES | YES | YES | HIGH |
| A8. WORM / Immutability | - | - | YES | YES | YES | HIGH |
| A9. Zero Trust policy | - | - | - | YES | YES | LOW |
| **B. DATA PROTECTION** | | | | | | |
| B1. Compression | YES | YES | YES | YES | YES | HIGH |
| B2. Deduplication | YES | - | YES | YES | YES | HIGH |
| B3. Checksums | YES | YES | YES | YES | YES | HIGH |
| B4. RAID / Erasure coding | YES | YES | YES | YES | YES | HIGH |
| B5. Error correction | YES | YES | YES | - | - | MEDIUM |
| B6. Secure delete | YES | - | YES | YES | YES | HIGH |
| **C. GOVERNANCE** | | | | | | |
| C1. Tags | - | - | YES | YES | YES | HIGH |
| C2. Tag policies | - | - | YES | YES | YES | HIGH |
| C3. Data classification | - | - | YES | YES | YES | HIGH |
| C4. Compliance framework | - | - | YES | YES | YES | HIGH |
| C5. Data sovereignty | - | - | YES | YES | YES | HIGH |
| C6. Retention policy | - | - | YES | YES | YES | HIGH |
| C7. Legal hold | - | - | YES | YES | YES | HIGH |
| C8. Data masking | - | - | YES | YES | YES | MEDIUM |
| C9. Consent tracking | - | - | YES | YES | YES | LOW |
| C10. Data lineage | - | - | YES | YES | YES | MEDIUM |
| **D. PERFORMANCE** | | | | | | |
| D1. Storage tiering | - | - | YES | YES | YES | HIGH |
| D2. Caching policy | - | - | YES | YES | YES | HIGH |
| D3. Prefetch | YES | - | YES | YES | YES | MEDIUM |
| D4. QoS / Throttling | - | - | YES | YES | YES | HIGH |
| D5. I/O priority | - | - | YES | YES | YES | MEDIUM |
| D6. Block size | - | - | - | YES | YES | LOW |
| **E. REPLICATION** | | | | | | |
| E1. Replication mode | - | - | YES | YES | YES | HIGH |
| E2. Replication factor | - | - | YES | YES | YES | HIGH |
| E3. Geo-placement | - | - | YES | YES | YES | HIGH |
| E4. Sharding | - | - | - | YES | YES | MEDIUM |
| E5. Federation | - | - | YES | YES | YES | MEDIUM |
| E6. Conflict resolution | - | - | YES | YES | YES | MEDIUM |
| **F. LIFECYCLE** | | | | | | |
| F1. Versioning | - | - | YES | YES | YES | HIGH |
| F2. Snapshots | - | - | YES | YES | YES | HIGH |
| F3. Archival | - | - | YES | YES | YES | HIGH |
| F4. Deletion policy | - | - | YES | YES | YES | HIGH |
| F5. Time-lock | - | - | YES | YES | YES | HIGH |
| F6. TTL / Expiration | - | - | YES | YES | YES | HIGH |
| **G. INTELLIGENCE** | | | | | | |
| G1. AI classification | - | - | YES | YES | YES | MEDIUM |
| G2. Anomaly detection | - | - | - | YES | YES | MEDIUM |
| G3. Heat map scoring | - | - | YES | YES | YES | MEDIUM |
| G4. Content processing | - | - | YES | YES | YES | MEDIUM |
| G5. Predictive replication | - | - | - | YES | YES | LOW |
| **H. OTHER** | | | | | | |
| H1. Quotas | - | - | YES | YES | YES | HIGH |
| H2. Cost allocation | - | - | YES | YES | YES | MEDIUM |
| H3. Notifications | - | - | YES | YES | YES | MEDIUM |
| H4. Storage backend | - | - | - | YES | YES | HIGH |
| H5. Pipeline config | - | - | YES | YES | YES | MEDIUM |

**Features at ALL 5 levels (Block through VDE):** Encryption, Compression, Checksums, RAID/Erasure, Deduplication
**Features at Object/Container/VDE (3 levels):** 38 features (majority)
**Features at Container/VDE only (2 levels):** Auth, TLS, Sharding, Backend, Block size, Anomaly detection

---

## 4. Cascade Resolution Model

### Resolution Strategies by Category

| Category | Default Strategy | Rationale |
|----------|-----------------|-----------|
| Security | MostRestrictive | Child tightens, never loosens |
| Data Protection | MostRestrictive | Integrity guarantees cascade down |
| Governance | Merge + Inherit | Child inherits parent tags AND adds own |
| Compliance | Enforce | Parent retention/legal-hold cannot be shortened |
| Performance | Override | Child picks its own settings (hot data != cold data) |
| Replication | Inherit + Override | VDE default, containers can specialize |
| Lifecycle | MostRestrictive | Longest retention wins, legal hold absolute |

### Cascade Strategy Enum

```
MostRestrictive — Take highest intensity from all levels
Enforce         — Parent value is absolute, children CANNOT override
Inherit         — Child uses parent unless explicitly set
Override        — Closest to data wins
Merge           — Union of all levels (governance/tags)
```

All cascade strategies are themselves user-configurable per feature.

### Edge Cases Addressed

1. **Enforce vs Override conflict:** Enforce at higher level ALWAYS wins over Override at lower level
2. **Empty intermediate levels:** Skipped in resolution (Object inherits from VDE, skipping empty Container)
3. **Merge conflicts (governance):** Per-tag-key conflict resolver (configurable: MostRestrictive, Closest, Union)
4. **Policy change during active operation:** In-flight operations use policy snapshot from start time. New policy applies to next operation. Double-buffered versioned cache.
5. **Circular references:** Detected during policy store validation, rejected

---

## 5. Operational Profiles (The "Slider")

### Named Presets

```
MaxPerformance <------------------------------> MaxSecurity
    |         |         |         |         |
  "Speed"  "Balanced" "Standard" "Strict" "Paranoid"
```

### Profile Settings by Feature

| Feature | Speed | Balanced | Standard | Strict | Paranoid |
|---------|-------|----------|----------|--------|----------|
| Encryption | None | AES-128-CTR | AES-256-GCM | AES-256-GCM+HMAC | PQ-Hybrid+AES-256 |
| Compression | LZ4 (fast) | Snappy | Zstd-3 | Zstd-9 | Zstd-19+integrity |
| Checksum | CRC32 | XXHash64 | SHA-256 | SHA-3-256 | BLAKE3+dual-verify |
| Dedup | None | Block async | Chunk sync | Chunk+content | Full+verified |
| ACL check | VDE-level only | Container+Object | Full cascade | Full+audit | Full+audit+alert |
| Tag eval | Skip | VDE-level only | VDE+Container | Full cascade | Full+policy-engine |
| Replication | Async 1x | Async 2x | Sync 2x | Sync 3x | Sync 3x+verify |
| Versioning | None | Last-5 | All+30d | All+1yr | All+immutable |

### Per-Feature Intensity Configuration

Each feature has a spectrum of intensity levels with associated cost and protection scores:

```
FeaturePolicy {
  featureId: "Encryption"
  intensityLevels: [
    { level: 0, name: "None",        perfCost: 0.0,  protectionScore: 0 },
    { level: 1, name: "AES-128-CTR", perfCost: 0.05, protectionScore: 60 },
    { level: 2, name: "AES-256-GCM", perfCost: 0.12, protectionScore: 85 },
    { level: 3, name: "ChaCha20+HMAC", perfCost: 0.15, protectionScore: 90 },
    { level: 4, name: "PQ-Hybrid",   perfCost: 0.35, protectionScore: 100 }
  ]
}
```

### Per-Level Profile Override

Different levels can run different profiles:

```
VDE: profile=Strict (compliance VDE)
  +-- Container "hot-cache": profile=Speed (perf override)
  +-- Container "audit-trail": profile=Paranoid (untouchable)
  +-- Container "general": profile=inherit (uses VDE's Strict)
```

---

## 6. Performance Optimization Engine

### Check Classification

| Timing | Features | Frequency |
|--------|----------|-----------|
| CONNECT_TIME | VDE ACL, auth/MFA, sovereignty, VDE tags | Once per connection |
| SESSION_CACHED | Container ACL, profile resolution, intensity levels | Once per session |
| PER_OPERATION | Object ACL (if exists), encrypt/decrypt, compress/decompress, checksum | Every I/O |
| DEFERRED | Audit log, AI classification, dedup, tag policy, anomaly detection, async replication | Background |
| PERIODIC | Checksum scrub, compliance re-eval, heat-map recalc, retention enforcement | Scheduled |

### Three-Tier Fast Path

**Tier 1: VDE_ONLY (compiled constant)**
- When: No container or object overrides exist (70-80% of deployments)
- Cost: 0ns (JIT-inlined constant return)
- Implementation: Generated sealed class with direct method calls

**Tier 2: CONTAINER_STOP (cached lookup)**
- When: Containers have overrides but objects don't (15-25%)
- Cost: ~20ns (dictionary lookup)
- Implementation: Pre-computed per-container compiled policies

**Tier 3: FULL_CASCADE (bloom + resolve)**
- When: Objects have overrides (5-10%)
- Cost: ~5ns bloom filter + ~200ns full cascade (1% of ops hit full path)
- Implementation: Bloom filter per VDE for "has override?" + full resolution

### Materialized Policy Cache

Pre-computed effective policies stored in memory at VDE open time. Versioned for in-flight operation safety. Double-buffered swap on policy changes (no operations blocked during recompilation).

### Bloom Filter Skip Index

Maintains per-VDE bloom filter tracking which containers/objects have custom overrides:
- On write with custom policy: add to bloom filter
- On read: if bloom says "no override", skip cascade (guaranteed correct)
- False positives (~1%) cause unnecessary but correct cascade walk
- False negatives: impossible

### Compiled Policy Delegates

For hot-path features (encryption, compression, checksum), policy resolution is JIT-compiled into direct delegates at VDE open time. Recompiled only on policy changes (rare admin operation).

### Performance Benchmarks (Expected)

| Scenario | Per-Op Overhead | Total Hot-Path |
|----------|----------------|----------------|
| Stock Ticker (Speed) | ~12us/KB | LZ4 + CRC32 + cached ACL |
| Standard Enterprise | ~50us/KB | Zstd-3 + AES-256 + SHA-256 + cascade ACL |
| Military (Paranoid) | ~590us/KB | Zstd-19 + PQ-Hybrid + BLAKE3 + sync 3x + full audit |

---

## 7. AI Policy Intelligence

### AI Governance Autonomy Model

#### Autonomy Level Enum (per-feature, per-level)

```
ManualOnly     — AI has no involvement
Suggest        — AI produces recommendations, shown in dashboard
SuggestExplain — AI suggests with full rationale chain
AutoNotify     — AI applies changes, admin notified immediately
AutoSilent     — AI applies changes, logged but no notification
```

### AI Awareness Inputs

1. **Hardware awareness** — CPU capabilities (AES-NI, AVX-512), RAM, storage speed, thermal throttling. Example: "PQ-Hybrid needs X FLOPS, your hardware delivers 0.7X, recommending AES-256-GCM+HMAC"
2. **Data sensitivity awareness** — Observes actual data patterns. Example: "VDE-2 receives 98% non-PII office docs, Paranoid profile costs 340us/op, recommending Standard (45us/op)"
3. **Infrastructure change detection** — Hardware upgrade/network change. Example: "New NVMe tier detected, recommending block-level dedup (previously disabled)"
4. **Workload pattern awareness** — Time-of-day, seasonal, burst. Example: "Trading hours 12x writes, switching LZ4 during peak, Zstd-9 overnight"
5. **Threat awareness** — Anomaly feeds into policy tightening. Example: "4,000 sequential reads in 30s, escalating ACL to per-operation check"
6. **Cost awareness** — Cloud billing. Example: "PQ-Hybrid costs $0.012/GB, AES-256 costs $0.002/GB, switching non-classified saves $14K/mo"

### AI Observation Pipeline (Zero Hot-Path Impact)

```
Data Path (hot - zero AI overhead):
  Write -> Compress -> Encrypt -> Checksum -> Store
                                     |
                                     v (async, non-blocking)
                               MetricsBuffer (lock-free ring buffer)
                                     |
                                     v (background thread, every N seconds)
                               ObservationAggregator
                                     |
                    +----------------+----------------+
                    v                v                v
             HardwareProbe    WorkloadAnalyzer  ThreatDetector
                    |                |                |
                    +----------------+----------------+
                                     v
                               PolicyAdvisor (AI)
                                     |
                                     v
                          PolicyRecommendation
                                     |
                           +---------+---------+
                           v         v         v
                       AutoApply  Suggest   Log Only
```

Performance budget: configurable maxCpuOverhead (default 1%), auto-throttle if exceeded.

### Hybrid Configuration Model

```
Global AI Policy:
  autonomyLevel: Suggest          <- default for everything

  overrides:
    security.*:
      autonomyLevel: Manual        <- never touch security
      aiCanObserve: true           <- but CAN monitor and alert
      aiCanSuggest: false          <- don't even suggest changes

    performance.*:
      autonomyLevel: AutoNotify    <- optimize freely, just tell me
      adjustmentBounds:
        maxDegradation: 10%        <- never reduce protection >10%
        maxCostIncrease: 20%       <- never increase compute >20%

    compression.*:
      autonomyLevel: AutoSilent    <- full auto, don't bother me

    lifecycle.retention:
      autonomyLevel: Manual        <- compliance, don't touch

    replication:
      autonomyLevel: Suggest       <- suggest but I decide
      aiCanEscalate: true          <- except during detected failures
```

---

## 8. Authority Chain and Override Hierarchy

### Three-Level Authority

```
Level 1: Admin Policy (normal administrator configuration)
Level 2: AI Policy (auto-tune/emergency, subject to autonomy permissions)
Level 3: Super Admin Quorum (overrides EVERYTHING, AI CANNOT reverse)
```

### Emergency Escalation Protocol

1. ThreatDetector raises ALERT
2. Check: Does feature allow aiEmergencyEscalation?
3. If yes: AI applies EscalationPolicy (time-bounded, default 15 minutes)
4. Notify ALL admins immediately
5. Countdown timer:
   - Admin confirms -> escalation becomes permanent
   - Admin reverts -> original policy restored
   - Timer expires -> auto-reverts, incident logged

Each escalation generates an immutable EscalationRecord with tamper-proof hash.

### Super Admin Quorum (Multi-Key Authorization)

Like nuclear launch keys — multiple super admins must unanimously agree:

```
QuorumPolicy:
  requiredApprovals: 3     (e.g., 3-of-5 must agree)
  totalSuperAdmins: 5
  approvalWindow: 1 hour   (time to collect approvals)

  Actions requiring quorum:
    - Override AI escalation
    - Change security policies
    - Disable AI entirely
    - Modify quorum rules themselves
    - Grant/revoke super admin role
    - Delete VDE
    - Export encryption keys
    - Disable audit trail
```

### Defense Against Compromised Super Admins

1. **Quorum itself** — Compromising one isn't enough
2. **Hardware tokens** — MFA required (YubiKey, smart card)
3. **Geographic distribution** — Approvals from different network segments
4. **Time-lock on destructive actions** — 24hr cooling-off, ANY super admin can veto
5. **Dead man's switch** — No super admin activity for N days -> auto-lock to max security
6. **Immutable audit trail** — TamperProof pipeline, cryptographically chained
7. **Hardware root of trust (HSM)** — Physical presence for catastrophic changes ("break glass")

### Complete Authority Resolution Order

```
1. Super Admin quorum override? -> Use it. Done. (AI cannot override)
2. Active AI emergency escalation? -> Use it. (Unless Admin/SuperAdmin reverts)
3. Admin policy? -> Use it. (Subject to AI autonomy level)
4. System defaults. (Always present)
```

---

## 9. SDK Architecture

### DECISION: Policy Lives at Plugin Level, Not Strategy Level (Corrected)

The **policy** says WHAT to achieve. The **plugin** decides HOW (selects best strategy). The **strategy** executes.

| Layer | Responsibility | Example |
|-------|---------------|---------|
| **Policy** | What must be achieved | "Encryption: intensity >= 4, protectionScore >= 90" |
| **Plugin** | Select best strategy that satisfies policy | UltimateEncryptionPlugin evaluates hardware, picks best fit |
| **Strategy** | Execute the actual work | Aes256GcmStrategy.Encrypt(data) |

The strategy doesn't need to know about policies. The plugin is the intelligent broker:

```
UltimateEncryptionPlugin:
  policyRequirement: intensity >= 4
  availableStrategies: [
    { strategy: Aes128Ctr,    intensity: 1, protectionScore: 60, hwReq: low },
    { strategy: Aes256Gcm,    intensity: 2, protectionScore: 85, hwReq: medium },
    { strategy: ChaCha20Hmac, intensity: 3, protectionScore: 90, hwReq: medium },
    { strategy: PqHybrid,     intensity: 4, protectionScore: 100, hwReq: high }
  ]

  selectedStrategy = availableStrategies
    .Where(s => s.intensity >= policy.minIntensity)
    .Where(s => hardware.CanSupport(s.hwRequirement))
    .OrderByDescending(s => s.protectionScore)
    .First();
```

### DECISION: AI Integration Through Plugin Hierarchy

```
PluginBase (SDK)
  HAS: PolicyContext (every plugin is policy-aware)
  HAS: PolicyRequirements (what policy demands of this plugin)
  DOES: Reads effective policy, selects strategies accordingly
  |
  +-- IntelligenceAwarePluginBase (SDK)
  |     HAS: IAiHook (connection point for AI observation + tuning)
  |     HAS: ObservationEmitter (sends metrics to ring buffer)
  |     HAS: RecommendationReceiver (gets AI suggestions)
  |     DOES: Allows AI to observe plugin behavior and suggest/apply changes
  |     |
  |     +-- UltimateEncryptionPlugin (inherits IntelligenceAware)
  |     |     AI can observe: cipher overhead, failure rates, hardware utilization
  |     |     AI can suggest: "switch to ChaCha20 on this ARM container"
  |     |
  |     +-- UltimateCompressionPlugin (inherits IntelligenceAware)
  |     |     AI can observe: compression ratios, CPU cost, content patterns
  |     |     AI can suggest: "this container is mostly JPEG, skip compression"
  |     |
  |     +-- UltimateRAIDPlugin, UltimateReplicationPlugin, ... (all others)
  |
  +-- UltimateIntelligencePlugin (inherits PluginBase DIRECTLY, NOT IntelligenceAware)
        DOES NOT have IAiHook (prevents AI-observing-AI loop)
        HAS: PolicyContext (admin fully controls AI behavior via policy)
        Policy is ALWAYS ManualOnly by default
        Only admin/super-admin can change UltimateIntelligence's policy
        allowSelfModification: false (AI cannot change its own config)
        modificationRequiresQuorum: true
```

**Why UltimateIntelligence inherits PluginBase, not IntelligenceAwarePluginBase:**
- Prevents AI observing itself observing itself (infinite loop)
- Prevents AI tuning its own observation frequency
- Prevents AI suggesting changes to its own autonomy level (self-promotion attack)
- AI is the observer, not the observed. It reads hooks from IntelligenceAware plugins.

### DECISION: With Quorum Failsafe, AI Can Manage ALL Policies

Because the quorum can always override AI, the risk calculus changes. Default for high-security:

```
AI Autonomy Defaults (with quorum active):
  security.*:        SuggestExplain    (AI suggests security improvements with rationale)
  compliance.*:      SuggestExplain    (AI tracks CVEs, suggests fixes)
  performance.*:     AutoNotify        (AI optimizes freely, notifies)
  lifecycle.*:       Suggest           (AI suggests, human decides)
  replication.*:     AutoNotify        (AI adjusts replication based on failures)
  governance.*:      Suggest           (AI suggests tag/classification changes)
  intelligence.*:    ManualOnly        (ALWAYS - no self-modification)
```

### DECISION: Persistence Layer is Pluggable, Not TamperProof-Mandatory

```csharp
public interface IPolicyPersistence
{
    Task SavePolicyAsync(FeaturePolicy policy);
    Task<FeaturePolicy?> LoadPolicyAsync(string featureId, PolicyLevel level);
    Task SaveAuditRecordAsync(PolicyAuditRecord record);
    Task SaveEscalationRecordAsync(EscalationRecord record);
    Task SaveQuorumRecordAsync(QuorumRecord record);
}
```

Strategy-pattern implementations:

| Implementation | Use Case |
|---------------|----------|
| InMemoryPolicyPersistence | Testing, ephemeral deployments |
| FilePolicyPersistence | Single-node, writes to VDE superblock or sidecar |
| DatabasePolicyPersistence | Multi-node with replication |
| TamperProofPolicyPersistence | Blockchain-backed, immutable audit chain |
| HybridPolicyPersistence | Policies in DB, audit trail in TamperProof |

Configuration:
```json
{
  "policyEngine": {
    "persistence": {
      "policyStore": "Database",
      "auditStore": "TamperProof",
      "escalationStore": "TamperProof",
      "quorumStore": "TamperProof"
    }
  }
}
```

**Compliance validation:** If a compliance framework (HIPAA, SOC2) is active but auditStore is "File", the engine rejects the configuration: "HIPAA requires immutable audit trails. Change auditStore to TamperProof or remove HIPAA framework."

### New SDK Contracts

```
DataWarehouse.SDK/PolicyEngine/
  IPolicyEngine.cs           -- core resolution interface
  IEffectivePolicy.cs        -- resolved policy at a path
  IPolicyStore.cs            -- persistence of policy configs
  IPolicyPersistence.cs      -- pluggable persistence (NOT TamperProof-mandatory)
  FeaturePolicy.cs           -- policy definition (intensity levels, cascade rules)
  PolicyLevel.cs             -- enum: Block, Chunk, Object, Container, VDE
  CascadeStrategy.cs         -- enum: MostRestrictive, Merge, Enforce, Override, Inherit
  AiAutonomyLevel.cs         -- enum: ManualOnly, Suggest, SuggestExplain, AutoNotify, AutoSilent
  OperationalProfile.cs      -- named presets + custom profiles
  AuthorityChain.cs          -- Admin -> AI -> SuperAdmin override chain
  QuorumPolicy.cs            -- multi-key authorization
  PolicyResolutionContext.cs -- carries path, user, hardware context
  Cache/
    MaterializedPolicyCache.cs   -- pre-computed effective policies
    BloomFilterSkipIndex.cs      -- O(1) "has override?" checks
    CompiledPolicyDelegate.cs    -- JIT-compiled policy functions
  AI/
    IPolicyAdvisor.cs            -- AI suggestion/auto-tune interface
    IHardwareProbe.cs            -- hardware capability detection
    IWorkloadObserver.cs         -- data pattern monitoring
    IThreatSignalReceiver.cs     -- threat feeds into policy engine
    PolicyRecommendation.cs      -- AI output: what to change and why
```

### Backward Compatibility

Any v5.0 deployment upgraded to v6.0:
- All features continue working identically
- All configs auto-migrated to VDE-level policies
- No multi-level behavior unless admin explicitly configures it
- PolicyEngine transparent -- exists but doesn't change behavior
- AI is OFF by default (ManualOnly for all features)

---

## 10. Policy Data Residency -- Where Policy Physically Lives

### DECISION: Policy Travels With the Data It Governs

| Category | What | Where |
|----------|------|-------|
| VDE-level policy | Policies about the VDE itself | Inside VDE file, OUTSIDE user capacity |
| Container-level policy | Policies about containers | Inside VDE, within container metadata (user capacity) |
| Object-level policy | Policies about objects | Inside VDE, within inode xattrs |

### Three Residency Principles

1. **VDE-level policy** is infrastructure overhead (like superblock/bitmap). User pays no capacity cost. A 10MB VDE with 35KB of VDE-level policy = ~10.04MB file, but user sees 10MB usable.

2. **Container-level policy** is tenant data. It lives within the container's allocated space because container admins (tenants) configure their own overrides.

3. **Object-level policy** lives in inode extended attributes (4KB max). Most objects inherit (no policy xattr = zero overhead). Objects with overrides use sparse xattr storage.

---

## 11. DECISION: Custom VDE Format (Not Industry Standard)

### Why Custom, Not VHDX/VMDK/QCOW2

Industry standard VDE formats (VHD/VHDX, VMDK, QCOW2, VDI) are designed for ONE purpose: present a raw block device to an OS so the OS can put a filesystem on it.

Our VDE IS the filesystem. There's no OS layer in between. Using VHDX would mean either:
- (a) Put NTFS/ext4 inside VHDX, lose all deep integration (double indirection, no policy embedding) -- TERRIBLE
- (b) Write our structures directly on raw VHDX blocks, making VHDX a dead-weight wrapper -- POINTLESS

**What standard formats provide that we'd want:**
- Thin provisioning -> Our bitmap already supports this
- Snapshots -> Our CopyOnWrite/SnapshotManager already does this
- Dynamic resize -> Append to data region, update superblock
- Hypervisor mounting -> Not applicable (we're not a VM disk)

**What we need that NO standard provides:**
- Encrypted policy vault with HMAC seal
- Cryptographic binding between policy and data
- Capacity accounting excluding metadata overhead
- Inode-level policy xattrs
- WAL integrated into container
- Policy Overflow with hash chain

**Industry precedent:** ZFS, Ceph/BlueStore, HDFS, SQLite, PostgreSQL, MongoDB/WiredTiger, MinIO XL, Amazon S3 -- ALL use custom formats because the format IS the product.

**Interoperability provided through:**
- Import/export (dw export --format vhdx/raw)
- Standard API access (S3-compatible, FUSE mount, NFS/SMB, REST/gRPC)
- Fully documented public format specification

### Extended VDE Format v2.0

```
[VDE Header][Policy Vault][Superblock][Bitmap][Inode Table][WAL][Checksum Table][B-Tree Index][Data Region][Policy Overflow]
|  512B     |  ~35KB      |<----------------------- User Capacity ----------------------->|  variable  |
|           | (encrypted) |                          (10 MB)                               | (grows)    |
|<------------------------------------- Total File Size (~10.04 MB) ---------------------------------------->|
```

### VDE Header (512 bytes, fixed, sector-aligned)

```
Offset  Size   Field
0x00    4      Magic: "DWVD"
0x04    4      Format Version: 2 (bumped from 1)
0x08    8      Total file size
0x10    8      User capacity (what user configured)
0x18    8      Policy Vault offset
0x20    8      Policy Vault size
0x28    8      Superblock offset (user data region starts)
0x30    8      Policy Overflow offset (end of file)
0x38    8      Policy Overflow size
0x40    32     VDE Identity (unique ID, never changes)
0x60    32     Policy Binding Hash (HMAC covering vault + data region metadata)
0x80    32     Header Integrity Hash (BLAKE3 of bytes 0x00-0x7F)
0xA0    16     Creation timestamp
0xB0    16     Last policy modification timestamp
0xC0    64     Reserved for future use
0x100   256    Policy Vault Key Envelope (encrypted with VDE master key derivative)
```

### Policy Vault (variable, encrypted)

```
[Vault Header: 64B]
  Magic: "PVLT", version, entry count, total size, checksum, encryption indicator

[Policy Directory: variable]
  Array of { featureId(32B), offset(8B), size(8B), hash(32B) }
  Sorted by featureId for binary search O(log n)

[Policy Entries: variable, per feature]
  FeatureId (32B), PolicyLevel (1B), IntensityLevel (4B), CascadeStrategy (1B),
  AiAutonomyLevel (1B), AiEmergencyEscalation (1B), EffectiveTimestamp (8B),
  SetByAuthority (1B: Admin/AI/Quorum), SetByIdentity (32B),
  CustomConfig (variable, CBOR), EntryHash (32B BLAKE3)

[Authority Chain Record]
  Quorum config, super admin public keys, AI autonomy global config, escalation ring buffer

[Compliance Passport Summary]
  Active frameworks, sovereignty constraints, retention minimums, audit requirements

[Vault Seal: 64B]
  HMAC of entire vault using VDE master key
```

### Tamper Resistance (Deep Integration)

**Binding 1: Cryptographic Entanglement**
```
VDE Master Key
  +-- KDF("vde-data")    --> Data Encryption Key
  +-- KDF("vde-policy")  --> Policy Vault Encryption Key
  +-- KDF("vde-binding") --> Policy Binding Key

PolicyBindingHash = HMAC(PolicyBindingKey, PolicyVault || SuperblockHash || InodeTableRootHash || VdeIdentity)
```
- Can't detach vault and attach to different VDE (VdeIdentity mismatch)
- Can't modify data without invalidating binding (SuperblockHash changes)
- Can't modify vault without invalidating binding
- Can't read anything without VDE master key

**Binding 2: Structural Interleaving**
Policy Directory entries reference data region structures (affected inode ranges, data region checkpoints). Stripping the vault and replacing it breaks checkpoint verification.

**Binding 3: VDE Won't Open Without Valid Policy**
Open sequence: Header check -> derive policy key -> decrypt vault -> verify HMAC -> verify binding hash -> IF ANY FAILS: refuse to open, no data access.

**Hex editor attack resistance:**
| Attack | Prevention |
|--------|-----------|
| Read policies | Encrypted (random bytes without key) |
| Strip vault | Data won't decrypt (binding fails) |
| Replace with weaker vault | HMAC seal fails |
| Copy from another VDE | VdeIdentity mismatch |
| Modify single entry | Entry hash + vault seal fail |
| Truncate overflow | Overflow seal fails, degraded mode |

### Policy Overflow Region (end of file, grows over time)

```
[Overflow Header: 64B]
  Magic: "POVF", entry count, linked back to Policy Vault

[Escalation History]
  Ring buffer of last N escalation records (immutable)

[Policy Change Audit Trail]
  Append-only, hash-chained log (each entry hashes previous -- "poor man's TamperProof")

[Overflow Seal: 64B]
  HMAC of overflow region
```

### Size Estimates

| Component | Typical Size |
|-----------|-------------|
| VDE Header | 512 bytes |
| Policy Vault (94 features) | ~35 KB |
| Policy Overflow (after 1 year) | ~100 KB - 1 MB |
| **Total overhead for 10 MB VDE** | **~35 KB (0.35%)** |
| **Total overhead for 10 GB VDE** | **~35 KB (0.00035%)** |

---

## 12. Migration Path (v5.0 -> v6.0)

Phase A: SDK Foundation (PolicyEngine contracts, PluginBase PolicyContext, IntelligenceAwarePluginBase)
Phase B: VDE Format v2.0 (Policy Vault, header extension, migration tool for v1 VDEs)
Phase C: Policy Store (IPolicyPersistence implementations, auto-migrate existing config)
Phase D: Container-Level Policies (extend ContainerConfig with PolicyOverrides in inode)
Phase E: Object-Level Policies (extend InodeStructure with policy xattrs)
Phase F: Cascade Resolution Engine (resolve algorithm, wire to plugins)
Phase G: Fast-Path Optimization (bloom filter, compiled delegates, tier analysis)
Phase H: AI Integration (observation pipeline via IntelligenceAwarePluginBase, advisor, authority chain)
Phase I: Operational Profiles (presets, custom profiles, per-level override)
Phase J: Quorum System (multi-key auth, emergency escalation, dead man's switch)

---

## 11. Testing Strategy

### Layered Approach (total ~680+ tests)

| Layer | Tests | Scope |
|-------|-------|-------|
| PolicyEngine Unit | ~200 | Cascade resolution, policy store, compiled policies |
| Per-Feature Multi-Level | ~280 | Each of 94 features at 3 scenarios |
| Cross-Feature Interaction | ~50 | Feature combinations at different levels |
| AI Behavior | ~100 | Autonomy levels, authority chain, escalation |
| Performance | ~30 | Resolution timing, bloom filter rates, recompilation |
| Property-Based/Fuzz | ~20 generators | Random configs, determinism, in-flight safety |

### Combinatorial Reduction

Use pairwise testing (covering arrays) to reduce 11,750 theoretical combinations to ~200 representative pairs.

### Simulation Mode

PolicyEngine.Simulate(hypotheticalPolicy) -> returns what would change, affected count, estimated performance impact, AI assessment. For admins AND tests to validate without applying.

---

---

## 13. DECISION: Composable VDE Architecture (Not Monolithic)

### The Problem

A fully-specced VDE with ALL 20+ regions enabled has ~3.2% overhead. For a 1 GB military VDE, that's ~32 MB — acceptable. But for a hospital with 1 TB of GDPR-compliant patient records, baking in RAID metadata, streaming ring buffers, compute code cache, and consensus logs they'll never use wastes ~20 GB.

### The Solution: VDE as a Build-Time Profile

The DWVD v2.0 spec defines the **maximum envelope** — all possible regions, inode fields, and block types. At deployment, the user selects which **modules** to integrate. DW creates the VDE format with exactly those modules at runtime.

**Key principle:** Features NOT module-integrated STILL WORK through the plugin pipeline (Tier 2 performance). Module integration extracts the "last drops" of performance and intensity from the format itself (Tier 1).

### Three-Tier Performance Model

| Tier | Name | Performance | Example (Encryption) |
|------|------|-------------|---------------------|
| **Tier 1** | VDE-Integrated | Maximum (zero extra I/O) | KeySlot in inode, EncryptionHeader region, per-block algo in trailer, AEAD tags in Integrity Tree |
| **Tier 2** | Pipeline-Optimized | Very Good (~1 extra lookup/session) | IKeyStore lookup, cached session keys, encryption via pipeline Layer 2 |
| **Tier 3** | Basic | Good (~2 extra lookups/op) | Encrypt/decrypt via pipeline, external key management |

For most deployments, Tier 2 is already excellent (built over v1.0-v5.0). Tier 1 is for environments that need block-level, zero-overhead, format-native integration.

### 19 Composable Modules

Each module is a bit in a 32-bit `ModuleManifest` field in the Superblock:

| Bit | Module | Regions Added | Inode Bytes |
|-----|--------|---------------|-------------|
| 0 | Security (SEC) | PolicyVault + EncryptionHeader | +24 |
| 1 | Compliance (CMPL) | ComplianceVault + AuditLog | +12 |
| 2 | Intelligence (INTL) | IntelligenceCache | +12 |
| 3 | Tags (TAGS) | TagIndexRegion | +136 |
| 4 | Replication (REPL) | ReplicationState | +8 |
| 5 | RAID (RAID) | RAIDMetadata | +4 |
| 6 | Streaming (STRM) | StreamingAppend + DataWAL | +8 |
| 7 | Compute (COMP) | ComputeCodeCache | +0 |
| 8 | Fabric (FABR) | CrossVDEReferenceTable | +0 |
| 9 | Consensus (CNSS) | ConsensusLogRegion | +0 |
| 10 | Compression (CMPR) | DictionaryRegion | +4 |
| 11 | Integrity (INTG) | IntegrityTree (Merkle) | +0 |
| 12 | Snapshot (SNAP) | SnapshotTable | +0 |
| 13 | Query (QURY) | BTreeIndexForest (extended) | +4 |
| 14 | Privacy (PRIV) | AnonymizationTable | +2 |
| 15 | Sustainability (SUST) | (superblock metadata) | +4 |
| 16 | Transit (TRNS) | (superblock metadata) | +1 |
| 17 | Observability (OBSV) | MetricsLogRegion | +0 |
| 18 | AuditLog (ALOG) | AuditLogRegion | +0 |

Examples: `0x00000000` = Minimal core, `0x0000000F` = SEC+CMPL+INTL+TAGS, `0x0007FFFF` = ALL modules.

### Inode is Composable Too

Inode size is **fixed within a VDE** but **varies between VDEs** based on selected modules. An `InodeLayoutDescriptor` in the Superblock makes inodes self-describing — the VDE engine parses correctly regardless of which modules are active.

Inode sizes: 320B (minimal) → 384B (standard) → 512B (enterprise) → 576B (maximum). Padding bytes are reserved for future module additions without migration.

### Online Module Addition (Three Options)

When a user enables a feature later, they choose:

| Option | What Happens | Downtime | Performance |
|--------|-------------|----------|-------------|
| **1. Add to current VDE** | Allocate new region from free space + claim inode padding bytes | None | Tier 1 |
| **2. Create new VDE + migrate** | Build new VDE with module, bulk copy data | Minutes (or zero with fabric hot-swap) | Tier 1 |
| **3. Use without VDE integration** | Feature works through pipeline immediately | None | Tier 2 |

Option 1 works because:
- Region Directory has 127 slots (only 7-15 typically used)
- Inode padding bytes are designed to be claimable by future modules
- All operations are WAL-journaled (crash-safe)
- If inode has no padding left → falls back to background inode table migration (like ext4 online inode resize)

Option 3 is **always available, always instant, always zero-risk**.

### VDE Creation Profiles

| Profile | Modules | InodeSize | Overhead |
|---------|---------|-----------|----------|
| Minimal | Core only | 320B | ~1.4% |
| Standard | SEC + INTG + SNAP + CMPR | 384B | ~2.0% |
| Enterprise | SEC + CMPL + INTL + TAGS + INTG + SNAP + CMPR + ALOG | 512B | ~2.5% |
| Maximum Security | ALL 19 modules | 576B | ~3.2% |
| Edge/IoT | STRM + INTG (512B blocks) | 320B | ~1.8% |
| Analytics | INTL + CMPR + SNAP + QURY | 384B | ~2.0% |
| Fully Custom | User decides at deployment |  | from ~1.4 to ~3.2% |

### Additional VDE Enhancements

1. **dw:// Namespace Signature**: 16-byte magic at offset 0 (`DWVD` + version + `dw://` anchor). Namespace Registration Block with Ed25519-signed URI. VDE knows its own `dw://` address.

2. **External Tamper Detection**: HMAC-BLAKE3 header seal, metadata chain hash, file size sentinel, last writer identity. Configurable response: WARN → READ_ONLY → REQUIRE_AUTH → REFUSE → QUARANTINE.

3. **Emergency Recovery Block**: Fixed at block 9 (always same location). Plaintext VDE UUID, creation date, admin contact. Accessible even with all keys lost.

4. **Forward Compatibility**: Three-tier feature flags (incompatible / read-only-compatible / compatible) following ext4 model. `MinReaderVersion` and `MinWriterVersion` in Superblock.

5. **Thin Provisioning**: Sparse file semantics — unwritten blocks don't consume physical disk. `PhysicalAllocatedBlocks` vs `TotalBlocks` tracking.

6. **VDE Nesting**: VDE within VDE (up to 3 levels) for multi-tenant isolation and defense-in-depth encryption.

Full technical details in `.planning/v6.0-VDE-FORMAT-v2.0-SPEC.md` (1,760 lines).

---

---

## 13. DECISION: Metadata Residency Strategy (VDE ↔ Plugin Metadata Relationship)

### The Problem

Tier 1 (VDE format) and Tier 2 (plugin metadata) are not mutually exclusive. When a module is enabled, the relationship between VDE metadata and plugin metadata must be explicitly managed:
- **Existing data** needs lazy migration from Tier 2 to Tier 1 (mixed state within one VDE)
- **VDE corruption** should fall back to plugin metadata when a backup exists
- **Plugin metadata loss** should be recoverable from VDE metadata
- **Sensitive material** (hardware keys, HSM refs, external secrets) MUST NOT reside in the VDE
- **Write ordering** has different performance/safety tradeoffs for different metadata types

### The Solution: Per-Feature Metadata Residency Configuration

#### Three Residency Modes

```
MetadataResidencyMode:
  VdeOnly        — Metadata lives ONLY in VDE format. No plugin backup.
                   Fastest writes. Data is fully portable. Single point of failure.

  VdePrimary     — VDE format is primary. Plugin metadata is backup.
                   Reads: VDE first → plugin fallback on corruption.
                   Writes: configurable ordering (see WriteStrategy).

  PluginOnly     — Metadata lives ONLY in plugin's own store.
                   VDE inode fields left empty/zeroed (or store a reference URI).
                   Used when metadata MUST NOT reside in VDE (hardware keys,
                   HSM refs, external secrets, regulatory requirements).
```

No `PluginPrimary` mode — that's Tier 2 (don't enable the module).

#### Write Strategies (when VdePrimary)

```
WriteStrategy:
  Atomic         — VDE inode update + data write + plugin metadata write in one
                   WAL transaction. All succeed or all roll back.
                   Highest safety. Highest write latency.

  VdeFirstSync   — Write to VDE (WAL-journaled), then synchronously write to
                   plugin metadata. Return success only after both complete.
                   No cross-store transaction overhead but no risk window.

  VdeFirstLazy   — Write to VDE (WAL-journaled). Return success to caller.
                   Queue plugin metadata write for background flush.
                   Fast writes. Plugin metadata may lag by seconds/minutes.
                   Configurable flush interval (default 30s).
```

#### Read Strategy (when VdePrimary)

```
ReadStrategy:
  VdeFallback    — Read from VDE. If VDE metadata is missing, corrupt, or
                   fails validation (e.g., key doesn't decrypt), fall back
                   to plugin metadata. Log discrepancy. Auto-repair VDE.

  VdeStrict      — Read from VDE only. If corrupt, fail the operation.
                   Use when plugin metadata is known stale or when you want
                   immediate corruption detection without silent fallback.
```

#### Corruption Recovery Actions

```
CorruptionAction:
  FallbackAndRepair — Fall back to plugin metadata, auto-repair VDE metadata
                      from plugin source. Log + emit message bus event.

  FallbackOnly      — Fall back to plugin metadata, do NOT auto-repair.
                      Admin must manually trigger repair.

  FailAndAlert      — Fail the operation. Do not fall back. Alert admin.
                      Use for maximum integrity assurance (detect, don't mask).

  Quarantine        — Mark the object as quarantined. Refuse further access
                      until admin reviews and resolves.
```

### Hardware-Backed Keys: The Reference Pattern

When metadata MUST NOT reside in the VDE (e.g., HSM keys, FROST shards, cloud KMS), the VDE stores a **reference** instead of the actual material:

```
KeySlot (in Encryption Header) {
  SlotId:          0
  Status:          ACTIVE
  KeyType:         HARDWARE_EXTERNAL     ← indicates external-only
  WrappedKey:      [all zeros]           ← intentionally empty
  KeyFingerprint:  0xABCD1234...         ← safe to store (public info)
  KeyStoreRef:     "hsm://yubikey/slot-9a"  ← URI to external store
  Algorithm:       AES-256-GCM
  CreatedUtc:      2026-02-20T...
}
```

The inode's `EncryptionKeySlot` points to slot 0. The VDE engine reads the slot, sees `KeyType: HARDWARE_EXTERNAL`, and delegates to UltimateKeyManagement with the `KeyStoreRef` URI. **The actual key never touches the VDE.**

**Principle:** References are always safe to store in the VDE. Secrets are not. The VDE stores "what" and "where" — the plugin stores the sensitive material when the security model demands it.

#### Broader PluginOnly Examples

| Feature | Why PluginOnly | VDE stores instead |
|---|---|---|
| HSM-backed encryption keys | Key must not leave HSM boundary | `KeyStoreRef: "hsm://thales/partition-3"` + fingerprint |
| FROST threshold key shards | Shard must stay on designated node | `KeyStoreRef: "frost://node-2/shard-id"` + shard metadata |
| External certificate chains | CRL/OCSP responder is authoritative | `CertRef: "pkix://ca.example.com/chain"` + cert fingerprint |
| Cloud KMS keys | Cloud provider is the key authority | `KeyStoreRef: "kms://aws/key/arn:..."` + key ID |
| SPIFFE identities | Identity comes from SPIRE server | `IdentityRef: "spiffe://cluster/workload"` + SVID hash |
| External compliance DB | Regulatory records must stay in compliance DB | `ComplianceRef: "db://compliance-master/record-id"` |

### Lazy Migration Protocol (Existing Data After Module Enable)

When a module is newly enabled, existing objects have empty VDE inode fields. Migration is lazy by default:

```
Read object:
  1. Read inode → EncryptionKeySlot = 0 (empty)
  2. VDE has no key info for this object
  3. ReadStrategy = VdeFallback → fall back to plugin
  4. Plugin returns key from its own metadata store
  5. Operation succeeds
  6. (Background) If residencyMode != PluginOnly:
     write key slot reference back to inode → Tier 1 for next read
```

Optional eager migration: `dw vde migrate-metadata <vde> --module <module> --strategy eager [--max-iops N]`

### Default Residency by Metadata Type

| Metadata Type | Default Residency | Default Write | Rationale |
|---|---|---|---|
| Encryption key references | VdePrimary | VdeFirstSync | Redundancy, not secret |
| Hardware key material | PluginOnly | N/A | Must not leave HSM |
| Software wrapped DEKs | VdePrimary | VdeFirstSync | Portable + backup |
| RAID shard maps | VdePrimary | VdeFirstLazy | Rebuildable from parity |
| Replication DVV | VdePrimary | VdeFirstSync | Must be consistent |
| Compliance passports | VdePrimary | Atomic | Regulatory, no partial writes |
| Tag index | VdePrimary | VdeFirstLazy | Rebuildable from inodes |
| Hash chain | VdePrimary | Atomic | Integrity chain, atomic only |
| AI classification cache | VdeOnly | N/A | Rebuildable by re-inference |
| Compression dictionaries | VdeOnly | N/A | Non-sensitive, portable |
| Stream sequence numbers | VdeOnly | N/A | Ephemeral, performance-critical |

All defaults are user-configurable per feature per VDE via the policy engine.

### VDE Creation Flow (Deployment Prompt)

During `dw vde create`, the user is prompted for metadata residency:
1. Choose default strategy (VdeOnly / VdePrimary-Lazy / VdePrimary-Sync / VdePrimary-Atomic / Custom)
2. Hardware key detection: auto-sets PluginOnly for hardware-backed key material
3. Sensitive metadata review: table of all metadata types with their residency, editable
4. Confirmation and creation

---

## 22. VDE 2.0B — Multi-VDE Federation Architecture (Yottabyte+ Scale)

### The Problem

A single VDE is bounded by:
- Single file size limits (ext4: 16TB, XFS: 8EB, NTFS: 16EB)
- Single-node I/O bandwidth
- Single-device failure domain
- Single namespace (all objects in one container)

For yottabyte-scale deployments (hyperscale cloud, national archives, scientific computing), a single VDE is insufficient. Multiple VDEs must federate into a unified namespace while maintaining the full policy, security, and performance characteristics of individual VDEs.

### The Solution: VDE Federation (VDE 2.0B)

A **VDE Federation** is a logical grouping of multiple VDEs that presents a single unified namespace (`dw://federation-name/...`) while each member VDE retains its own physical format, policies, and locality.

```
VDE Federation "corp-storage"
├── VDE-A (us-east, 100TB, Paranoid profile)
│   ├── Container: financial-records
│   └── Container: audit-trail
├── VDE-B (eu-west, 500TB, Strict profile, GDPR sovereignty)
│   ├── Container: patient-data
│   └── Container: clinical-trials
├── VDE-C (ap-south, 50TB, Standard profile)
│   └── Container: analytics-cache
└── VDE-D (us-west, 2PB, Speed profile)
    ├── Container: media-archive
    └── Container: ml-training-data
```

### Federation Metadata

Each VDE stores a **FederationDescriptor** in its Superblock Extended Metadata:

```
FederationDescriptor {
  FederationId:       Guid
  FederationName:     "corp-storage"
  MemberVdeId:        Guid (this VDE's ID)
  MemberRole:         Primary | Secondary | ReadReplica | Archive
  NamespacePrefix:    "dw://corp-storage/"
  RoutingTable:       FederationRoutingEntry[]  // cached, refreshed via gossip
  CoordinatorVdeId:   Guid  // elected leader for metadata operations
  JoinedUtc:          DateTimeOffset
  FederationEpoch:    ulong  // incremented on membership changes
}

FederationRoutingEntry {
  VdeId:        Guid
  Endpoint:     StorageAddress  // how to reach this VDE
  Namespaces:   string[]        // which namespace prefixes this VDE owns
  Status:       Online | Draining | Offline | Rebuilding
  Capabilities: VdeCapabilities // which modules are enabled
  LastHeartbeat: DateTimeOffset
}
```

### Federation Operations

| Operation | Behavior |
|-----------|----------|
| **Store** | Route to correct VDE based on placement policy (sovereignty, locality, capacity). If target VDE is full, overflow to next eligible VDE. |
| **Retrieve** | Route to owning VDE. If unavailable, route to replica (if replicated across VDEs). |
| **List** | Fan-out to all member VDEs, merge results. Cursor-based pagination across VDEs. |
| **Search** | Fan-out search, merge + rank results. Each VDE searches its local index. |
| **Migrate** | Move objects between member VDEs (for rebalancing, sovereignty changes, tiering). |
| **Snapshot** | Coordinated snapshot across all member VDEs (2PC protocol). |
| **Policy** | Federation-level policy cascades to member VDEs. Member VDE policy can tighten but not loosen. |

### Placement Policy (Cross-VDE)

```
FederationPlacementPolicy {
  // Sovereignty: objects tagged region:eu MUST go to EU-located VDE
  sovereigntyRules: [
    { tagMatch: "region:eu", targetVdes: ["VDE-B"], mode: Enforce }
  ]

  // Tiering: hot data on NVMe VDEs, cold data on archive VDEs
  tieringRules: [
    { accessPattern: Hot, targetVdes: ["VDE-A", "VDE-C"] },
    { accessPattern: Archive, targetVdes: ["VDE-D"] }
  ]

  // Capacity: when VDE reaches 90%, start routing to next eligible
  capacityThreshold: 0.90
  overflowStrategy: RoundRobin | LeastUsed | Closest

  // Cross-VDE replication: critical objects replicated to 2+ VDEs
  crossVdeReplication: {
    enabled: true,
    factor: 2,
    placementConstraint: DifferentRegion  // geo-diversity
  }
}
```

### Federation Coordinator

One VDE is elected as the **Federation Coordinator** (via Raft among member VDEs). The coordinator:
- Maintains the authoritative routing table
- Handles membership changes (VDE join/leave/fail)
- Coordinates cross-VDE snapshots and migrations
- Enforces federation-level policy
- Does NOT handle data path operations (those route directly to member VDEs)

Coordinator election uses the existing UltimateConsensus plugin. Failure triggers re-election within 10 seconds.

### Scale Targets

| Metric | Target |
|--------|--------|
| Member VDEs per federation | 1 to 10,000 |
| Total federated capacity | Up to yottabyte (1 YB = 10^24 bytes) |
| Cross-VDE latency (metadata) | <10ms (LAN), <100ms (WAN) |
| Cross-VDE latency (data) | Network-bound (direct VDE-to-VDE) |
| Federation routing table | Cached locally, refreshed via gossip (SWIM protocol) |
| Membership change convergence | <30 seconds for 1,000 VDE federation |

---

## 23. Device-Level RAID — Compound Block Devices

### The Problem

Current UltimateRAID operates at the **data level** — it stripes, mirrors, and distributes data objects across storage strategies. This is powerful for data redundancy but misses a fundamental layer: **physical device aggregation**.

When DW runs on bare metal with multiple physical drives (NVMe, SSD, HDD), there's no mechanism to:
- Combine multiple drives into a single logical block device
- Provide device-level redundancy (drive failure tolerance)
- Present a unified capacity pool to the VDE layer
- Manage hot-spare drives and automatic rebuild

This is what hardware RAID controllers, Linux md, ZFS vdevs, and Windows Storage Spaces do — and DW needs it natively.

### The Solution: Two-Level RAID Architecture

```
Level 1: DEVICE RAID (NEW — Phase 90)
  Physical drives → Compound Block Device
  Operates on: raw block I/O
  Managed by: UltimateRAID plugin (new device-level strategies)

  Example: 4x NVMe → RAID-10 compound device (2x mirror, 2x stripe)

Level 2: DATA RAID (EXISTING — current UltimateRAID)
  Data objects → Redundant data placement
  Operates on: logical objects/chunks
  Managed by: UltimateRAID plugin (existing data-level strategies)

  Example: Object replicated 3x across storage backends
```

### Compound Block Device Architecture

```
CompoundBlockDevice : IBlockDevice {
  // Presents a single IBlockDevice interface to the VDE layer
  // Internally manages an array of physical IBlockDevice instances

  PhysicalDevices: IBlockDevice[]       // raw drives
  RaidLevel: DeviceRaidLevel            // RAID-0/1/5/6/10/Z1/Z2/Z3
  StripeSize: int                       // 64KB - 1MB (auto-tuned)
  ParityLayout: ParityDistribution      // Left/Right/Rotating
  HotSpares: IBlockDevice[]             // standby drives
  RebuildState: RebuildProgress?        // null if healthy

  // IBlockDevice implementation
  ReadBlockAsync(blockNumber, buffer, ct)
  WriteBlockAsync(blockNumber, data, ct)
  GetBlockCount()                       // total usable capacity
  GetBlockSize()                        // physical sector size
  Flush()
  Trim(blockRange)                      // pass through to SSDs
}
```

### Device RAID Levels

| Level | Description | Min Drives | Capacity | Fault Tolerance |
|-------|-------------|-----------|----------|-----------------|
| RAID-0 | Stripe | 2 | N × size | None (performance only) |
| RAID-1 | Mirror | 2 | 1 × size | N-1 drives |
| RAID-5 | Distributed parity | 3 | (N-1) × size | 1 drive |
| RAID-6 | Double parity | 4 | (N-2) × size | 2 drives |
| RAID-10 | Stripe of mirrors | 4 | N/2 × size | 1 per mirror |
| RAID-Z1 | ZFS-style single parity | 3 | (N-1) × size | 1 drive |
| RAID-Z2 | ZFS-style double parity | 4 | (N-2) × size | 2 drives |
| RAID-Z3 | ZFS-style triple parity | 5 | (N-3) × size | 3 drives |
| JBOD | Concatenation (no redundancy) | 1 | Sum of all | None |

### Device Discovery Flow

```
1. HardwareProbe (existing v3.0, Phase 32)
   ├── WindowsHardwareProbe (WMI: Win32_DiskDrive)
   ├── LinuxHardwareProbe (sysfs: /sys/block/*, /dev/nvme*)
   └── MacOsHardwareProbe (system_profiler, diskutil)

2. DeviceInventory (NEW)
   ├── Enumerate all block devices
   ├── Classify: NVMe / SSD / HDD / USB / Network
   ├── Read SMART data (health, wear level, temperature)
   ├── Detect existing partitions/filesystems (don't touch in-use drives)
   └── Report: device path, capacity, type, health, availability

3. CompoundBlockDevice creation (NEW)
   ├── User selects drives and RAID level (CLI: `dw device create-array`)
   ├── Or: auto-configuration based on drive count and deployment profile
   ├── Write device array metadata to reserved blocks on each member drive
   └── Present IBlockDevice to VDE layer

4. VDE creation on CompoundBlockDevice
   ├── `dw vde create --device /dev/dw-array0` (or auto-detect)
   ├── VDE sees single large block device
   └── VDE format (superblock, regions, etc.) written normally
```

### Auto-Configuration Profiles

| Profile | When | RAID Level | Rationale |
|---------|------|-----------|-----------|
| Single Drive | 1 drive | JBOD (passthrough) | No redundancy possible |
| Development | 2 drives | RAID-1 | Mirror for safety |
| Standard | 3-4 drives | RAID-5 | Balance capacity/safety |
| Enterprise | 4-8 drives | RAID-6 or RAID-10 | Double fault tolerance |
| Hyperscale | 8+ drives | RAID-Z2 + hot spare | ZFS-proven at scale |
| Performance | Any count | RAID-0 | User explicitly chose speed |

### Rebuild and Hot Spare

- **Automatic rebuild**: When a drive fails, if a hot spare is available, rebuild starts automatically
- **Rebuild priority**: Configurable (background, normal, urgent). Default: background with I/O throttling
- **Scrub schedule**: Periodic background verification of parity consistency (like ZFS scrub)
- **SMART monitoring**: Predictive failure — start rebuild to hot spare BEFORE the drive fails
- **Rebuild progress**: Reported via observability, message bus events, and CLI (`dw device status`)

---

## 24. Bare Metal to User Storage — The Complete Flow

### The Full Path

```
┌─────────────────────────────────────────────────────────────────┐
│                    PHYSICAL HARDWARE LAYER                       │
│  NVMe  SSD  SSD  HDD  HDD  HDD  HDD  HDD  NVMe  NVMe          │
│   0     1    2    3    4    5    6    7    8     9               │
└───┬─────┬────┬────┬────┬────┬────┬────┬────┬─────┬──────────────┘
    │     │    │    │    │    │    │    │    │     │
    v     v    v    v    v    v    v    v    v     v
┌─────────────────────────────────────────────────────────────────┐
│              1. HARDWARE DISCOVERY (Phase 32, v3.0)             │
│  HardwareProbe → DeviceInventory → CapabilityRegistry          │
│  Detects: device type, capacity, SMART health, NVMe queues     │
└─────────────────────────────────────────────────────────────────┘
    │
    v
┌─────────────────────────────────────────────────────────────────┐
│         2. DEVICE-LEVEL RAID (NEW — Phase 90)                   │
│  CompoundBlockDevice aggregates physical drives                 │
│                                                                 │
│  Array 0 (Performance):  NVMe0 + NVMe8 + NVMe9 → RAID-0       │
│  Array 1 (Data):         HDD3-7 → RAID-Z2 + SSD1 hot-cache    │
│  Array 2 (Mirror):       SSD1 + SSD2 → RAID-1 (metadata/WAL)  │
│                                                                 │
│  Each array presents single IBlockDevice                        │
└─────────────────────────────────────────────────────────────────┘
    │
    v
┌─────────────────────────────────────────────────────────────────┐
│            3. VDE CREATION (v3.0 + v6.0 enhancements)           │
│  VDE created ON a CompoundBlockDevice (or raw device/file)      │
│  Superblock → Region Directory → Allocation Groups → Data       │
│  Modules selected: SEC + INTG + CMPR + TAGS (composable v2.0)  │
│  Inode layout: Standard256 (fits most workloads)                │
│  Block size: 4KB (auto-selected based on workload profile)      │
└─────────────────────────────────────────────────────────────────┘
    │
    v
┌─────────────────────────────────────────────────────────────────┐
│          4. VDE FEDERATION (NEW — Phase 91)                     │
│  Multiple VDEs joined into federation namespace                 │
│  dw://corp-storage/ → routes to correct member VDE              │
│  Placement policy: sovereignty, tiering, capacity-based         │
│  Cross-VDE replication for critical data                        │
└─────────────────────────────────────────────────────────────────┘
    │
    v
┌─────────────────────────────────────────────────────────────────┐
│        5. STORAGE FABRIC (v5.0 Phase 63, UniversalFabric)       │
│  dw:// namespace resolution → backend routing                   │
│  VDE registered as storage backend alongside cloud, network     │
│  Multi-backend: some data in VDE, some in S3, transparent       │
└─────────────────────────────────────────────────────────────────┘
    │
    v
┌─────────────────────────────────────────────────────────────────┐
│         6. PLUGIN PIPELINE (v1.0-v5.0, all phases)              │
│  Write: data → Compress → Encrypt → Checksum → RAID → Store    │
│  Read:  Store → RAID → Checksum → Decrypt → Decompress → data  │
│  Policy engine (v6.0) selects intensity per operation           │
│  AI observes and tunes (v6.0)                                   │
└─────────────────────────────────────────────────────────────────┘
    │
    v
┌─────────────────────────────────────────────────────────────────┐
│           7. USER-VISIBLE STORAGE CAPACITY                      │
│                                                                 │
│  CLI:  dw store my-file.pdf --tags "dept:legal,class:PII"       │
│  SDK:  client.StoreAsync("my-file.pdf", stream, tags)           │
│  SQL:  INSERT INTO documents (name, data) VALUES (...)          │
│  S3:   PUT /bucket/my-file.pdf (S3-compatible API)              │
│  FUSE: cp my-file.pdf /mnt/datawarehouse/legal/                 │
│  NFS:  mount -t nfs dw-host:/export /mnt/dw                    │
│                                                                 │
│  User sees: unified namespace, policy-enforced, AI-optimized    │
│  User doesn't see: RAID arrays, VDE internals, federation       │
└─────────────────────────────────────────────────────────────────┘
```

### Plugin Adjustments for Device-Level RAID

The introduction of CompoundBlockDevice requires adjustments in several plugins:

| Plugin | Adjustment |
|--------|------------|
| **UltimateRAID** | Add device-level RAID strategies alongside existing data-level strategies. New strategy base: `DeviceRaidStrategyBase`. Strategies: Raid0Device, Raid1Device, Raid5Device, Raid6Device, Raid10Device, RaidZ1Device, RaidZ2Device, RaidZ3Device, JbodDevice. Each manages physical IBlockDevice arrays. |
| **UltimateStorage** | VDE storage strategy (`VdeStorageStrategy`) already works with IBlockDevice. No changes needed — VDE sits on CompoundBlockDevice transparently. |
| **UltimateFilesystem** | FUSE/WinFSP drivers already expose VDE as filesystem. No changes needed — compound device is below VDE. |
| **UltimateReplication** | Cross-VDE federation replication uses existing replication strategies. New: `FederationReplicationStrategy` for coordinating cross-VDE object moves and syncs. |
| **UniversalFabric** | Register compound devices and VDE federations as fabric endpoints. New: `FederationFabricStrategy` for routing to federated VDEs. |
| **UniversalObservability** | Add device health metrics: SMART data, rebuild progress, RAID array health, per-drive IOPS/latency. New dashboard panels. |
| **UltimateResourceManager** | Manage compound device resources: track capacity across arrays, alert on low space, coordinate drive additions/removals. |
| **UltimateDeployment** | Device array creation as part of deployment workflow. Auto-detect drives, suggest RAID configuration, create arrays before VDE creation. |

### Why Two RAID Levels?

```
DEVICE RAID (Level 1):          DATA RAID (Level 2):
  Physical redundancy              Logical redundancy
  Drive failure tolerance          Object-level protection
  Block-level operations           Object/chunk-level operations
  Local to one machine             Can span multiple machines
  Always-on (hardware mandates)    Policy-driven (user chooses)
  Managed by OS/driver layer       Managed by plugin pipeline
  Example: RAID-6 (lose 2 drives)  Example: 3x replication (3 sites)
```

Both levels can be active simultaneously:
- Device RAID protects against physical drive failures
- Data RAID protects against node/site failures and provides geo-distribution

A deployment might have: `RAID-6 device array` → `VDE on array` → `3x cross-VDE federation replication` → user sees a single, highly-available, geo-distributed storage system.

---

*This document captures the design discussion as of 2026-02-22. It will be consumed by the research and requirements phases when milestone v6.0 is formalized.*

**Decision Log:**
1. Policy at Plugin level, not Strategy level (Plugin selects strategy based on policy)
2. AI integration through IntelligenceAwarePluginBase (UltimateIntelligence excluded to prevent loops)
3. Quorum failsafe enables AI management of ALL policies including security
22. VDE 2.0B Federation — multi-VDE unified namespace for yottabyte+ scale, coordinator election via Raft, federation-level placement policy (sovereignty, tiering, capacity), cross-VDE replication
23. Device-Level RAID — CompoundBlockDevice aggregates physical drives (RAID-0/1/5/6/10/Z1/Z2/Z3/JBOD), two-level RAID architecture (device + data), auto-configuration profiles, hot spare + SMART predictive rebuild
24. Bare Metal to Storage Flow — 7-layer path from physical hardware through device RAID, VDE creation, federation, fabric, plugin pipeline to user-visible storage
4. Persistence layer is pluggable (InMemory/File/Database/TamperProof/Hybrid)
5. Custom VDE format (not VHDX/VMDK) -- format IS the product
6. Policy data embedded in VDE with cryptographic binding (not detachable)
7. VDE-level policy outside user capacity, Container/Object-level within user capacity
8. **Composable VDE architecture** -- spec is maximum envelope, user selects modules at deployment, features work at Tier 2 without module integration, three options for later module addition (online modify, new VDE, pipeline fallback)
9. **dw:// namespace embedded in VDE** -- self-identifying format with Ed25519-signed namespace authority
10. **External tamper detection** -- HMAC seals, chain hashes, file size sentinels, configurable tamper response levels
11. **File extension `.dwvd`** -- DataWarehouse Virtual Disk, registered across Windows (ProgID + shell handlers), Linux (freedesktop MIME + libmagic), macOS (UTI conforming to public.disk-image). MIME: `application/vnd.datawarehouse.dwvd`. Secondary extensions: `.dwvd.snap`, `.dwvd.delta`, `.dwvd.meta`, `.dwvd.lock`. Shell handler is NOT a separate project — it's 3 registry entries pointing to the existing `dw` CLI binary, implemented as part of installer/packaging.
12. **Plugin consolidation audit** -- 17 non-Ultimate plugins must be reviewed: each either justified as standalone (orchestrator / unique requirements) or merged as a strategy into the appropriate Ultimate plugin. Added as a late v6.0 phase.
13. **Metadata Residency Strategy** -- Per-feature, per-module, user-configurable metadata residency (VdeOnly/VdePrimary/PluginOnly), write strategies (Atomic/VdeFirstSync/VdeFirstLazy), read strategies (VdeFallback/VdeStrict), corruption recovery actions (FallbackAndRepair/FallbackOnly/FailAndAlert/Quarantine). Hardware-backed keys use PluginOnly with reference URIs in VDE. Lazy migration for existing data after module enable. All defaults user-overridable via policy engine.
14. **Dynamic Scaling Architecture** -- All structural limits are configurable, not constant. The principle: DW scales unlimited, restricted only by hardware.
    - **Write concurrency**: Stripe count dynamic (default 64, auto-scales to 256/512/1024 based on detected NVMe queue depth and CPU cores). Dual WAL (metadata + data) is default; configurable to single WAL for constrained environments.
    - **B-Tree**: Dynamic branching factor (computed from block size and average key size at creation, recomputable online). B-Tree Forest for keyspace partitioning. Cache size auto-tuned from available RAM (default: 5% of RAM or 1,000 nodes, whichever is larger). Memory-mapped I/O option for B-Tree and inode table at scale.
    - **Inode extents**: Dynamic, not fixed. Initial allocation: 4 extents (sufficient for most files). Grows to 8, 16, 32 on demand by consuming inode padding bytes. Beyond 32: indirect extent block (extent of extents). Shrinks when extents are freed (e.g., sparse hole punch). InodeLayoutDescriptor records current extent count per VDE.
    - **DVV region**: Dynamic size. Default 256 bytes (32 nodes). Auto-grows via Replication State Region expansion (online, WAL-journaled). At 10K+ nodes: hierarchical DVV (per-region DVV, regions form higher-level DVV — like CRUSH placement groups).
    - **Cluster nodes**: Default 100, no hard upper limit. Protocol designed for 10K+ nodes. Gossip protocol (SWIM) scales O(log N) per membership event. Consensus (Raft) used within regions (≤100 nodes each), CRDT across regions.
    - **Replication fan-out**: Default 100, configurable up to cluster size. At 1,000+ replicas: tree-based replication (origin → 10 relays → 100 replicas each) instead of star topology. Configurable topology: star, tree, mesh, chain.
    - **Message bus**: Default 1K msg/s per publisher, scalable to 1M+ msg/s. Bounded channel with configurable backpressure (drop-oldest, block-writer, shed-load). Per-topic throughput limits. Batch delivery mode (bundle N messages per dispatch). Optional off-heap ring buffer for zero-GC messaging at extreme throughput.
    - **Inode cache**: Auto-sized from available RAM (default: 10% of RAM or 10K inodes, whichever is larger). Memory-mapped inode table option — OS page cache becomes the cache, no application-level LRU needed. At 1B+ objects: tiered cache (L1: hot inodes in-process, L2: memory-mapped, L3: on-disk).
    - **Region Directory**: 127 slots is the v2.0 format constant (fits in 2 blocks). If 127 regions are insufficient (unlikely — 19 modules use ~25 regions), a secondary Region Directory block extends to 254 slots.
    - All scaling parameters recorded in VDE Superblock Extended Metadata so a VDE opened on different hardware auto-tunes to the new environment.
15. **Competitive Edge Architecture** -- Principle: existing plugin strategies STAY as they are (SMB/NFS/iSCSI/FC/NVMe-oF/Delta Lake/Iceberg/BM25/Schema Registry all exist as 1,000+ line strategies). Phase 85 adds VDE-native fast paths — a zero-copy block export layer that protocols can use to bypass the plugin pipeline when serving raw blocks. This is NOT replacing strategies; it's adding a Tier 0 path: `client → protocol strategy → VDE block export → disk` (skipping plugin orchestration). Formal verification (TLA+) proves correctness of the hardest algorithms (WAL, Raft, Bε-tree) — this is not testing, it's mathematical proof of invariants. OS-level security (seccomp-bpf, pledge/unveil) constrains what syscalls DW can make — defense in depth beyond application-level security. Streaming SQL is a query layer OVER existing UltimateStreamingData — not replacing it, but adding SQL semantics on top. Safety certification (DO-178C prep) is about PROVING bounded execution — traceability from requirement to code to test, not new features.
16. **Variable-Width Block Addressing** -- Fixed-width addresses waste space. VDE superblock declares `AddressWidth: u8` (32/48/64/128 bits). All on-disk pointers use exactly that width. 99% of VDEs need only 32-bit (covers 16TB), so they store 4 bytes per pointer instead of 8 or 16. Online width promotion: when VDE grows past current width's capacity, a background rewrite widens pointers while VDE stays mounted. Shrinking works after compaction. In-memory representation always uses 64-bit (or 128-bit if promoted) — compression happens at the I/O boundary. The superblock `AddressWidth` field is checked at open time; readers that don't support the declared width refuse to open (MinReaderVersion enforces this).
17. **Adaptive Index Engine (AIE) — Continuous Transparent Morphing** -- The index is a living organism that morphs across a continuous spectrum — forward as data grows, backward as data shrinks. DW autonomously decides the optimal structure at every moment. The index is NOT monolithic — it is sharded, striped, and tiered like a RAID array.
    - **The Morphing Spectrum (forward):**
      - **Level 0: Direct Pointer Array** (1–64 objects) — flat `object[]` with direct index. O(1) read/write. Smallest possible overhead: zero metadata, zero structure.
      - **Level 1: Sorted Array** (64–4K objects) — binary search, O(log N). Inserts shift elements (acceptable at this size). Cache-line-friendly: entire index fits in L1/L2 cache.
      - **Level 2: ART (Adaptive Radix Tree)** (4K–256K objects) — O(k) lookup, 4 adaptive node types, SIMD Node16 search, 8.1 bytes/key. In-memory only. Zero rebalancing overhead.
      - **Level 3: Bε-tree** (256K–1B objects) — write-optimized on-disk tree with message buffers (ε=0.5). 64x fewer write I/Os than B-tree. Identical read performance. ART persists as L0 write buffer (absorbs bursts, flushes in batches).
      - **Level 4: Bε-tree + Learned Overlay** (1B–100B objects) — ALEX learned index model trained on actual key distribution, predicts Bε-tree leaf position in O(1). Falls back to Bε-tree traversal on miss. Model retrains incrementally on distribution drift.
      - **Level 5: Sharded Bε-tree Forest** (100B–10T objects) — Hilbert curve partitions keyspace. Each shard is an independent Bε-tree at its own morph level. Learned routing model routes to correct shard in O(1). Shards auto-split on size threshold, auto-merge on low utilization.
      - **Level 6: Distributed Probabilistic Routing** (10T+ objects) — CRUSH distributes shards across cluster nodes. Bloofi hierarchical bloom filter provides O(1) "which node?" pre-routing. Clock-SI for snapshot isolation without centralized oracle. Each node runs its own local AIE — shards on different nodes can be at different morph levels.
    - **Backward Morphing (shrink):** Deletes trigger morph-down evaluation. When a Bε-tree's live key count drops below 30% of its level's threshold, the IndexMorphAdvisor triggers compaction + demotion. Forest shards below threshold merge. A 1T-object deployment that deletes 99.9% of data will morph backward through every level until it reaches a sorted array or direct pointer — automatically, transparently, without admin intervention.
    - **The IndexMorphAdvisor (autonomous decision engine):**
      - Runs as a background task, observes: object count, read/write ratio, key distribution entropy, cache hit rate, I/O latency percentiles, memory pressure, storage device characteristics (NVMe queue depth, rotational vs flash).
      - Decision algorithm: each morph level has a `promoteThreshold` (object count + workload score) and `demoteThreshold` (object count × 0.3 + idle time). Thresholds are self-tuning: if a promotion causes latency regression, it auto-reverts and adjusts the threshold upward.
      - All decisions logged to VDE Audit Log region. Admin can override via policy: `IndexMorphPolicy: { maxLevel: 4, preferWrite: true }` (e.g., force write-optimized structure even at small scale).
    - **Index Parallelism (Index RAID):**
      - **Index Striping (RAID-0):** A single logical index is striped across N VDE regions or N storage devices. Parallel read: query fans out to all stripes, results merged. Write: round-robin to stripes. N is auto-tuned from detected NVMe queue count and CPU cores.
      - **Index Mirroring (RAID-1):** Hot index replicated to M copies for read concurrency. Writes go to all mirrors (synchronous or async configurable). On corruption: healthy mirror serves reads while damaged mirror rebuilds.
      - **Index Sharding (Partitioning):** Key range divided into S shards. Each shard is an independent AIE instance at its own morph level. A shard with 50M keys might be a Bε-tree while an adjacent shard with 500 keys is a sorted array. Shard boundaries auto-adjust for even distribution.
      - **Index Tiering (L1/L2/L3):** Hot keys promoted to in-memory ART (L1), warm keys in Bε-tree on NVMe (L2), cold keys in learned index on archive storage (L3). Promotion/demotion driven by access frequency tracked via a count-min sketch (probabilistic, O(1), fixed memory).
      - **Index RAID levels:** Configurable per VDE: `IndexRAID: { stripe: 4, mirror: 2, shards: auto }`. Default `auto` picks based on hardware probes.
    - **Zero-Downtime Transitions:** Every morph transition is:
      1. **WAL-journaled** — crash at any point during transition recovers to the pre-transition or post-transition state (never partial).
      2. **Copy-on-write** — new structure built alongside old; readers use old structure until new is complete; atomic pointer swap at commit.
      3. **Background** — transition runs on a dedicated thread pool with I/O priority below normal operations; configurable max CPU% (default 10%).
      4. **Measurable** — transition progress reported via observability (% complete, estimated time remaining, I/O budget consumed).
      5. **Cancellable** — admin can cancel a transition mid-flight; it rolls back cleanly.
    - **Current B-tree becomes a fallback.** The existing BTree.cs is preserved as `LegacyBTreeIndex` for v1.0 VDE compatibility. New VDEs default to AIE Level 0 (direct pointer) and morph upward as data arrives.
18. **Advanced Algorithms & Native Performance** -- Algorithms that augment AIE and provide maximum-performance native integration:
    - **Bw-Tree (lock-free variant)**: For concurrent hot-path index access. Delta records prepended via CAS (no locks), epoch-based GC for reclamation. 5-18x speedup over lock-based B-tree. Used for the in-memory metadata cache and session-scoped indexes.
    - **Masstree (trie-of-B+-trees)**: For extreme-concurrency key-value lookup. 8-byte key slicing, optimistic readers (never dirty shared cache lines), lightweight per-node writer locks. 26-1000x over range-query-capable stores. Used for hot-path object lookup in the VDE namespace tree.
    - **LMAX Disruptor pattern**: For message bus internals. Pre-allocated ring buffer (power-of-2 size), cache-line-padded sequences, sequence barriers, pluggable wait strategies. 100M+ msgs/sec. Replaces Channel<T> on hot paths.
    - **Extendible Hashing**: For inode table growth. Directory doubles on overflow (O(2^d) pointer copies, zero data copies). Single disk access per lookup. Replaces fixed-offset linear inode array. Supports trillion-object inode tables.
    - **Hilbert Space-Filling Curve**: For multi-dimensional → 1D mapping. Best locality preservation of any SFC (3-11x query speedup over Z-order). Used for spatial queries, multi-attribute range scans, and Bε-tree Forest partitioning.
    - **Trained Zstd Dictionaries**: For compression. Train dictionary from VDE data sample at creation time, store in Compression Dictionary Region. 2-5x better ratio for small blocks (4KB).
    - **io_uring — Native Linux I/O (NOT a wrapper):**
      - Implementation: thin `[LibraryImport]` source-generated bindings directly to `liburing.so` — NOT a managed wrapper library. Direct syscall via `io_uring_enter` for submission, memory-mapped SQ/CQ rings shared with kernel.
      - Registered buffers: pre-pinned DMA-capable `NativeMemory.AlignedAlloc` pages (4KB aligned) registered once at VDE open; all subsequent I/O uses registered buffer IDs (avoids per-I/O pin/unpin).
      - Ring-per-thread: each VDE I/O thread owns its own io_uring instance — zero cross-thread synchronization on the submission path.
      - SQPoll mode: dedicated kernel polling thread for sustained high-throughput workloads (configurable, off by default to avoid CPU burn on idle VDEs).
      - NVMe passthrough: `IORING_OP_URING_CMD` for direct NVMe command submission, bypassing filesystem entirely when VDE is on raw block device.
      - Fallback: on non-Linux or when liburing is absent, gracefully falls back to standard `RandomAccess.ReadAsync`/`WriteAsync`. Detection at VDE open time via `RuntimeInformation.IsOSPlatform`.
      - Target: 33x throughput over sync I/O, 2.5x over epoll for network I/O.
    - **HNSW + Product Quantization — Native GPU-Accelerated Vector Search:**
      - CPU path (always available): Pure C# HNSW graph with `Vector256<float>` / `Avx2` SIMD for distance computation. Product Quantization with 256-centroid codebooks per subspace. Asymmetric Distance Computation (ADC) via precomputed lookup tables. Target: <5ms at 1M vectors, <50ms at 100M vectors.
      - GPU path (CUDA, when available): ILGPU (open-source .NET GPU compiler) compiles C# kernels to PTX/CUDA at runtime — no separate CUDA toolkit install needed, no C++ build step. GPU kernels for: batch distance computation (1024 vectors per warp), parallel graph traversal (multi-query), PQ codebook training (k-means on GPU). Falls back to CPU path transparently when no CUDA GPU detected.
      - Alternative GPU path: for maximum raw throughput, optional P/Invoke to NVIDIA cuVS (CAGRA algorithm) via `[LibraryImport("libcuvs")]`. This is the fastest known GPU ANN implementation. Activated only when cuVS shared library is present; otherwise ILGPU path.
      - Storage: HNSW graph + PQ codebooks stored in VDE Intelligence Cache region. Graph layers serialized as adjacency lists with variable-width neighbor IDs. PQ codebooks stored as float32 centroid tables.
      - Index RAID applies: HNSW graph can be sharded by vector ID range across multiple VDE regions for parallel search.
    - **Count-Min Sketch for access tracking**: Fixed-memory (configurable, default 64KB) probabilistic frequency counter. Tracks key access frequency for Index Tiering promotion/demotion decisions. O(1) update, O(1) query, bounded error. Decays over time (halve all counters periodically) to adapt to changing workloads.
    - **All native paths have managed fallbacks.** DW runs on ANY platform at full correctness. Native acceleration (io_uring, CUDA, AVX2) activates transparently when hardware/OS supports it. The performance hierarchy: `GPU > SIMD > managed` — always picking the best available.
19. **VDE Scalable Internals — Static Format, Adaptive Subsystems** -- The on-disk VDE format is STATIC (superblock, region directory, magic bytes never morph). Each subsystem INSIDE the VDE uses adaptive strategies that auto-tune based on scale. This is the AIE philosophy applied to everything else.
    - **Block Allocation: Allocation Groups (ext4/XFS model):**
      - VDE data region divided into allocation groups (default 128MB each, configurable via policy).
      - Each group: own bitmap, free-space count, per-group lock. Allocations are group-local (zero contention for concurrent writers).
      - Group count grows as VDE grows. Metadata stored in Allocation Group Descriptor Table region.
      - Small VDEs (<128MB): single group = zero overhead over current design.
      - Best-fit vs first-fit allocator per group (configurable, default first-fit for sequential, best-fit for random).
    - **Cache: ARC (Adaptive Replacement Cache) with 3 Tiers:**
      - **L1 (in-process ARC):** Self-tuning between recency list (T1) and frequency list (T2), ghost lists (B1/B2) track eviction history for adaptation. Replaces TTL-only DefaultCacheManager.
      - **L2 (memory-mapped):** VDE regions mmap'd for zero-copy read. OS page cache manages eviction. Activated when VDE > available RAM.
      - **L3 (NVMe read cache):** Warm blocks written to dedicated fast device. For tiered storage (HDD primary + NVMe cache).
      - Small VDEs: L1 only (pure in-memory, near-zero overhead). Config: `CachePolicy: { l1Size: auto, l2Enabled: true, l3Device: null }`.
    - **Inode: Variable-Width with Inline Data:**
      - **Compact inode (64 bytes):** Objects ≤48 bytes stored INLINE (zero block allocation). Covers config files, small records, tag-only entries.
      - **Standard inode (256 bytes):** Current format with WORKING indirect/double-indirect/triple-indirect blocks. Supports files up to 64GB (4KB blocks). Fix the NotSupportedException for indirect blocks.
      - **Extended inode (512+ bytes):** Inline xattrs, nanosecond timestamps, compression dictionary reference, per-object encryption IV, version chain pointer.
      - **Extent-based addressing:** Replace 12 direct + indirect pointers with extent tree (start block + length). 1TB file = ~64 extents vs ~268M indirect pointers. ≤4 extents stored inline in inode, overflow to extent block.
      - **Inode layout:** Superblock declares `InodeLayout: Compact64 | Standard256 | Extended512 | Mixed` (Mixed = auto-selection per object based on content size). Configurable at VDE creation.
    - **Per-Object Block Sizes (Sub-Block Allocation):**
      - Superblock declares primary block size. Objects can use large extents (contiguous multi-block runs as one allocation unit).
      - **Sub-block packing:** Multiple small objects share one block (tail-merging). Allocation group tracks sub-block allocation via secondary bitmap.
      - Result: 4KB config files and 4GB video files coexist efficiently in same VDE.
    - **SQL Engine Optimization (OLTP + OLAP):**
      - **OLTP (SQL-Over-Objects):**
        - MVCC: WAL-based multi-version concurrency. Writers append WAL records with transaction ID. Readers acquire snapshot (WAL sequence number), see only versions ≤ snapshot. No global lock.
        - Prepared query cache: parsed + planned queries cached by fingerprint (parameterized).
        - Merge join: for sorted inputs (Bε-tree range scans are inherently sorted). Complements hash join and nested-loop.
      - **OLAP (SQL Analytics):**
        - Columnar VDE regions: optional per-table columnar storage with run-length + dictionary encoding. `CREATE TABLE ... WITH (FORMAT=COLUMNAR)`.
        - Zone maps: per-extent min/max/null_count in extent headers. Query planner pushes predicates to storage — skip entire extents without reading.
        - SIMD vectorized execution: `Vector256<float>` / `Avx2` for SUM/COUNT/MIN/MAX/AVG and comparison predicates. Process 8 values per instruction.
        - Spill-to-disk: aggregation exceeding memory budget spills to temp VDE region (hash partitioned). Configurable per-query memory limit.
        - Predicate pushdown: WHERE clauses pushed into storage scan layer. Combined with zone maps, skips >90% of data for selective queries.
    - **Tag Index: Persistent Roaring Bitmaps:**
      - Replace in-memory ConcurrentDictionary + HashSet<long> with persistent inverted index in VDE Tag Index region.
      - **Roaring bitmaps:** Hybrid sorted arrays (sparse) + bitsets (dense). ~2 bytes/element average. O(1) AND/OR/NOT.
      - **Tag bloom filter per allocation group:** 1KB bloom answers "does this group contain tag=X?" in O(1). Skip entire groups during tag queries.
      - Low-cardinality tags: roaring bitmap scan O(result_size). High-cardinality tags: Bε-tree sub-index per key (integrated with AIE).
    - **Encryption & Compression: Per-Extent Operations:**
      - Current per-block model adds per-block IV + header overhead. New: per-extent operations — encrypt/compress contiguous range of blocks as one unit.
      - One IV per extent, one dictionary per extent, bulk AES-NI processing. Small objects = extent of 1 block (same as today). Large objects = 256 blocks (1MB) per extent.
    - **MVCC (Multi-Version Concurrency Control):**
      - WAL-based: each write appends WAL record with transaction ID. Readers snapshot = current WAL sequence number.
      - Version chain: inode `VersionChainHead` pointer. Previous versions in dedicated MVCC Region. Inline for small changes.
      - GC: background vacuum removes versions older than oldest active snapshot + retention window.
      - Isolation levels: Read Committed (default), Snapshot Isolation, Serializable (predicate locks).
    - **Checksums: Hierarchical (Block → Extent → Object):**
      - Per-block XxHash64 (fast), per-extent CRC32C (medium), per-object Merkle root (strong). Stored in Integrity Tree region.
      - Merkle tree enables binary-search corruption localization. Paranoid mode: full Merkle on every read. Performance mode: extent CRC32C on read, Merkle in background.
    - **Replication & Snapshots: Extent-Aware:**
      - CoW snapshots mark entire extents as shared (not individual blocks). Reduces snapshot metadata by orders of magnitude for large files.
      - Replication delta: changed extents (not blocks). Larger granularity with proportionally less tracking overhead.
    - **What Morphs vs What's Static:** Index morphs (AIE). Cache adapts (ARC self-tunes). Everything else is static-but-configurable — the right choice is made at VDE creation or per-object, not dynamically at runtime. This avoids unnecessary overhead while supporting any scale.
20. **Universal Dynamic Scaling — Every Subsystem Grows and Shrinks** -- Every subsystem in DW (not just the index and VDE internals) must dynamically scale from tiny to yottabyte. The systemic problem: every Ultimate* plugin uses unbounded ConcurrentDictionary for entity state, all state is in-memory only (lost on restart), no eviction, no partitioning, no horizontal distribution. The fix is an SDK-level scaling contract that every plugin inherits.
    - **SDK Scaling Contract (IScalableSubsystem):**
      - `BoundedCache<TKey, TValue>` — configurable eviction (LRU/ARC/TTL), auto-sized from available RAM, ghost-list adaptation
      - `IPersistentBackingStore` — pluggable persistence (file, database, VDE region, external). State survives restarts. Lazy load from backing store on cache miss.
      - `IScalingPolicy` — runtime-reconfigurable Max* limits via configuration hierarchy (Instance→Tenant→User). No restart needed.
      - `IPartitionable` — horizontal sharding when single-node is insufficient. Partition key configurable per subsystem.
      - `IBackpressureAware` — graceful degradation under load. Configurable strategies: drop-oldest, block-producer, shed-load, degrade-quality.
    - **Blockchain / TamperProof:**
      - Replace `List<Block>` with segmented mmap'd page store. Each segment = 64MB of blocks. Only hot segments cached in memory.
      - Fix `long→int` cast for block indexing (support >2B blocks).
      - Shard journal file by block range (one file per 1M blocks).
      - Per-tier locks instead of single `SemaphoreSlim(1,1)`.
      - Bounded `_manifestCache` and `_validationCache` with LRU eviction.
      - Make `MaxConcurrentWrites` configurable (not locked to tier count of 4).
    - **AEDS:**
      - Replace `Dictionary` caches with bounded LRU caches (configurable max entries).
      - Replace single `SemaphoreSlim(1,1)` with `ConcurrentDictionary` or per-collection locks.
      - Make `MaxConcurrentChunks` and `ChunkSizeBytes` dynamically adjustable.
      - Add partitioned job queues for horizontal scaling.
    - **WASM Runtime:**
      - Make `MaxPages` (currently 256 = 16MB) configurable per-module.
      - Make `MaxConcurrentExecutions` (currently hardcoded 100) dynamically adjustable based on system load.
      - Add module state persistence (currently all in-memory = lost on restart).
      - Add warm module instance pooling.
    - **Consensus (Raft):**
      - Multi-Raft: partition state across multiple Raft groups (each ≤100 nodes).
      - Segmented log store: replace single file with segment files (one per 10K entries). Memory-mapped hot segments.
      - Connection pooling for RPC (replace per-call `TcpClient` creation).
      - Dynamic election timeouts based on observed network latency.
    - **Message Bus:**
      - Runtime reconfiguration of `KernelLimitsConfig` without restart.
      - Add persistent message queuing option (WAL-backed) for durability beyond 100K in-memory limit.
      - Add topic partitioning for horizontal scale-out (partition key per topic).
      - Backpressure signaling: producers notified when approaching limits.
      - Integration with Disruptor pattern (AD-18/AIE-17) for hot paths.
    - **Replication:**
      - Per-key or per-namespace strategy selection (not single global active strategy).
      - Persistent replication queue (WAL-backed) for in-flight replication durability.
      - Dynamic replica discovery and management.
      - Stream conflict data instead of loading full blobs for `SequenceEqual` comparison.
    - **Streaming:**
      - **Implement `PublishAsync`/`SubscribeAsync`** (currently stubs returning empty).
      - Wire `ScalabilityConfig` (auto-scale, min/max instances, thresholds, backpressure) to actual logic.
      - Add checkpoint storage integration for exactly-once processing.
    - **ACL / Access Control:**
      - Replace `List<PolicyAccessDecision> _auditLog` with ring buffer or externalized audit store.
      - Make weighted evaluation threshold (currently hardcoded >50%) configurable.
      - Add parallel strategy evaluation option.
      - Make behavior analysis thresholds configurable.
    - **Resilience:**
      - **Fix `ExecuteWithResilienceAsync`** — currently calls `action(ct)` directly without applying the strategy's resilience logic. Operations are unprotected.
      - Make circuit breaker and bulkhead options dynamically adjustable at runtime.
      - Add adaptive thresholds based on observed latency/error rates.
      - Add distributed circuit breaker state sharing across cluster nodes.
    - **DataCatalog / Governance / Lineage / DataMesh:**
      - All four plugins have the same problem: 3-5 unbounded ConcurrentDictionary instances per plugin with no eviction, no persistence, all state lost on restart.
      - Solution: replace with `BoundedCache` + `IPersistentBackingStore` pattern.
      - DataCatalog: persistent asset store with LRU cache + pagination for listing.
      - Governance: persistent policy store with TTL cache + parallel strategy evaluation.
      - Lineage: persistent graph store (or graph DB strategy delegation) + graph partitioning.
      - DataMesh: persistent domain store with cross-domain federation + bounded caches.
    - **Compression / Encryption:**
      - Compression: adaptive buffer size based on input size and available memory. Parallel chunk compression.
      - Encryption: runtime hardware capability re-detection. Dynamic `MaxConcurrentMigrations`.
    - **Search / Vectors:**
      - Extract search into proper inverted index infrastructure (currently just ConcurrentDictionary iteration).
      - Add search result pagination/streaming.
      - Add vector index sharding for horizontal scaling.
    - **Database Storage:**
      - Fix `HandleRetrieveAsync` to stream data (currently reads entire blob into `MemoryStream.ToArray()`).
      - Add query result pagination/streaming.
      - Parallelize health checks across strategies.
    - **Filesystem:**
      - Dynamic I/O scheduling with configurable queue depths.
      - Runtime kernel bypass capability re-detection (for container environments).
    - **Backup / Data Protection:**
      - Make `MaxConcurrentJobs` (currently 4) dynamically adjustable based on I/O capacity.
      - Add backup operation persistence for crash recovery.
    - **Pipeline:**
      - Add configurable maximum pipeline depth.
      - Add concurrent transaction limits.
      - Add `CapturedState` size limits per stage.
    - **DataFabric:**
      - Make `MaxNodes` (Star=1000, Mesh=500, Federated=10000) configurable at runtime per deployment.
      - Add dynamic topology switching based on node count.
    - **Design Pattern:** Every subsystem gets the same treatment: `unbounded ConcurrentDictionary` → `BoundedCache<K,V>` backed by `IPersistentBackingStore`, all `Max*` limits become runtime-reconfigurable via policy engine, sequential evaluation loops become `Task.WhenAll` where strategies are independent. The SDK provides the infrastructure; plugins just configure it.
21. **Ecosystem Compatibility — The Missing Table Stakes** -- DW cannot win if the data world can't talk to it. Six gaps that are non-negotiable for adoption:
    - **Multi-Language Client SDKs:**
      - **Python SDK (first priority):** The data world runs on Python. pandas, PySpark, Jupyter, MLflow, Airflow, dbt — all Python. Without a Python SDK, DW is invisible to 90% of data engineers.
      - Implementation: gRPC client generated from `.proto` definitions (already have gRPC strategy in UltimateInterface). Python package on PyPI. Covers: connect, store, retrieve, query (SQL), tag, search, stream.
      - **Java SDK:** Enterprise integration (Spark, Kafka Connect, Flink, Spring). JAR on Maven Central. gRPC-based.
      - **Go SDK:** Cloud-native/DevOps tooling (Terraform providers, Kubernetes operators). Module on pkg.go.dev. gRPC-based.
      - **Rust SDK:** Performance-critical and embedded use cases. Crate on crates.io. gRPC or direct binary protocol.
      - **JavaScript/TypeScript SDK:** Web dashboards, Node.js integrations. npm package. gRPC-web or REST-based.
      - **Pattern:** All SDKs share the same `.proto` definitions. One source of truth, five generated clients + idiomatic wrappers. SDK test suite runs against all languages.
    - **PostgreSQL Wire Protocol:**
      - Implement PostgreSQL v3 wire protocol (Frontend/Backend protocol) in UltimateInterface as a new strategy.
      - Any PostgreSQL client (psql, pgAdmin, DBeaver, SQLAlchemy, JDBC, npgsql) connects to DW and runs SQL.
      - DW's SQL engine (SqlParserEngine + CostBasedQueryPlanner + QueryExecutionEngine) already exists — this is a transport layer, not a new query engine.
      - Maps: `CREATE TABLE` → VDE object creation, `SELECT/INSERT/UPDATE/DELETE` → SQL-over-objects, `\d` → catalog queries.
      - Extended query protocol (prepared statements, parameterized queries) for security and performance.
      - Wire-level TLS (same as PostgreSQL `sslmode=require`).
      - **NOT full PostgreSQL compatibility** — we don't implement PL/pgSQL, stored procedures, triggers, or PostgreSQL-specific functions. We implement the wire protocol + standard SQL. This is similar to what CockroachDB, YugabyteDB, and QuestDB do.
    - **Native Parquet/Arrow/ORC Format Support:**
      - **Apache Parquet:** Read/write Parquet files as first-class VDE objects. Column pruning, predicate pushdown, row group skipping via zone maps. Use `Apache.Arrow` NuGet package (official .NET binding).
      - **Apache Arrow:** In-memory columnar format for zero-copy data exchange. Arrow IPC for inter-process communication. Arrow Flight for high-throughput data transfer (gRPC-based, already have gRPC strategy).
      - **Apache ORC:** Read/write ORC files. Stripe-level statistics for predicate pushdown.
      - **Integration with VDE columnar regions (VOPT-16):** VDE internal columnar storage uses Arrow memory format. Parquet read → Arrow → VDE columnar is zero-copy. VDE columnar → Parquet write is zero-copy.
      - **Delta Lake / Iceberg table format:** Already planned in EDGE-13. Transaction log stored natively in VDE (not as files). Time-travel reads VDE snapshots directly.
    - **Infrastructure-as-Code (IaC) Deployment:**
      - **Terraform Provider:** `terraform-provider-datawarehouse` — manages DW instances, VDEs, users, policies, plugins, replication topology. Written in Go using Terraform Plugin SDK v2. Published to Terraform Registry.
      - **Pulumi Provider:** Generated from Terraform provider via `pulumi-terraform-bridge` (standard approach). Supports Python, TypeScript, Go, C#.
      - **Helm Chart:** `helm-chart-datawarehouse` — Kubernetes deployment with: StatefulSet for DW nodes, PersistentVolumeClaim for VDE storage, ConfigMap for configuration, Secret for credentials, Service for client access, Ingress for external access. Supports single-node and clustered deployment.
      - **Kubernetes Operator (stretch):** Custom Resource Definitions (CRDs) for `DataWarehouse`, `VirtualDisk`, `ReplicationPolicy`. Operator manages lifecycle, scaling, upgrades. Built with `dotnet-operator-sdk` or `kubebuilder`.
    - **Jepsen Distributed Correctness Testing:**
      - Jepsen is the gold standard for proving distributed systems work correctly under network partitions, clock skew, and crashes. FoundationDB, CockroachDB, TigerBeetle, etcd all have Jepsen reports.
      - **Test categories:**
        - Linearizability of reads/writes under network partitions
        - Raft consensus correctness (leader election, log replication, split-brain prevention)
        - CRDT convergence under concurrent writes with network delays
        - WAL recovery after crash (VDE data integrity)
        - Snapshot isolation correctness under concurrent transactions (MVCC)
        - Replication consistency (DVV convergence, no lost writes)
      - **Implementation:** Jepsen uses Clojure + Docker. DW needs a Jepsen test harness that: deploys N DW nodes in Docker containers, injects faults (network partition, process kill, clock skew, disk corruption), runs workloads (register, set, list-append, bank), validates history against consistency models (linearizable, serializable, snapshot-isolation).
      - **Alternative/complement:** Elle (Jepsen's built-in checker for transactional consistency), Maelstrom (lightweight Jepsen-like for protocol testing).
      - **Connection to TLA+ (EDGE-03/04):** TLA+ proves the algorithm is correct. Jepsen proves the IMPLEMENTATION is correct. Both are needed.
    - **Connection Pooling:**
      - Already partially addressed in DSCL-07 (Raft connection pooling). Extend to ALL inter-node communication:
      - SDK contract: `IConnectionPool<TConnection>` with configurable min/max connections, idle timeout, health checking, connection validation.
      - Implementations: TCP pool (for Raft RPC, replication, fabric mesh), gRPC channel pool (for client SDKs), HTTP/2 connection pool (for REST/Arrow Flight).
      - Per-node connection limits to prevent resource exhaustion.
      - Connection affinity for session-scoped operations.
