#!/usr/bin/env python3
"""
In-Memory Implementation Analyzer for DataWarehouse
====================================================
Scans all .cs files to find in-memory data structures and classifies them:
  - PERSIST: Likely needs persistence (state that accumulates over time)
  - CACHE: Caching pattern (can be rebuilt from source, but benefits from persistence)
  - TRANSIENT: Legitimate transient state (request-scoped, connection-scoped, etc.)
  - UNKNOWN: Needs manual review

Usage:
    python Metadata/in-memory-analysis.py [--json] [--persist-only] [--verbose]

Output: Metadata/in-memory-analysis-report.json (structured) + console summary
"""

import os
import re
import json
import sys
from pathlib import Path
from collections import defaultdict

# Configuration
ROOT = Path(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
EXCLUDE_DIRS = {"bin", "obj", ".git", ".vs", "node_modules", ".planning", ".omc", "Metadata"}
EXCLUDE_FILES = {"AssemblyInfo.cs", "GlobalUsings.cs"}

# Patterns that indicate in-memory state
INMEMORY_PATTERNS = [
    # Dictionaries / Maps
    (r'(?:private|protected|internal)\s+(?:readonly\s+)?(?:static\s+)?(?:BoundedDictionary|Dictionary|ConcurrentDictionary|SortedDictionary|ImmutableDictionary|OrderedDictionary)<[^>]+>\s+(\w+)',
     "dictionary", "PERSIST"),
    # Lists / Collections
    (r'(?:private|protected|internal)\s+(?:readonly\s+)?(?:static\s+)?(?:BoundedList|List|ConcurrentBag|ConcurrentQueue|ConcurrentStack|LinkedList|SortedList|HashSet|SortedSet)<[^>]+>\s+(\w+)',
     "collection", "CACHE"),
    # Queues
    (r'(?:private|protected|internal)\s+(?:readonly\s+)?(?:static\s+)?(?:BoundedQueue|Queue|PriorityQueue|Channel)<[^>]+>\s+(\w+)',
     "queue", "TRANSIENT"),
    # Caches (explicit)
    (r'(?:private|protected|internal)\s+(?:readonly\s+)?(?:static\s+)?(?:MemoryCache|IMemoryCache)\s+(\w+)',
     "memory_cache", "CACHE"),
    # Counters / Metrics (long, int fields with metric-like names)
    (r'(?:private|protected|internal)\s+(?:readonly\s+)?(?:static\s+)?(?:long|int|ulong)\s+(_total\w+|_count\w*|_metric\w*|_bytes\w*)',
     "counter", "PERSIST"),
    # State machines / flags that accumulate
    (r'(?:private|protected|internal)\s+(?:readonly\s+)?(?:static\s+)?(?:Dictionary|BoundedDictionary)<string,\s*(?:object|string|byte\[\]|Stream)>\s+(\w+)',
     "state_store", "PERSIST"),
    # Timer/Stopwatch state
    (r'(?:private|protected|internal)\s+(?:readonly\s+)?(?:Stopwatch|Timer|System\.Timers\.Timer)\s+(\w+)',
     "timer", "TRANSIENT"),
    # Semaphore/Lock state
    (r'(?:private|protected|internal)\s+(?:readonly\s+)?(?:static\s+)?(?:SemaphoreSlim|ReaderWriterLockSlim|Mutex|object)\s+(_\w*lock\w*|_\w*sync\w*|_\w*gate\w*|_\w*semaphore\w*)',
     "sync_primitive", "TRANSIENT"),
    # Byte arrays / buffers
    (r'(?:private|protected|internal)\s+(?:readonly\s+)?(?:static\s+)?byte\[\]\s+(_\w*buffer\w*|_\w*data\w*|_\w*payload\w*|_\w*cache\w*)',
     "buffer", "CACHE"),
]

# Heuristics to reclassify based on context
PERSIST_HINTS = [
    r'_state', r'_registry', r'_store', r'_index', r'_catalog',
    r'_history', r'_log', r'_audit', r'_config', r'_settings',
    r'_mapping', r'_metadata', r'_manifest', r'_checkpoint',
    r'_snapshot', r'_journal', r'_ledger', r'_inventory',
]

TRANSIENT_HINTS = [
    r'_pending', r'_active', r'_current', r'_temp', r'_tmp',
    r'_request', r'_response', r'_connection', r'_session',
    r'_disposed', r'_cancellation', r'_cts',
]


def classify_field(field_name, default_class, surrounding_code):
    """Reclassify based on field name and surrounding code patterns."""
    name_lower = field_name.lower()

    # Check persist hints
    for hint in PERSIST_HINTS:
        if re.search(hint, name_lower):
            return "PERSIST"

    # Check transient hints
    for hint in TRANSIENT_HINTS:
        if re.search(hint, name_lower):
            return "TRANSIENT"

    # Check if field is cleared in Dispose/StopAsync (likely transient)
    if re.search(rf'{re.escape(field_name)}\.Clear\(\)', surrounding_code):
        return "TRANSIENT"
    if re.search(rf'{re.escape(field_name)}\s*=\s*null', surrounding_code):
        return "TRANSIENT"

    # Check if field is populated from external source (likely cache)
    if re.search(rf'{re.escape(field_name)}\[.*\]\s*=.*await.*Async', surrounding_code):
        return "CACHE"

    return default_class


def scan_file(filepath):
    """Scan a single .cs file for in-memory implementations."""
    findings = []
    try:
        with open(filepath, 'r', encoding='utf-8', errors='replace') as f:
            content = f.read()
            lines = content.split('\n')
    except Exception:
        return findings

    # Skip auto-generated files
    if any(marker in content[:500] for marker in ['<auto-generated', '// <autogenerated']):
        return findings

    for pattern, category, default_class in INMEMORY_PATTERNS:
        for match in re.finditer(pattern, content):
            field_name = match.group(1)
            pos = match.start()
            line_num = content[:pos].count('\n') + 1

            # Get surrounding context (50 lines)
            start_line = max(0, line_num - 10)
            end_line = min(len(lines), line_num + 40)
            context = '\n'.join(lines[start_line:end_line])

            classification = classify_field(field_name, default_class, context)

            # Get the class name
            class_match = None
            for cm in re.finditer(r'(?:class|struct|record)\s+(\w+)', content[:pos]):
                class_match = cm
            class_name = class_match.group(1) if class_match else "Unknown"

            # Get the full declaration line
            decl_line = lines[line_num - 1].strip() if line_num <= len(lines) else ""

            findings.append({
                "file": str(filepath.relative_to(ROOT)),
                "line": line_num,
                "class": class_name,
                "field": field_name,
                "category": category,
                "classification": classification,
                "declaration": decl_line,
            })

    return findings


def main():
    json_mode = "--json" in sys.argv
    persist_only = "--persist-only" in sys.argv
    verbose = "--verbose" in sys.argv

    all_findings = []
    files_scanned = 0

    for root, dirs, files in os.walk(ROOT):
        dirs[:] = [d for d in dirs if d not in EXCLUDE_DIRS]
        for fname in files:
            if not fname.endswith('.cs') or fname in EXCLUDE_FILES:
                continue
            filepath = Path(root) / fname
            files_scanned += 1
            findings = scan_file(filepath)
            all_findings.extend(findings)

    # Sort by classification priority
    priority = {"PERSIST": 0, "CACHE": 1, "UNKNOWN": 2, "TRANSIENT": 3}
    all_findings.sort(key=lambda f: (priority.get(f["classification"], 9), f["file"], f["line"]))

    if persist_only:
        all_findings = [f for f in all_findings if f["classification"] == "PERSIST"]

    # Summary
    by_class = defaultdict(list)
    by_category = defaultdict(int)
    by_project = defaultdict(lambda: defaultdict(int))

    for f in all_findings:
        by_class[f["classification"]].append(f)
        by_category[f["category"]] += 1
        # Extract project
        parts = f["file"].replace("\\", "/").split("/")
        if parts[0] == "Plugins" and len(parts) > 1:
            proj = parts[1]
        elif parts[0] in ("DataWarehouse.SDK", "DataWarehouse.Kernel", "DataWarehouse.Shared",
                          "DataWarehouse.Tests", "DataWarehouse.GUI", "DataWarehouse.Launcher"):
            proj = parts[0]
        else:
            proj = parts[0]
        by_project[proj][f["classification"]] += 1

    # Write JSON report
    report = {
        "files_scanned": files_scanned,
        "total_findings": len(all_findings),
        "by_classification": {k: len(v) for k, v in by_class.items()},
        "by_category": dict(by_category),
        "by_project": {k: dict(v) for k, v in sorted(by_project.items())},
        "findings": all_findings,
    }

    report_path = ROOT / "Metadata" / "in-memory-analysis-report.json"
    with open(report_path, 'w') as f:
        json.dump(report, f, indent=2)

    if json_mode:
        print(json.dumps(report, indent=2))
        return

    # Console output
    print(f"=== IN-MEMORY IMPLEMENTATION ANALYSIS ===")
    print(f"Files scanned: {files_scanned}")
    print(f"Total in-memory structures found: {len(all_findings)}")
    print()

    print("=== BY CLASSIFICATION ===")
    for cls in ["PERSIST", "CACHE", "TRANSIENT", "UNKNOWN"]:
        items = by_class.get(cls, [])
        print(f"  {cls:12s}: {len(items):>5}")
    print()

    print("=== BY CATEGORY ===")
    for cat, count in sorted(by_category.items(), key=lambda x: -x[1]):
        print(f"  {cat:20s}: {count:>5}")
    print()

    print("=== BY PROJECT (PERSIST items) ===")
    for proj, classes in sorted(by_project.items(), key=lambda x: -x[1].get("PERSIST", 0)):
        persist_count = classes.get("PERSIST", 0)
        if persist_count > 0:
            print(f"  {proj:60s}: {persist_count:>4} PERSIST")
    print()

    if verbose:
        print("=== PERSIST ITEMS (need persistence review) ===")
        for f in by_class.get("PERSIST", []):
            print(f"  {f['file']}:{f['line']}  {f['class']}.{f['field']}  [{f['category']}]")
            print(f"    {f['declaration']}")
            print()

    print(f"\nReport written to: {report_path}")
    print(f"Run with --persist-only to see only items needing persistence")
    print(f"Run with --verbose to see detailed PERSIST items")


if __name__ == "__main__":
    main()
